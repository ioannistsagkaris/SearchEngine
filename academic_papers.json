[
    {
        "ID": 1,
        "Title": "Mitigating Gender Bias in Code Large Language Models via Model Editing",
        "Authors": [
            "Zhanyue Qin",
            "Haochuan Wang",
            "Zecheng Wang",
            "Deyuan Liu",
            "Cunhang Fan",
            "Zhao Lv",
            "Zhiying Tu",
            "Dianhui Chu",
            "Dianbo Sui"
        ],
        "Abstract": "In recent years, with the maturation of large language model (LLM) technology and the emergence of high-quality programming code datasets, researchers have become increasingly confident in addressing the challenges of program synthesis automatically. However, since most of the training samples for LLMs are unscreened, it is inevitable that LLMs' performance may not align with real-world scenarios, leading to the presence of social bias. To evaluate and quantify the gender bias in code LLMs, we propose a dataset named CodeGenBias (Gender Bias in the Code Generation) and an evaluation metric called FB-Score (Factual Bias Score) based on the actual gender distribution of correlative professions. With the help of CodeGenBias and FB-Score, we evaluate and analyze the gender bias in eight mainstream Code LLMs. Previous work has demonstrated that model editing methods that perform well in knowledge editing have the potential to mitigate social bias in LLMs. Therefore, we develop a model editing approach named MG-Editing (Multi-Granularity model Editing), which includes the locating and editing phases. Our model editing method MG-Editing can be applied at five different levels of model parameter granularity: full parameters level, layer level, module level, row level, and neuron level. Extensive experiments not only demonstrate that our MG-Editing can effectively mitigate the gender bias in code LLMs while maintaining their general code generation capabilities, but also showcase its excellent generalization. At the same time, the experimental results show that, considering both the gender bias of the model and its general code generation capability, MG-Editing is most effective when applied at the row and neuron levels of granularity.",
        "Publication date": "10 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.07820"
    },
    {
        "ID": 2,
        "Title": "Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models",
        "Authors": [
            "Haocheng Xu",
            "Haotian Hu",
            "Sitao Huang"
        ],
        "Abstract": "High-level synthesis (HLS) allows hardware designers to create hardware designs with high-level programming languages like C/C++/OpenCL, which greatly improves hardware design productivity. However, existing HLS flows require programmers' hardware design expertise and rely on programmers' manual code transformations and directive annotations to guide compiler optimizations. Optimizing HLS designs requires non-trivial HLS expertise and tedious iterative process in HLS code optimization. Automating HLS code optimizations has become a burning need. Recently, large language models (LLMs) trained on massive code and programming tasks have demonstrated remarkable proficiency in comprehending code, showing the ability to handle domain-specific programming queries directly without labor-intensive fine-tuning. In this work, we propose a novel retrieval-augmented LLM-based approach to effectively optimize high-level synthesis (HLS) programs. Our proposed method leverages few-shot learning, enabling large language models to adopt domain-specific knowledge through natural language prompts. We propose a unique framework, Retrieve Augmented Large Language Model Aided Design (RALAD), designed to enhance LLMs' performance in HLS code optimization tasks. RALAD employs advanced embedding techniques and top-\\emph{k} search algorithms to dynamically source relevant knowledge from extensive databases, thereby providing contextually appropriate responses to complex programming queries. Our implementation of RALAD on two specialized domains, utilizing comparatively smaller language models, achieves an impressive 80\\% success rate in compilation tasks and outperforms general LLMs by 3.7 -- 19$\\times$ in latency improvement.",
        "Publication date": "9 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.07356"
    },
    {
        "ID": 3,
        "Title": "BLAS-like Interface for Binary Tensor Contractions",
        "Authors": [
            "Niklas HÃ¶rnblad"
        ],
        "Abstract": "In the world of linear algebra computation, a well-established standard exists called BLAS(Basic Linear Algebra Subprograms). This standard has been crucial for the development of software using linear algebra operations. Its benefits include portability with efficiency and mitigation of suboptimal re-implementations of linear algebra operations. Multilinear algebra is an extension of linear algebra in which the central objects are tensors, which are generalizations of vectors and matrices. Though tensor operations are becoming more common, they do not have a standard like BLAS. Such standardization would be beneficial and decrease the now-visible replication of work, as many libraries nowadays use their own implementations. This master thesis aims to work towards such a standard by discovering whether or not a BLAS-like interface is possible for the operation binary tensor contraction. To answer this, an interface has been developed in the programming language C together with an implementation and tested to see if it would be sufficient. The interface developed is:\n  xGETT(RANKA, EXTA, INCA, A, RANKB, EXTB, INCB, B, CONTS, CONTA, CONTB, PERM, INCC, C)\n  with the implementation and tests, it has been deemed sufficient as a BLAS-like interface for binary tensor contractions and possible to use in a BLAS-like standardization for tensor operations.",
        "Publication date": "9 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.06770"
    },
    {
        "ID": 4,
        "Title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?",
        "Authors": [
            "Fumiya Uchiyama",
            "Takeshi Kojima",
            "Andrew Gambardella",
            "Qi Cao",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "Abstract": "Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks. Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.",
        "Publication date": "9 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.06735"
    },
    {
        "ID": 5,
        "Title": "Large Language Models as Code Executors: An Exploratory Study",
        "Authors": [
            "Chenyang Lyu",
            "Lecheng Yan",
            "Rui Xing",
            "Wenxi Li",
            "Younes Samih",
            "Tianbo Ji",
            "Longyue Wang"
        ],
        "Abstract": "The capabilities of Large Language Models (LLMs) have significantly evolved, extending from natural language processing to complex tasks like code understanding and generation. We expand the scope of LLMs' capabilities to a broader context, using LLMs to execute code snippets to obtain the output. This paper pioneers the exploration of LLMs as code executors, where code snippets are directly fed to the models for execution, and outputs are returned. We are the first to comprehensively examine this feasibility across various LLMs, including OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the o1 model achieved over 90% accuracy in code execution, while others demonstrated lower accuracy levels. Furthermore, we introduce an Iterative Instruction Prompting (IIP) technique that processes code snippets line by line, enhancing the accuracy of weaker models by an average of 7.22% (with the highest improvement of 18.96%) and an absolute average improvement of 3.86% against CoT prompting (with the highest improvement of 19.46%). Our study not only highlights the transformative potential of LLMs in coding but also lays the groundwork for future advancements in automated programming and the completion of complex tasks.",
        "Publication date": "10 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.06667"
    },
    {
        "ID": 6,
        "Title": "Synthesizing Interpretable Control Policies through Large Language Model Guided Search",
        "Authors": [
            "Carlo Bosio",
            "Mark W. Mueller"
        ],
        "Abstract": "The combination of Large Language Models (LLMs), systematic evaluation, and evolutionary algorithms has enabled breakthroughs in combinatorial optimization and scientific discovery. We propose to extend this powerful combination to the control of dynamical systems, generating interpretable control policies capable of complex behaviors. With our novel method, we represent control policies as programs in standard languages like Python. We evaluate candidate controllers in simulation and evolve them using a pre-trained LLM. Unlike conventional learning-based control techniques, which rely on black box neural networks to encode control policies, our approach enhances transparency and interpretability. We still take advantage of the power of large AI models, but leverage it at the policy design phase, ensuring that all system components remain interpretable and easily verifiable at runtime. Additionally, the use of standard programming languages makes it straightforward for humans to finetune or adapt the controllers based on their expertise and intuition. We illustrate our method through its application to the synthesis of an interpretable control policy for the pendulum swing-up and the ball in cup tasks. We make the code available at https://github.com/muellerlab/synthesizing_interpretable_control_policies.git",
        "Publication date": "7 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.05406"
    },
    {
        "ID": 7,
        "Title": "SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?",
        "Authors": [
            "John Yang",
            "Carlos E. Jimenez",
            "Alex L. Zhang",
            "Kilian Lieret",
            "Joyce Yang",
            "Xindi Wu",
            "Ori Press",
            "Niklas Muennighoff",
            "Gabriel Synnaeve",
            "Karthik R. Narasimhan",
            "Diyi Yang",
            "Sida I. Wang",
            "Ofir Press"
        ],
        "Abstract": "Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from GitHub repositories. However, SWE-bench uses only Python repositories, with problem statements presented predominantly as text and lacking visual elements such as images. This limited coverage motivates our inquiry into how existing systems might perform on unrepresented software engineering domains (e.g., front-end, game development, DevOps), which use different programming languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench M), to evaluate systems on their ability to fix bugs in visual, user-facing JavaScript software. SWE-bench M features 617 task instances collected from 17 JavaScript libraries used for web interface design, diagramming, data visualization, syntax highlighting, and interactive mapping. Each SWE-bench M task instance contains at least one image in its problem statement or unit tests. Our analysis finds that top-performing SWE-bench systems struggle with SWE-bench M, revealing limitations in visual problem-solving and cross-language generalization. Lastly, we show that SWE-agent's flexible language-agnostic features enable it to substantially outperform alternatives on SWE-bench M, resolving 12% of task instances compared to 6% for the next best system.",
        "Publication date": "4 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.03859"
    },
    {
        "ID": 8,
        "Title": "Integrating Natural Language Prompting Tasks in Introductory Programming Courses",
        "Authors": [
            "Chris Kerslake",
            "Paul Denny",
            "David H Smith IV",
            "James Prather",
            "Juho Leinonen",
            "Andrew Luxton-Reilly",
            "Stephen MacNeil"
        ],
        "Abstract": "Introductory programming courses often emphasize mastering syntax and basic constructs before progressing to more complex and interesting programs. This bottom-up approach can be frustrating for novices, shifting the focus away from problem solving and potentially making computing less appealing to a broad range of students. The rise of generative AI for code production could partially address these issues by fostering new skills via interaction with AI models, including constructing high-level prompts and evaluating code that is automatically generated. In this experience report, we explore the inclusion of two prompt-focused activities in an introductory course, implemented across four labs in a six-week module. The first requires students to solve computational problems by writing natural language prompts, emphasizing problem-solving over syntax. The second involves students crafting prompts to generate code equivalent to provided fragments, to foster an understanding of the relationship between prompts and code. Most of the students in the course had reported finding programming difficult to learn, often citing frustrations with syntax and debugging. We found that self-reported difficulty with learning programming had a strong inverse relationship with performance on traditional programming assessments such as tests and projects, as expected. However, performance on the natural language tasks was less strongly related to self-reported difficulty, suggesting they may target different skills. Learning how to communicate with AI coding models is becoming an important skill, and natural language prompting tasks may appeal to a broad range of students.",
        "Publication date": "3 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.03063"
    },
    {
        "ID": 9,
        "Title": "Demonstration Attack against In-Context Learning for Code Intelligence",
        "Authors": [
            "Yifei Ge",
            "Weisong Sun",
            "Yihang Lou",
            "Chunrong Fang",
            "Yiran Zhang",
            "Yiming Li",
            "Xiaofang Zhang",
            "Yang Liu",
            "Zhihong Zhao",
            "Zhenyu Chen"
        ],
        "Abstract": "Recent advancements in large language models (LLMs) have revolutionized code intelligence by improving programming productivity and alleviating challenges faced by software developers. To further improve the performance of LLMs on specific code intelligence tasks and reduce training costs, researchers reveal a new capability of LLMs: in-context learning (ICL). ICL allows LLMs to learn from a few demonstrations within a specific context, achieving impressive results without parameter updating. However, the rise of ICL introduces new security vulnerabilities in the code intelligence field. In this paper, we explore a novel security scenario based on the ICL paradigm, where attackers act as third-party ICL agencies and provide users with bad ICL content to mislead LLMs outputs in code intelligence tasks. Our study demonstrates the feasibility and risks of such a scenario, revealing how attackers can leverage malicious demonstrations to construct bad ICL content and induce LLMs to produce incorrect outputs, posing significant threats to system security. We propose a novel method to construct bad ICL content called DICE, which is composed of two stages: Demonstration Selection and Bad ICL Construction, constructing targeted bad ICL content based on the user query and transferable across different query inputs. Ultimately, our findings emphasize the critical importance of securing ICL mechanisms to protect code intelligence systems from adversarial manipulation.",
        "Publication date": "3 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.02841"
    },
    {
        "ID": 10,
        "Title": "CUDA-based optical parametric oscillator simulator",
        "Authors": [
            "Alfredo Daniel Sanchez",
            "Chaitanya Kumar Suddapalli",
            "Majid Ebrahim-Zadeh"
        ],
        "Abstract": "The coupled-wave equations (CWEs) in nonlinear optics are the fundamental starting point in the study, analysis, and understanding of various frequency conversion processes in dielectric media subjected to intense laser radiation. In this work, a useful package for the modeling of optical parametric oscillators (OPOs) based on the Split-Step Fourier Method algorithm is presented. The algorithm is scripted in the CUDA programming language in order to speed up the calculations and obtain results in a relatively short time frame by using a graphics processing unit (GPU). Our results show a speedup higher than 50X for vector size of $2^{14}$ in comparison with the analogous code scripted for running only in CPU. The package implements the CWEs to model the propagation of light in second-order nonlinear crystals widely used in optical frequency conversion experiments. In addition, the code allows the user to adapt the cavity configuration by selecting the resonant electric fields and/or incorporating intracavity elements. The package is useful for modeling OPOs or other mathematically similar problems.",
        "Publication date": "3 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.02532"
    },
    {
        "ID": 11,
        "Title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging",
        "Authors": [
            "Yuling Shi",
            "Songsong Wang",
            "Chengcheng Wan",
            "Xiaodong Gu"
        ],
        "Abstract": "While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.",
        "Publication date": "5 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.01215"
    },
    {
        "ID": 12,
        "Title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on Multi-processor Machines",
        "Authors": [
            "Francesco Quaglia"
        ],
        "Abstract": "In this article we present PARSIR (PARallel SImulation Runner), a package that enables the effective exploitation of shared-memory multi-processor machines for running discrete event simulation models. PARSIR is a compile/run-time environment for discrete event simulation models developed with the {\\tt C} programming language. The architecture of PARSIR has been designed in order to keep low the amount of CPU-cycles required for running models. This is achieved via the combination of a set of techniques like: 1) causally consistent batch-processing of simulation events at an individual simulation object for caching effectiveness; 2) high likelihood of disjoint access parallelism; 3) the favoring of memory accesses on local NUMA (Non-Uniform-Memory-Access) nodes in the architecture, while still enabling well balanced workload distribution via work-stealing from remote nodes; 4) the use of RMW (Read-Modify-Write) machine instructions for fast access to simulation engine data required by the worker threads for managing the concurrent simulation objects and distributing the workload. Furthermore, any architectural solution embedded in the PARSIR engine is fully transparent to the application level code implementing the simulation model. We also provide experimental results showing the effectiveness of PARSIR when running the reference PHOLD benchmark on a NUMA shared-memory multi-processor machine equipped with 40 CPUs.",
        "Publication date": "1 October, 2024",
        "Link": "https://arxiv.org/pdf/2410.00644"
    },
    {
        "ID": 13,
        "Title": "Validated enclosure of renormalization fixed points via Chebyshev series and the DFT",
        "Authors": [
            "Maxime Breden",
            "Jorge Gonzalez",
            "J. D Mireles James"
        ],
        "Abstract": "This work develops a computational framework for proving existence, uniqueness, isolation, and stability results for real analytic fixed points of $m$-th order Feigenbaum-CvitanoviÄ renormalization operators. Our approach builds on the earlier work of Lanford, Eckman, Wittwer, Koch, Burbanks, Osbaldestin, and Thurlby \\cite{iii1982computer,eckmann1987complete,MR0727816, burbanks2021rigorous2,burbanks2021rigorous1}, however the main point of departure between ours and previous studies is that we discretize the domain of the renormalization operators using Chebyshev rather than Taylor series. The advantage of Chebyshev series is that they are naturally adapted to spaces of real analytic functions, in the sense that they converge on ellipses containing real intervals rather than on disks in $\\mathbb{C}$. The main disadvantage of working with Chebyshev series in this context is that the essential operations of rescaling and composition are less straight forward for Chebysehv than for Taylor series. These difficulties are overcome using a combination of a-priori information about decay rates in the Banach space with a-posteriori estimates on Chebyshev interpolation errors for analytic functions.\n  Our arguments are implemented in the Julia programming language and exploit extended precision floating point interval arithmetic. In addition to proving the existence of multiple renormalization fixed points of order $m = 3, \\ldots, 10$, and computing validated bounds on the values of their the universal constants, we also reprove the existence of the classical $m=2$ Feigenbaum renormalization fixed point and compute its universal constants to close to 500 correct decimal digits.",
        "Publication date": "30 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.20457"
    },
    {
        "ID": 14,
        "Title": "Resource Allocation for Stable LLM Training in Mobile Edge Computing",
        "Authors": [
            "Chang Liu",
            "Jun Zhao"
        ],
        "Abstract": "As mobile devices increasingly become focal points for advanced applications, edge computing presents a viable solution to their inherent computational limitations, particularly in deploying large language models (LLMs). However, despite the advancements in edge computing, significant challenges remain in efficient training and deploying LLMs due to the computational demands and data privacy concerns associated with these models. This paper explores a collaborative training framework that integrates mobile users with edge servers to optimize resource allocation, thereby enhancing both performance and efficiency. Our approach leverages parameter-efficient fine-tuning (PEFT) methods, allowing mobile users to adjust the initial layers of the LLM while edge servers handle the more demanding latter layers. Specifically, we formulate a multi-objective optimization problem to minimize the total energy consumption and delay during training. We also address the common issue of instability in model performance by incorporating stability enhancements into our objective function. Through novel fractional programming technique, we achieve a stationary point for the formulated problem. Simulations demonstrate that our method reduces the energy consumption as well as the latency, and increases the reliability of LLMs across various mobile settings.",
        "Publication date": "30 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.20247"
    },
    {
        "ID": 15,
        "Title": "Pragma driven shared memory parallelism in Zig by supporting OpenMP loop directives",
        "Authors": [
            "David Kacs",
            "Joseph Lee",
            "Justs Zarins",
            "Nick Brown"
        ],
        "Abstract": "The Zig programming language, which is designed to provide performance and safety as first class concerns, has become popular in recent years. Given that Zig is built upon LLVM, and-so enjoys many of the benefits provided by the ecosystem, including access to a rich set of backends, Zig has significant potential for high performance workloads. However, it is yet to gain acceptance in HPC and one of the reasons for this is that support for the pragma driven shared memory parallelism is missing.\n  In this paper we describe enhancing the Zig compiler to add support for OpenMP loop directives. Then exploring performance using NASA's NAS Parallel Benchmark (NPB) suite. We demonstrate that not only does our integration of OpenMP with Zig scale comparatively to Fortran and C reference implementations of NPB, but furthermore Zig provides up to a 1.25 times performance increase compared to Fortran.",
        "Publication date": "30 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.20148"
    },
    {
        "ID": 16,
        "Title": "Verifying Functional Correctness Properties At the Level of Java Bytecode",
        "Authors": [
            "Marco Paganoni",
            "Carlo A. Furia"
        ],
        "Abstract": "The breakneck evolution of modern programming languages aggravates the development of deductive verification tools, which struggle to timely and fully support all new language features. To address this challenge, we present ByteBack: a verification technique that works on Java bytecode. Compared to high-level languages, intermediate representations such as bytecode offer a much more limited and stable set of features; hence, they may help decouple the verification process from changes in the source-level language.\n  ByteBack offers a library to specify functional correctness properties at the level of the source code, so that the bytecode is only used as an intermediate representation that the end user does not need to work with. Then, ByteBack reconstructs some of the information about types and expressions that is erased during compilation into bytecode but is necessary to correctly perform verification. Our experiments with an implementation of ByteBack demonstrate that it can successfully verify bytecode compiled from different versions of Java, and including several modern language features that even state-of-the-art Java verifiers (such as KeY and OpenJML) do not directly support$\\unicode{x2013}$thus revealing how ByteBack's approach can help keep up verification technology with language evolution.",
        "Publication date": "30 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.20071"
    },
    {
        "ID": 17,
        "Title": "Reasoning About Exceptional Behavior At the Level of Java Bytecode",
        "Authors": [
            "Marco Paganoni",
            "Carlo A. Furia"
        ],
        "Abstract": "A program's exceptional behavior can substantially complicate its control flow, and hence accurately reasoning about the program's correctness. On the other hand, formally verifying realistic programs is likely to involve exceptions -- a ubiquitous feature in modern programming languages.\n  In this paper, we present a novel approach to verify the exceptional behavior of Java programs, which extends our previous work on ByteBack. ByteBack works on a program's bytecode, while providing means to specify the intended behavior at the source-code level; this approach sets ByteBack apart from most state-of-the-art verifiers that target source code. To explicitly model a program's exceptional behavior in a way that is amenable to formal reasoning, we introduce Vimp: a high-level bytecode representation that extends the Soot framework's Grimp with verification-oriented features, thus serving as an intermediate layer between bytecode and the Boogie intermediate verification language. Working on bytecode through this intermediate layer brings flexibility and adaptability to new language versions and variants: as our experiments demonstrate, ByteBack can verify programs involving exceptional behavior in all versions of Java, as well as in Scala and Kotlin (two other popular JVM languages).",
        "Publication date": "30 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.20056"
    },
    {
        "ID": 18,
        "Title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Object-Oriented Programming",
        "Authors": [
            "Tianyang Wang",
            "Ziqian Bi",
            "Keyu Chen",
            "Jiawei Xu",
            "Qian Niu",
            "Junyu Liu",
            "Benji Peng",
            "Ming Li",
            "Sen Zhang",
            "Xuanhe Pan",
            "Jinlang Wang",
            "Pohsun Feng",
            "Caitlyn Heqi Yin",
            "Yizhu Wen",
            "Ming Liu"
        ],
        "Abstract": "Object-Oriented Programming (OOP) has become a crucial paradigm for managing the growing complexity of modern software systems, particularly in fields like machine learning, deep learning, large language models (LLM), and data analytics. This work provides a comprehensive introduction to the integration of OOP techniques within these domains, with a focus on improving code modularity, maintainability, and scalability. We begin by outlining the evolution of computing and the rise of OOP, followed by an in-depth discussion of key OOP principles such as encapsulation, inheritance, polymorphism, and abstraction. The practical application of these principles is demonstrated using Python, a widely adopted language in AI and data science. Furthermore, we examine how design patterns and modular programming can be employed to enhance the structure and efficiency of machine learning systems. In subsequent sections, we apply these OOP concepts to real-world AI tasks, including the encapsulation of preprocessing workflows, machine learning model training, and evaluation. Detailed examples illustrate how OOP can be used to build reusable, scalable machine learning systems while maintaining code clarity and reducing redundancy.This work is intended to serve as a bridge for both beginners and experienced developers, equipping them with the necessary knowledge to apply OOP methodologies in AI-driven projects, ultimately fostering the development of more robust and maintainable systems.",
        "Publication date": "9 October, 2024",
        "Link": "https://arxiv.org/pdf/2409.19916"
    },
    {
        "ID": 19,
        "Title": "Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models",
        "Authors": [
            "Xin Li",
            "Weize Chen",
            "Qizhi Chu",
            "Haopeng Li",
            "Zhaojun Sun",
            "Ran Li",
            "Chen Qian",
            "Yiwei Wei",
            "Zhiyuan Liu",
            "Chuan Shi",
            "Maosong Sun",
            "Cheng Yang"
        ],
        "Abstract": "The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph analysis. The benchmark, datasets and enhanced open-source models are available at https://github.com/BUPT-GAMMA/ProGraph.",
        "Publication date": "29 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.19667"
    },
    {
        "ID": 20,
        "Title": "Polynomial Universes and Dependent Types",
        "Authors": [
            "C. B. AberlÃ©",
            "David I. Spivak"
        ],
        "Abstract": "Awodey, later with Newstead, showed how polynomial pseudomonads $(u,1,Î£)$ with extra structure (termed \"natural models\" by Awodey) hold within them the categorical semantics for dependent type theory. Their work presented these ideas clearly but ultimately led them outside of the category of polynomial functors in order to explain all of the structure possessed by such models of type theory.\n  This paper builds off that work -- explicating the categorical semantics of dependent type theory by axiomatizing them \\emph{entirely} in the language of polynomial functors. In order to handle the higher-categorical coherences required for such an explanation, we work with polynomial functors internally in the language of Homotopy Type Theory, which allows for higher-dimensional structures such as pseudomonads, etc. to be expressed purely in terms of the structure of a suitably-chosen $\\infty$-category of polynomial functors. The move from set theory to Homotopy Type Theory thus has a twofold effect of enabling a simpler exposition of natural models, which is at the same time amenable to formalization in a proof assistant, such as Agda.\n  Moreover, the choice to remain firmly within the setting of polynomial functors reveals many additional structures of natural models that were otherwise left implicit or not considered by Awodey \\& Newstead. Chief among these, we highlight the fact that every polynomial pseudomonad $(u,1,Î£)$ as above that is also equipped with structure to interpret dependent product types gives rise to a self-distributive law $u \\triangleleft u\\to u \\triangleleft u$, which witnesses the usual distributive law of dependent products over dependent sums.",
        "Publication date": "27 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.19176"
    },
    {
        "ID": 21,
        "Title": "Exploring LLM-Driven Explanations for Quantum Algorithms",
        "Authors": [
            "Giordano d'Aloisio",
            "Sophie Fortz",
            "Carol Hanna",
            "Daniel Fortunato",
            "Avner Bensoussan",
            "EÃ±aut Mendiluze Usandizaga",
            "Federica Sarro"
        ],
        "Abstract": "Background: Quantum computing is a rapidly growing new programming paradigm that brings significant changes to the design and implementation of algorithms. Understanding quantum algorithms requires knowledge of physics and mathematics, which can be challenging for software developers.\n  Aims: In this work, we provide a first analysis of how LLMs can support developers' understanding of quantum code. Method: We empirically analyse and compare the quality of explanations provided by three widely adopted LLMs (Gpt3.5, Llama2, and Tinyllama) using two different human-written prompt styles for seven state-of-the-art quantum algorithms. We also analyse how consistent LLM explanations are over multiple rounds and how LLMs can improve existing descriptions of quantum algorithms.\n  Results: Llama2 provides the highest quality explanations from scratch, while Gpt3.5 emerged as the LLM best suited to improve existing explanations. In addition, we show that adding a small amount of context to the prompt significantly improves the quality of explanations. Finally, we observe how explanations are qualitatively and syntactically consistent over multiple rounds.\n  Conclusions: This work highlights promising results, and opens challenges for future research in the field of LLMs for quantum code explanation. Future work includes refining the methods through prompt optimisation and parsing of quantum code explanations, as well as carrying out a systematic assessment of the quality of explanations.",
        "Publication date": "26 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.19028"
    },
    {
        "ID": 22,
        "Title": "Not the Silver Bullet: LLM-enhanced Programming Error Messages are Ineffective in Practice",
        "Authors": [
            "Eddie Antonio Santos",
            "Brett A. Becker"
        ],
        "Abstract": "The sudden emergence of large language models (LLMs) such as ChatGPT has had a disruptive impact throughout the computing education community. LLMs have been shown to excel at producing correct code to CS1 and CS2 problems, and can even act as friendly assistants to students learning how to code. Recent work shows that LLMs demonstrate unequivocally superior results in being able to explain and resolve compiler error messages -- for decades, one of the most frustrating parts of learning how to code. However, LLM-generated error message explanations have only been assessed by expert programmers in artificial conditions. This work sought to understand how novice programmers resolve programming error messages (PEMs) in a more realistic scenario. We ran a within-subjects study with $n$ = 106 participants in which students were tasked to fix six buggy C programs. For each program, participants were randomly assigned to fix the problem using either a stock compiler error message, an expert-handwritten error message, or an error message explanation generated by GPT-4. Despite promising evidence on synthetic benchmarks, we found that GPT-4 generated error messages outperformed conventional compiler error messages in only 1 of the 6 tasks, measured by students' time-to-fix each problem. Handwritten explanations still outperform LLM and conventional error messages, both on objective and subjective measures.",
        "Publication date": "27 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.18661"
    },
    {
        "ID": 23,
        "Title": "Evaluation of OpenAI o1: Opportunities and Challenges of AGI",
        "Authors": [
            "Tianyang Zhong",
            "Zhengliang Liu",
            "Yi Pan",
            "Yutong Zhang",
            "Yifan Zhou",
            "Shizhe Liang",
            "Zihao Wu",
            "Yanjun Lyu",
            "Peng Shu",
            "Xiaowei Yu",
            "Chao Cao",
            "Hanqi Jiang",
            "Hanxu Chen",
            "Yiwei Li",
            "Junhao Chen",
            "Huawen Hu",
            "Yihen Liu",
            "Huaqin Zhao",
            "Shaochen Xu",
            "Haixing Dai",
            "Lin Zhao",
            "Ruidong Zhang",
            "Wei Zhao",
            "Zhenyuan Yang",
            "Jingyuan Chen"
        ],
        "Abstract": "This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include:\n  -83.3% success rate in solving complex competitive programming problems, surpassing many human experts.\n  -Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models.\n  -100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions.\n  -Advanced natural language inference capabilities across general and specialized domains like medicine.\n  -Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis.\n  -Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields.\n  -Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills.\n  -Effective performance in social media analysis, including sentiment analysis and emotion recognition.\n  The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence.",
        "Publication date": "27 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.18486"
    },
    {
        "ID": 24,
        "Title": "Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization",
        "Authors": [
            "Mucong Ding",
            "Chenghao Deng",
            "Jocelyn Choo",
            "Zichu Wu",
            "Aakriti Agrawal",
            "Avi Schwarzschild",
            "Tianyi Zhou",
            "Tom Goldstein",
            "John Langford",
            "Anima Anandkumar",
            "Furong Huang"
        ],
        "Abstract": "While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still blank. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six state-of-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.",
        "Publication date": "26 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.18433"
    },
    {
        "ID": 25,
        "Title": "Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents",
        "Authors": [
            "Junting Lu",
            "Zhiyang Zhang",
            "Fangkai Yang",
            "Jue Zhang",
            "Lu Wang",
            "Chao Du",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "Abstract": "Multimodal large language models (MLLMs) have enabled LLM-based agents to directly interact with application user interfaces (UIs), enhancing agents' performance in complex tasks. However, these agents often suffer from high latency and low reliability due to the extensive sequential UI interactions. To address this issue, we propose AXIS, a novel LLM-based agents framework prioritize actions through application programming interfaces (APIs) over UI actions. This framework also facilitates the creation and expansion of APIs through automated exploration of applications. Our experiments on Office Word demonstrate that AXIS reduces task completion time by 65%-70% and cognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans. Our work contributes to a new human-agent-computer interaction (HACI) framework and a fresh UI design principle for application providers in the era of LLMs. It also explores the possibility of turning every applications into agents, paving the way towards an agent-centric operating system (Agent OS).",
        "Publication date": "25 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.17140"
    },
    {
        "ID": 26,
        "Title": "Declarative Integration and Management of Large Language Models through Finite Automata: Application to Automation, Communication, and Ethics",
        "Authors": [
            "Thierry Petit",
            "Arnault Pachot",
            "Claire Conan-Vrinat",
            "Alexandre Dubarry"
        ],
        "Abstract": "This article introduces an innovative architecture designed to declaratively combine Large Language Models (LLMs) with shared histories, and triggers to identify the most appropriate LLM for a given task. Our approach is general and declarative, relying on the construction of finite automata coupled with an event management system. The developed tool is crafted to facilitate the efficient and complex integration of LLMs with minimal programming effort, especially, but not only, for integrating methods of positive psychology to AI. The flexibility of our technique is demonstrated through applied examples in automation, communication, and ethics.",
        "Publication date": "2 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.13693"
    },
    {
        "ID": 27,
        "Title": "Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts",
        "Authors": [
            "Ming Wang",
            "Yuanzhong Liu",
            "Xiaoyu Liang",
            "Yijie Huang",
            "Daling Wang",
            "Xiaocui Yang",
            "Sijia Shen",
            "Shi Feng",
            "Xiaoming Zhang",
            "Chaofeng Guan",
            "Yifei Zhang"
        ],
        "Abstract": "LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to assist them in their work poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structural design, incurring high learning costs and it is not conducive to the iterative updating of prompts, especially for non-AI experts. Inspired by structured reusable programming languages, we propose LangGPT, a structural prompt design framework. Furthermore, we introduce Minstrel, a multi-generative agent system with reflection to automate the generation of structural prompts. Experiments and the case study illustrate that structural prompts generated by Minstrel or written manually significantly enhance the performance of LLMs. Furthermore, we analyze the ease of use of structural prompts through a user survey in our online community.",
        "Publication date": "20 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.13449"
    },
    {
        "ID": 28,
        "Title": "Memory Consistency and Program Transformations",
        "Authors": [
            "Akshay Gopalakrishnan",
            "Clark Verbrugge",
            "Mark Batty"
        ],
        "Abstract": "A memory consistency model specifies the allowed behaviors of shared memory concurrent programs. At the language level, these models are known to have a non-trivial impact on the safety of program optimizations, limiting the ability to rearrange/refactor code without introducing new behaviors. Existing programming language memory models try to address this by permitting more (relaxed/weak) concurrent behaviors but are still unable to allow all the desired optimizations. A core problem is that weaker consistency models may also render optimizations unsafe, a conclusion that goes against the intuition of them allowing more behaviors. This exposes an open problem of the compositional interaction between memory consistency semantics and optimizations: which parts of the semantics correspond to allowing/disallowing which set of optimizations is unclear. In this work, we establish a formal foundation suitable enough to understand this compositional nature, decomposing optimizations into a finite set of elementary effects on program execution traces, over which aspects of safety can be assessed. We use this decomposition to identify a desirable compositional property (complete) that would guarantee the safety of optimizations from one memory model to another. We showcase its practicality by proving such a property between Sequential Consistency (SC) and $SC_{RR}$, the latter allowing independent read-read reordering over $SC$. Our work potentially paves way to a new design methodology of programming-language memory models, one that places emphasis on the optimizations desired to be performed.",
        "Publication date": "18 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.12013"
    },
    {
        "ID": 29,
        "Title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with Pythonic Syntax",
        "Authors": [
            "Augusto Seben da Rosa",
            "Marlon Daniel Angeli",
            "Jorge Aikes Junior",
            "Alef Iury Ferreira",
            "Lucas Rafael Gris",
            "Anderson da Silva Soares",
            "Arnaldo Candido Junior",
            "Frederico Santos de Oliveira",
            "Gabriel Trevisan Damke",
            "Rafael Teixeira Sousa"
        ],
        "Abstract": "We developed a jitted compiler for training Artificial Neural Networks using C++, LLVM and Cuda. It features object-oriented characteristics, strong typing, parallel workers for data pre-processing, pythonic syntax for expressions, PyTorch like model declaration and Automatic Differentiation. We implement the mechanisms of cache and pooling in order to manage VRAM, cuBLAS for high performance matrix multiplication and cuDNN for convolutional layers. Our experiments with Residual Convolutional Neural Networks on ImageNet, we reach similar speed but degraded performance. Also, the GRU network experiments show similar accuracy, but our compiler have degraded speed in that task. However, our compiler demonstrates promising results at the CIFAR-10 benchmark, in which we reach the same performance and about the same speed as PyTorch. We make the code publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
        "Publication date": "17 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.11600"
    },
    {
        "ID": 30,
        "Title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming",
        "Authors": [
            "Chalamalasetti Kranti",
            "Sherzod Hakimov",
            "David Schlangen"
        ],
        "Abstract": "While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there. ``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs. To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation. As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios. In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language. We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code. With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples. Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops).",
        "Publication date": "18 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.11041"
    },
    {
        "ID": 31,
        "Title": "Context-aware Code Segmentation for C-to-Rust Translation using Large Language Models",
        "Authors": [
            "Momoko Shiraishi",
            "Takahiro Shinagawa"
        ],
        "Abstract": "There is strong motivation to translate C code into Rust code due to the continuing threat of memory safety vulnerabilities in existing C programs and the significant attention paid to Rust as an alternative to the C language. While large language models (LLMs) show promise for automating this translation by generating more natural and safer code than rule-based methods, previous studies have shown that LLM-generated Rust code often fails to compile, even for relatively small C programs, due to significant differences between the two languages and context window limitations. We propose an LLM-based translation scheme that improves the success rate of translating large-scale C code into compilable Rust code. Our approach involves three key techniques: (1) pre-processing the C code to better align its structure and expressions with Rust, (2) segmenting the code into optimally sized translation units to avoid exceeding the LLM's context window limits, and (3) iteratively compiling and repairing errors while maintaining consistency between translation units using context-supplementing prompts. Compilation success is an essential first step in achieving functional equivalence, as only compilable code can be further tested. In experiments with 20 benchmark C programs, including those exceeding 4 kilo lines of code, we successfully translated all programs into compilable Rust code without losing corresponding parts of the original code.",
        "Publication date": "16 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.10506"
    },
    {
        "ID": 32,
        "Title": "Overcoming linguistic barriers in code assistants: creating a QLoRA adapter to improve support for Russian-language code writing instructions",
        "Authors": [
            "C. B. Pronin",
            "A. V. Volosova",
            "A. V. Ostroukh",
            "Yu. N. Strogov"
        ],
        "Abstract": "In this paper, an approach to training and evaluating an adapter model for the popular language model \"zephyr-7b-beta\" is described. The adapter was developed to improve the performance of the base model in tasks related to programming and understanding the Russian language. Considering the high quality of the original model in tasks in the English language, the goal of the research was to expand its linguistic and technical spectrum. The proposed adapter was trained using a large and diverse dataset, including question-answer pairs related to programming, as well code-related texts in Russian language. The applied training methodology ensures an improvement in the model's quality of answers in understanding and generating Python code based on Russian instructions. We evaluated the performance of the base model with the installed adapter using various metrics, comparing it to the base model as well as other state-of-the-art models in this field. The obtained results showed significant improvement, both in tasks related to writing Python code and in processing the Russian language, confirming the effectiveness of the proposed adapter.",
        "Publication date": "14 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.09353"
    },
    {
        "ID": 33,
        "Title": "Developing an Interactive OpenMP Programming Book with Large Language Models",
        "Authors": [
            "Xinyao Yi",
            "Anjia Wang",
            "Yonghong Yan",
            "Chunhua Liao"
        ],
        "Abstract": "This paper presents an approach to authoring a textbook titled Interactive OpenMP Programming with the assistance of Large Language Models (LLMs). The writing process utilized state-of-the-art LLMs, including Gemini Pro 1.5, Claude 3, and ChatGPT-4, to generate the initial structure and outline of the book, as well as the initial content for specific chapters. This content included detailed descriptions of individual OpenMP constructs and practical programming examples. The outline and content have then undergone extensive manual revisions to meet our book goals. In this paper, we report our findings about the capabilities and limitations of these LLMs. We address critical questions concerning the necessity of textbook resources and the effectiveness of LLMs in creating fundamental and practical programming content. Our findings suggest that while LLMs offer significant advantages in generating textbook content, they require careful integration with traditional educational methodologies to ensure depth, accuracy, and pedagogical effectiveness. The Interactive OpenMP Programming book is developed with the framework of Jupyter Book, enabling the execution of code within the book from the web browser, providing instant feedback and a dynamic learning experience that stands in contrast to traditional educational resources. The book represents a significant step towards modernizing programming education, offering insights into practical strategies for generating the textbook through advanced AI tools.",
        "Publication date": "14 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.09296"
    },
    {
        "ID": 34,
        "Title": "Evaluating the Performance of Large Language Models in Competitive Programming: A Multi-Year, Multi-Grade Analysis",
        "Authors": [
            "Adrian Marius Dumitran",
            "Adrian Catalin Badea",
            "Stefan-Gabriel Muscalu"
        ],
        "Abstract": "This study explores the performance of large language models (LLMs) in solving competitive programming problems from the Romanian Informatics Olympiad at the county level. Romania, a leading nation in computer science competitions, provides an ideal environment for evaluating LLM capabilities due to its rich history and stringent competition standards. We collected and analyzed a dataset comprising 304 challenges from 2002 to 2023, focusing on solutions written by LLMs in C++ and Python for these problems. Our primary goal is to understand why LLMs perform well or poorly on different tasks. We evaluated various models, including closed-source models like GPT-4 and open-weight models such as CodeLlama and RoMistral, using a standardized process involving multiple attempts and feedback rounds. The analysis revealed significant variations in LLM performance across different grades and problem types. Notably, GPT-4 showed strong performance, indicating its potential use as an educational tool for middle school students. We also observed differences in code quality and style across various LLMs",
        "Publication date": "31 August, 2024",
        "Link": "https://arxiv.org/pdf/2409.09054"
    },
    {
        "ID": 35,
        "Title": "Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols",
        "Authors": [
            "Charlie Griffin",
            "Louis Thomson",
            "Buck Shlegeris",
            "Alessandro Abate"
        ],
        "Abstract": "To evaluate the safety and usefulness of deployment protocols for untrusted AIs, AI Control uses a red-teaming exercise played between a protocol designer and an adversary. This paper introduces AI-Control Games, a formal decision-making model of the red-teaming exercise as a multi-objective, partially observable, stochastic game. We also introduce methods for finding optimal protocols in AI-Control Games, by reducing them to a set of zero-sum partially observable stochastic games. We apply our formalism to model, evaluate and synthesise protocols for deploying untrusted language models as programming assistants, focusing on Trusted Monitoring protocols, which use weaker language models and limited human assistance. Finally, we demonstrate the utility of our formalism by showcasing improvements over empirical studies in existing settings, evaluating protocols in new settings, and analysing how modelling assumptions affect the safety and usefulness of protocols.",
        "Publication date": "12 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.07985"
    },
    {
        "ID": 36,
        "Title": "CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification",
        "Authors": [
            "Zeqing Qin",
            "Yiwei Wu",
            "Lansheng Han"
        ],
        "Abstract": "Large Language Models (LLMs) have shown great promise in vulnerability identification. As C/C++ comprises half of the Open-Source Software (OSS) vulnerabilities over the past decade and updates in OSS mainly occur through commits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing Commits (VCCs) is essential. However, current studies primarily focus on further pre-training LLMs on massive code datasets, which is resource-intensive and poses efficiency challenges. In this paper, we enhance the ability of BERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose CodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++ programs and LLMs. Based on commits, CLNX efficiently converts the source code into a more natural representation while preserving key details. Specifically, CLNX first applies structure-level naturalization to decompose complex programs, followed by token-level naturalization to interpret complex symbols. We evaluate CLNX on public datasets of 25,872 C/C++ functions with their commits. The results show that CLNX significantly enhances the performance of LLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new state-of-the-art and identifies 38 OSS vulnerabilities in the real world.",
        "Publication date": "11 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.07407"
    },
    {
        "ID": 37,
        "Title": "SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs",
        "Authors": [
            "Wanli Qian",
            "Chenfeng Gao",
            "Anup Sathya",
            "Ryo Suzuki",
            "Ken Nakagaki"
        ],
        "Abstract": "This paper introduces text-to-shape-display, a novel approach to generating dynamic shape changes in pin-based shape displays through natural language commands. By leveraging large language models (LLMs) and AI-chaining, our approach allows users to author shape-changing behaviors on demand through text prompts without programming. We describe the foundational aspects necessary for such a system, including the identification of key generative elements (primitive, animation, and interaction) and design requirements to enhance user interaction, based on formative exploration and iterative design processes. Based on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a 24 x 24 shape display, which translates the user's textual command into executable code and allows for quick exploration through a web-based control interface. We evaluate the effectiveness of SHAPE-IT in two ways: 1) performance evaluation and 2) user evaluation (N= 10). The study conclusions highlight the ability to facilitate rapid ideation of a wide range of shape-changing behaviors with AI. However, the findings also expose accuracy-related challenges and limitations, prompting further exploration into refining the framework for leveraging AI to better suit the unique requirements of shape-changing systems.",
        "Publication date": "10 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.06205"
    },
    {
        "ID": 38,
        "Title": "Advancing Automated Knowledge Transfer in Evolutionary Multitasking via Large Language Models",
        "Authors": [
            "Yuxiao Huang",
            "Xuebin Lv",
            "Shenghao Wu",
            "Jibin Wu",
            "Liang Feng",
            "Kay Chen Tan"
        ],
        "Abstract": "Evolutionary Multi-task Optimization (EMTO) is a paradigm that leverages knowledge transfer across simultaneously optimized tasks for enhanced search performance. To facilitate EMTO's performance, various knowledge transfer models have been developed for specific optimization tasks. However, designing these models often requires substantial expert knowledge. Recently, large language models (LLMs) have achieved remarkable success in autonomous programming, aiming to produce effective solvers for specific problems. In this work, a LLM-based optimization paradigm is introduced to establish an autonomous model factory for generating knowledge transfer models, ensuring effective and efficient knowledge transfer across various optimization tasks. To evaluate the performance of the proposed method, we conducted comprehensive empirical studies comparing the knowledge transfer model generated by the LLM with existing state-of-the-art knowledge transfer methods. The results demonstrate that the generated model is able to achieve superior or competitive performance against hand-crafted knowledge transfer models in terms of both efficiency and effectiveness.",
        "Publication date": "6 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.04270"
    },
    {
        "ID": 39,
        "Title": "Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language Models for Text-to-Code Generation",
        "Authors": [
            "Luis Mayer",
            "Christian Heumann",
            "Matthias AÃenmacher"
        ],
        "Abstract": "In recent years, large language models (LLMs) have emerged as powerful tools with potential applications in various fields, including software engineering. Within the scope of this research, we evaluate five different state-of-the-art LLMs - Bard, BingChat, ChatGPT, Llama2, and Code Llama - concerning their capabilities for text-to-code generation. In an empirical study, we feed prompts with textual descriptions of coding problems sourced from the programming website LeetCode to the models with the task of creating solutions in Python. Subsequently, the quality of the generated outputs is assessed using the testing functionalities of LeetCode. The results indicate large differences in performance between the investigated models. ChatGPT can handle these typical programming challenges by far the most effectively, surpassing even code-specialized models like Code Llama. To gain further insights, we measure the runtime as well as the memory usage of the generated outputs and compared them to the other code submissions on Leetcode. A detailed error analysis, encompassing a comparison of the differences concerning correct indentation and form of the generated code as well as an assignment of the incorrectly solved tasks to certain error categories allows us to obtain a more nuanced picture of the results and potential for improvement. The results also show a clear pattern of increasingly incorrect produced code when the models are facing a lot of context in the form of longer prompts.",
        "Publication date": "6 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.04164"
    },
    {
        "ID": 40,
        "Title": "No Man is an Island: Towards Fully Automatic Programming by Code Search, Code Generation and Program Repair",
        "Authors": [
            "Quanjun Zhang",
            "Chunrong Fang",
            "Ye Shang",
            "Tongke Zhang",
            "Shengcheng Yu",
            "Zhenyu Chen"
        ],
        "Abstract": "Automatic programming attempts to minimize human intervention in the generation of executable code, and has been a long-standing challenge in the software engineering community. To advance automatic programming, researchers are focusing on three primary directions: (1) code search that reuses existing code snippets from external databases; (2) code generation that produces new code snippets from natural language; and (3) program repair that refines existing code snippets by fixing detected bugs. Despite significant advancements, the effectiveness of state-of-the-art techniques is still limited, such as the usability of searched code and the correctness of generated code.\n  Motivated by the real-world programming process, where developers usually use various external tools to aid their coding processes, such as code search engines and code testing tools, in this work, we propose \\toolname{}, an automatic programming framework that leverages recent large language models (LLMs) to integrate the three research areas to address their inherent limitations. In particular, our framework first leverages different code search strategies to retrieve similar code snippets, which are then used to further guide the code generation process of LLMs. Our framework further validates the quality of generated code by compilers and test cases, and constructs repair prompts to query LLMs for generating correct patches. We conduct preliminary experiments to demonstrate the potential of our framework, \\eg helping CodeLlama solve 267 programming problems with an improvement of 62.53\\%. As a generic framework, \\toolname{} can integrate various code search, generation, and repair tools, combining these three research areas together for the first time. More importantly, it demonstrates the potential of using traditional SE tools to enhance the usability of LLMs in automatic programming.",
        "Publication date": "5 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.03267"
    },
    {
        "ID": 41,
        "Title": "Multi-language Unit Test Generation using LLMs",
        "Authors": [
            "Rangeet Pan",
            "Myeongsoo Kim",
            "Rahul Krishna",
            "Raju Pavuluri",
            "Saurabh Sinha"
        ],
        "Abstract": "Implementing automated unit tests is an important but time consuming activity in software development. Developers dedicate substantial time to writing tests for validating an application and preventing regressions. To support developers in this task, software engineering research over the past few decades has developed many techniques for automating unit test generation. However, despite this effort, usable tools exist for very few programming languages -- mainly Java, C, and C# and, more recently, for Python. Moreover, studies have found that automatically generated tests suffer poor readability and often do not resemble developer-written tests. In this work, we present a rigorous investigation of how large language models (LLMs) can help bridge the gap. We describe a generic pipeline that incorporates static analysis to guide LLMs in generating compilable and high-coverage test cases. We illustrate how the pipeline can be applied to different programming languages, specifically Java and Python, and to complex software requiring environment mocking. We conducted a through empirical study to assess the quality of the generated tests in terms of coverage, mutation score, and test naturalness -- evaluating them on standard as well as enterprise Java applications and a large Python benchmark. Our results demonstrate that LLM-based test generation, when guided by static analysis, can be competitive with, and even outperform, state-of-the-art test-generation techniques in coverage achieved while also producing considerably more natural test cases that developers find easy to read and understand. We also present the results of a user study, conducted with 161 professional developers, that highlights the naturalness characteristics of the tests generated by our approach.",
        "Publication date": "4 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.03093"
    },
    {
        "ID": 42,
        "Title": "Debugging with Open-Source Large Language Models: An Evaluation",
        "Authors": [
            "Yacine Majdoub",
            "Eya Ben Charrada"
        ],
        "Abstract": "Large language models have shown good potential in supporting software development tasks. This is why more and more developers turn to LLMs (e.g. ChatGPT) to support them in fixing their buggy code. While this can save time and effort, many companies prohibit it due to strict code sharing policies. To address this, companies can run open-source LLMs locally. But until now there is not much research evaluating the performance of open-source large language models in debugging. This work is a preliminary evaluation of the capabilities of open-source LLMs in fixing buggy code. The evaluation covers five open-source large language models and uses the benchmark DebugBench which includes more than 4000 buggy code instances written in Python, Java and C++. Open-source LLMs achieved scores ranging from 43.9% to 66.6% with DeepSeek-Coder achieving the best score for all three programming languages.",
        "Publication date": "4 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.03031"
    },
    {
        "ID": 43,
        "Title": "An Array Intermediate Language for Mixed Cryptography",
        "Authors": [
            "Vivian Ding",
            "CoÅku Acay",
            "Andrew C. Myers"
        ],
        "Abstract": "We introduce AIRduct, a new array-based intermediate representation designed to support generating efficient code for interactive programs employing multiple cryptographic mechanisms. AIRduct is intended as an IR for the Viaduct compiler, which can synthesize secure, distributed programs with an extensible suite of cryptography. Therefore, AIRduct supports an extensible variety of cryptographic mechanisms, including MPC and ZKP.",
        "Publication date": "3 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.01587"
    },
    {
        "ID": 44,
        "Title": "Statically Contextualizing Large Language Models with Typed Holes",
        "Authors": [
            "Andrew Blinn",
            "Xiang Li",
            "June Hyung Kim",
            "Cyrus Omar"
        ],
        "Abstract": "Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate context, particularly when working with definitions not in the training data nor near the cursor. This paper demonstrates that tight integration with the type and binding structure of a language, as exposed by its language server, can address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server identifies the type and typing context of the hole being filled, even in the presence of errors, ensuring that a meaningful program sketch is always available. This allows prompting with codebase-wide contextual information not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications. These applications serve as challenge problems due to their reliance on application-specific data structures. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.",
        "Publication date": "1 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.00921"
    },
    {
        "ID": 45,
        "Title": "Benchmarking LLM Code Generation for Audio Programming with Visual Dataflow Languages",
        "Authors": [
            "William Zhang",
            "Maria Leon",
            "Ryan Xu",
            "Adrian Cardenas",
            "Amelia Wissink",
            "Hanna Martin",
            "Maya Srikanth",
            "Kaya Dorogi",
            "Christian Valadez",
            "Pedro Perez",
            "Citlalli Grijalva",
            "Corey Zhang",
            "Mark Santolucito"
        ],
        "Abstract": "Node-based programming languages are increasingly popular in media arts coding domains. These languages are designed to be accessible to users with limited coding experience, allowing them to achieve creative output without an extensive programming background. Using LLM-based code generation to further lower the barrier to creative output is an exciting opportunity. However, the best strategy for code generation for visual node-based programming languages is still an open question. In particular, such languages have multiple levels of representation in text, each of which may be used for code generation. In this work, we explore the performance of LLM code generation in audio programming tasks in visual programming languages at multiple levels of representation. We explore code generation through metaprogramming code representations for these languages (i.e., coding the language using a different high-level text-based programming language), as well as through direct node generation with JSON. We evaluate code generated in this way for two visual languages for audio programming on a benchmark set of coding problems. We measure both correctness and complexity of the generated code. We find that metaprogramming results in more semantically correct generated code, given that the code is well-formed (i.e., is syntactically correct and runs). We also find that prompting for richer metaprogramming using randomness and loops led to more complex code.",
        "Publication date": "1 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.00856"
    },
    {
        "ID": 46,
        "Title": "Welding R and C++: A Tale of Two Programming Languages",
        "Authors": [
            "Mauricio Vargas Sepulveda"
        ],
        "Abstract": "This article compares `cpp11armadillo` and `cpp11eigen`, new R packages that integrate the powerful Armadillo and Eigen C++ libraries for linear algebra into the R programming environment. This article provides a detailed comparison between Armadillo and Eigen speed and syntax. The goal of these packages is to simplify a part of the process of solving bottlenecks by using C++ within R, these offer additional ease of integration for users who require high-performance linear algebra operations in their R workflows. This document aims to discuss the tradeoff between computational efficiency and accessibility.",
        "Publication date": "7 September, 2024",
        "Link": "https://arxiv.org/pdf/2409.00568"
    },
    {
        "ID": 47,
        "Title": "Extending the C/C++ Memory Model with Inline Assembly",
        "Authors": [
            "Paulo EmÃ­lio de Vilhena",
            "Ori Lahav",
            "Viktor Vafeiadis",
            "Azalea Raad"
        ],
        "Abstract": "Programs written in C/C++ often include inline assembly: a snippet of architecture-specific assembly code used to access low-level functionalities that are impossible or expensive to simulate in the source language. Although inline assembly is widely used, its semantics has not yet been formally studied.\n  In this paper, we overcome this deficiency by investigating the effect of inline assembly on the consistency semantics of C/C++ programs. We propose the first memory model of the C++ Programming Language with support for inline assembly for Intel's x86 including non-temporal stores and store fences. We argue that previous provably correct compiler optimizations and correct compiler mappings should remain correct under such an extended model and we prove that this requirement is met by our proposed model.",
        "Publication date": "2 September, 2024",
        "Link": "https://arxiv.org/pdf/2408.17208"
    },
    {
        "ID": 48,
        "Title": "Automating Pruning in Top-Down Enumeration for Program Synthesis Problems with Monotonic Semantics",
        "Authors": [
            "Keith J. C. Johnson",
            "Rahul Krishnan",
            "Thomas Reps",
            "Loris D'Antoni"
        ],
        "Abstract": "In top-down enumeration for program synthesis, abstraction-based pruning uses an abstract domain to approximate the set of possible values that a partial program, when completed, can output on a given input. If the set does not contain the desired output, the partial program and all its possible completions can be pruned. In its general form, abstraction-based pruning requires manually designed, domain-specific abstract domains and semantics, and thus has only been used in domain-specific synthesizers.\n  This paper provides sufficient conditions under which a form of abstraction-based pruning can be automated for arbitrary synthesis problems in the general-purpose Semantics-Guided Synthesis (SemGuS) framework without requiring manually-defined abstract domains. We show that if the semantics of the language for which we are synthesizing programs exhibits some monotonicity properties, one can obtain an abstract interval-based semantics for free from the concrete semantics of the programming language, and use such semantics to effectively prune the search space. We also identify a condition that ensures such abstract semantics can be used to compute a precise abstraction of the set of values that a program derivable from a given hole in a partial program can produce. These precise abstractions make abstraction-based pruning more effective.\n  We implement our approach in a tool, Moito, which can tackle synthesis problems defined in the SemGuS framework. Moito can automate interval-based pruning without any a-priori knowledge of the problem domain, and solve synthesis problems that previously required domain-specific, abstraction-based synthesizers -- e.g., synthesis of regular expressions, CSV file schema, and imperative programs from examples.",
        "Publication date": "28 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.15822"
    },
    {
        "ID": 49,
        "Title": "Verifying Solutions to Semantics-Guided Synthesis Problems",
        "Authors": [
            "Charlie Murphy",
            "Keith Johnson",
            "Thomas Reps",
            "Loris D'Antoni"
        ],
        "Abstract": "Semantics-Guided Synthesis (SemGuS) provides a framework to specify synthesis problems in a solver-agnostic and domain-agnostic way, by allowing a user to provide both the syntax and semantics of the language in which the desired program should be synthesized. Because synthesis and verification are closely intertwined, the SemGuS framework raises the problem of how to verify programs in a solver and domain-agnostic way.\n  We prove that the problem of verifying whether a program is a valid solution to a SemGuS problem can be reduced to proving validity of a query in the `CLP calculus, a fixed-point logic that generalizes Constrained Horn Clauses and co-Constrained Horn Clauses. Our encoding into `CLP allows us to further classify the SemGuS verification problems into ones that are reducible to validity of (i) first-order-logic formulas, (ii) Constrained Horn Clauses, (iii) co-Constrained Horn Clauses, and (iv) `CLP queries. Furthermore, our encoding shines light on some limitations of the SemGuS framework, such as its inability to model nondeterminism and reactive synthesis. We thus propose a modification to SemGuS that makes it more expressive, and for which verifying solutions is exactly equivalent to proving validity of a query in the `CLP calculus. Our implementation of SemGuS verifiers based on the above encoding can verify instances that were not even encodable in previous work. Furthermore, we use our SemGuS verifiers within an enumeration-based SemGuS solver to correctly synthesize solutions to SemGuS problems that no previous SemGuS synthesizer could solve.",
        "Publication date": "27 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.15475"
    },
    {
        "ID": 50,
        "Title": "Synthesizing Formal Semantics from Executable Interpreters",
        "Authors": [
            "Jiangyi Liu",
            "Charlie Murphy",
            "Anvay Grover",
            "Keith J. C. Johnson",
            "Thomas Reps",
            "Loris D'Antoni"
        ],
        "Abstract": "Program verification and synthesis frameworks that allow one to customize the language in which one is interested typically require the user to provide a formally defined semantics for the language. Because writing a formal semantics can be a daunting and error-prone task, this requirement stands in the way of such frameworks being adopted by non-expert users. We present an algorithm that can automatically synthesize inductively defined syntax-directed semantics when given (i) a grammar describing the syntax of a language and (ii) an executable (closed-box) interpreter for computing the semantics of programs in the language of the grammar. Our algorithm synthesizes the semantics in the form of Constrained-Horn Clauses (CHCs), a natural, extensible, and formal logical framework for specifying inductively defined relations that has recently received widespread adoption in program verification and synthesis. The key innovation of our synthesis algorithm is a Counterexample-Guided Synthesis (CEGIS) approach that breaks the hard problem of synthesizing a set of constrained Horn clauses into small, tractable expression-synthesis problems that can be dispatched to existing SyGuS synthesizers. Our tool Synantic synthesized inductively-defined formal semantics from 14 interpreters for languages used in program-synthesis applications. When synthesizing formal semantics for one of our benchmarks, Synantic unveiled an inconsistency in the semantics computed by the interpreter for a language of regular expressions; fixing the inconsistency resulted in a more efficient semantics and, for some cases, in a 1.2x speedup for a synthesizer solving synthesis problems over such a language.",
        "Publication date": "6 September, 2024",
        "Link": "https://arxiv.org/pdf/2408.14668"
    },
    {
        "ID": 51,
        "Title": "MetaFFI -- Multilingual Indirect Interoperability System",
        "Authors": [
            "Tsvi Cherny-Shahar",
            "Amiram Yehudai"
        ],
        "Abstract": "The development of software applications using multiple programming languages has increased in recent years, as it allows the selection of the most suitable language and runtime for each component of the system and the integration of third-party libraries. However, this practice involves complexity and error proneness, due to the absence of an adequate system for the interoperability of multiple programming languages. Developers are compelled to resort to workarounds, such as library reimplementation or language-specific wrappers, which are often dependent on C as the common denominator for interoperability. These challenges render the use of multiple programming languages a burdensome and demanding task that necessitates highly skilled developers for implementation, debugging, and maintenance, and raise doubts about the benefits of interoperability. To overcome these challenges, we propose MetaFFI, a pluggable in-process indirect-interoperability system that allows the loading and utilization of entities from multiple programming languages. This is achieved by exploiting the less restrictive shallow binding mechanisms (e.g., Foreign Function Interface) to offer deep binding features (e.g., object creation, methods, fields). MetaFFI provides a runtime-independent framework to load and \\emph{xcall} (Cross-Call) foreign entities (e.g., functions, objects). MetaFFI uses Common Data Types (CDTs) to pass parameters and return values, including objects and complex types, and even cross-language callbacks. The indirect interoperability approach of MetaFFI has the significant advantage of requiring only $2n$ mechanisms to support $n$ languages, as opposed to the direct interoperability approaches that need $n^2$ mechanisms. We have successfully tested the binding between Go, Python3.11, and Java in a proof-of-concept on Windows and Ubuntu.",
        "Publication date": "26 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.14175"
    },
    {
        "ID": 52,
        "Title": "Making Formulog Fast: An Argument for Unconventional Datalog Evaluation (Extended Version)",
        "Authors": [
            "Aaron Bembenek",
            "Michael Greenberg",
            "Stephen Chong"
        ],
        "Abstract": "By combining Datalog, SMT solving, and functional programming, the language Formulog provides an appealing mix of features for implementing SMT-based static analyses (e.g., refinement type checking, symbolic execution) in a natural, declarative way. At the same time, the performance of its custom Datalog solver can be an impediment to using Formulog beyond prototyping -- a common problem for Datalog variants that aspire to solve large problem instances. In this work we speed up Formulog evaluation, with surprising results: while 2.2x speedups are obtained by using the conventional techniques for high-performance Datalog (e.g., compilation, specialized data structures), the big wins come by abandoning the central assumption in modern performant Datalog engines, semi-naive Datalog evaluation. In its place, we develop eager evaluation, a concurrent Datalog evaluation algorithm that explores the logical inference space via a depth-first traversal order. In practice, eager evaluation leads to an advantageous distribution of Formulog's SMT workload to external SMT solvers and improved SMT solving times: our eager evaluation extensions to the Formulog interpreter and SoufflÃ©'s code generator achieve mean 5.2x and 7.6x speedups, respectively, over the optimized code generated by off-the-shelf SoufflÃ© on SMT-heavy Formulog benchmarks.\n  Using compilation and eager evaluation, Formulog implementations of refinement type checking, bottom-up pointer analysis, and symbolic execution achieve speedups on 20 out of 23 benchmarks over previously published, hand-tuned analyses written in F#, Java, and C++, providing strong evidence that Formulog can be the basis of a realistic platform for SMT-based static analysis. Moreover, our experience adds nuance to the conventional wisdom that semi-naive evaluation is the one-size-fits-all best Datalog evaluation algorithm for static analysis workloads.",
        "Publication date": "26 September, 2024",
        "Link": "https://arxiv.org/pdf/2408.14017"
    },
    {
        "ID": 53,
        "Title": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates",
        "Authors": [
            "Zhihong Sun",
            "Yao Wan",
            "Jia Li",
            "Hongyu Zhang",
            "Zhi Jin",
            "Ge Li",
            "Chen Lyu"
        ],
        "Abstract": "Large Language Models (LLMs), such as GPT-4, StarCoder, and CodeLlama, are transforming the way developers approach programming by automatically generating code based on given natural language descriptions. Despite advancements, generating syntactically and semantically correct code remains challenging, especially for complex programming tasks. Existing approaches typically generate multiple candidate solutions using LLMs to increase the likelihood of producing correct code. However, selecting the correct code from these candidates-a process known as code ranking-remains a major challenge. Current research on code ranking can be categorized into execution-based and non-execution-based methods. Execution-based methods, although effective, encounter notable limitations, such as scarcity of quality unit tests and security risks. Non-execution-based methods like CodeRanker, which rely solely on classification labels to train a code ranker, struggle to capture subtle errors and provide detailed error insights. Recognizing the strengths and limitations of both approaches, we propose a new method. The key insight of our work is that an effective code ranker is expected to truly comprehend the underlying causes of erroneous code, as relying solely on classification labels is insufficient. Inspired by this, this paper puts forward RankEF, an innovative approach for code ranking that leverages execution feedback. RankEF employs multi-task learning to integrate code classification with execution feedback generation. This approach enables the model to understand the reasons behind incorrect code, distinguishing between correct and incorrect solutions without the need to execute the code during the ranking phase. Experiments on three code generation benchmarks demonstrate that RankEF significantly outperforms the state-of-the-art CodeRanker.",
        "Publication date": "19 September, 2024",
        "Link": "https://arxiv.org/pdf/2408.13976"
    },
    {
        "ID": 54,
        "Title": "Optimizing Collaboration of LLM based Agents for Finite Element Analysis",
        "Authors": [
            "Chuan Tian",
            "Yilei Zhang"
        ],
        "Abstract": "This paper investigates the interactions between multiple agents within Large Language Models (LLMs) in the context of programming and coding tasks. We utilize the AutoGen framework to facilitate communication among agents, evaluating different configurations based on the success rates from 40 random runs for each setup. The study focuses on developing a flexible automation framework for applying the Finite Element Method (FEM) to solve linear elastic problems. Our findings emphasize the importance of optimizing agent roles and clearly defining their responsibilities, rather than merely increasing the number of agents. Effective collaboration among agents is shown to be crucial for addressing general FEM challenges. This research demonstrates the potential of LLM multi-agent systems to enhance computational automation in simulation methodologies, paving the way for future advancements in engineering and artificial intelligence.",
        "Publication date": "23 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.13406"
    },
    {
        "ID": 55,
        "Title": "A Web-Based Solution for Federated Learning with LLM-Based Automation",
        "Authors": [
            "Chamith Mawela",
            "Chaouki Ben Issaid",
            "Mehdi Bennis"
        ],
        "Abstract": "Federated Learning (FL) offers a promising approach for collaborative machine learning across distributed devices. However, its adoption is hindered by the complexity of building reliable communication architectures and the need for expertise in both machine learning and network programming. This paper presents a comprehensive solution that simplifies the orchestration of FL tasks while integrating intent-based automation. We develop a user-friendly web application supporting the federated averaging (FedAvg) algorithm, enabling users to configure parameters through an intuitive interface. The backend solution efficiently manages communication between the parameter server and edge nodes. We also implement model compression and scheduling algorithms to optimize FL performance. Furthermore, we explore intent-based automation in FL using a fine-tuned Language Model (LLM) trained on a tailored dataset, allowing users to conduct FL tasks using high-level prompts. We observe that the LLM-based automated solution achieves comparable test accuracy to the standard web-based solution while reducing transferred bytes by up to 64% and CPU time by up to 46% for FL tasks. Also, we leverage the neural architecture search (NAS) and hyperparameter optimization (HPO) using LLM to improve the performance. We observe that by using this approach test accuracy can be improved by 10-20% for the carried out FL tasks.",
        "Publication date": "23 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.13010"
    },
    {
        "ID": 56,
        "Title": "CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution",
        "Authors": [
            "Ruiyang Xu",
            "Jialun Cao",
            "Yaojie Lu",
            "Hongyu Lin",
            "Xianpei Han",
            "Ben He",
            "Shing-Chi Cheung",
            "Le Sun"
        ],
        "Abstract": "Code benchmarks such as HumanEval are widely adopted to evaluate Large Language Models' (LLMs) coding capabilities. However, there is an unignorable programming language bias in existing code benchmarks -- over 95% code generation benchmarks are dominated by Python, leaving the LLMs' capabilities in other programming languages such as Java and C/C++ unknown. Moreover, coding task bias is also crucial. Most benchmarks focus on code generation capability, while benchmarks for code reasoning (given input, reasoning output; and given output, reasoning input), an essential coding capability, are insufficient. Yet, constructing multi-lingual benchmarks can be expensive and labor-intensive, and codes in contest websites such as Leetcode suffer from data contamination during training. To fill this gap, we propose CRUXEVAL-X, a multi-lingual code reasoning benchmark that contains 19 programming languages. It comprises at least 600 subjects for each language, along with 19K content-consistent tests in total. In particular, the construction pipeline of CRUXEVAL-X works in a fully automated and test-guided manner, which iteratively generates and repairs based on execution feedback. Also, to cross language barriers (e.g., dynamic/static type systems in Python/C++), we formulated various transition rules between language pairs to facilitate translation. Our intensive evaluation of 24 representative LLMs reveals the correlation between language pairs. For example, TypeScript and JavaScript show a significant positive correlation, while Racket has less correlation with other languages. More interestingly, even a model trained solely on Python can achieve at most 34.4% Pass@1 in other languages, revealing the cross-language generalization of LLMs.",
        "Publication date": "23 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.13001"
    },
    {
        "ID": 57,
        "Title": "Bridging the gap between natural user expression with complex automation programming in smart homes",
        "Authors": [
            "Yingtian Shi",
            "Xiaoyi Liu",
            "Chun Yu",
            "Tianao Yang",
            "Cheng Gao",
            "Chen Liang",
            "Yuanchun Shi"
        ],
        "Abstract": "A long-standing challenge in end-user programming (EUP) is to trade off between natural user expression and the complexity of programming tasks. As large language models (LLMs) are empowered to handle semantic inference and natural language understanding, it remains under-explored how such capabilities can facilitate end-users to configure complex automation more naturally and easily. We propose AwareAuto, an EUP system that standardizes user expression and finishes two-step inference with the LLMs to achieve automation generation. AwareAuto allows contextual, multi-modality, and flexible user expression to configure complex automation tasks (e.g., dynamic parameters, multiple conditional branches, and temporal constraints), which are non-manageable in traditional EUP solutions. By studying realistic, complex rules data, AwareAuto gains 91.7% accuracy in matching user intentions and feasibility. We introduced user interaction to ensure system controllability and usability. We discuss the opportunities and challenges of incorporating LLMs in end-user programming techniques and grounding complex smart home contexts.",
        "Publication date": "22 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.12687"
    },
    {
        "ID": 58,
        "Title": "PyMarian: Fast Neural Machine Translation and Evaluation in Python",
        "Authors": [
            "Thamme Gowda",
            "Roman Grundkiewicz",
            "Elijah Rippeth",
            "Matt Post",
            "Marcin Junczys-Dowmunt"
        ],
        "Abstract": "The deep learning language of choice these days is Python; measured by factors such as available libraries and technical support, it is hard to beat. At the same time, software written in lower-level programming languages like C++ retain advantages in speed. We describe a Python interface to Marian NMT, a C++-based training and inference toolkit for sequence-to-sequence models, focusing on machine translation. This interface enables models trained with Marian to be connected to the rich, wide range of tools available in Python. A highlight of the interface is the ability to compute state-of-the-art COMET metrics from Python but using Marian's inference engine, with a speedup factor of up to 7.8$\\times$ the existing implementations. We also briefly spotlight a number of other integrations, including Jupyter notebooks, connection with prebuilt models, and a web app interface provided with the package. PyMarian is available in PyPI via $\\texttt{pip install pymarian}$.",
        "Publication date": "14 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.11853"
    },
    {
        "ID": 59,
        "Title": "LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification Testsuites",
        "Authors": [
            "Zachariah Sollenberger",
            "Jay Patel",
            "Christian Munley",
            "Aaron Jarmusch",
            "Sunita Chandrasekaran"
        ],
        "Abstract": "Large Language Models (LLM) are evolving and have significantly revolutionized the landscape of software development. If used well, they can significantly accelerate the software development cycle. At the same time, the community is very cautious of the models being trained on biased or sensitive data, which can lead to biased outputs along with the inadvertent release of confidential information. Additionally, the carbon footprints and the un-explainability of these black box models continue to raise questions about the usability of LLMs.\n  With the abundance of opportunities LLMs have to offer, this paper explores the idea of judging tests used to evaluate compiler implementations of directive-based programming models as well as probe into the black box of LLMs. Based on our results, utilizing an agent-based prompting approach and setting up a validation pipeline structure drastically increased the quality of DeepSeek Coder, the LLM chosen for the evaluation purposes.",
        "Publication date": "21 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.11729"
    },
    {
        "ID": 60,
        "Title": "To Tag, or Not to Tag: Translating C's Unions to Rust's Tagged Unions",
        "Authors": [
            "Jaemin Hong",
            "Sukyoung Ryu"
        ],
        "Abstract": "Automatic C-to-Rust translation is a promising way to enhance the reliability of legacy system software. However, C2Rust, an industrially developed translator, generates Rust code with unsafe features, undermining the translation's objective. While researchers have proposed techniques to remove unsafe features in C2Rust-generated code, these efforts have targeted only a limited subset of unsafe features. One important unsafe feature remaining unaddressed is a union, a type consisting of multiple fields sharing the same memory storage. Programmers often place a union with a tag in a struct to record the last-written field, but they can still access wrong fields. In contrast, Rust's tagged unions combine tags and unions at the language level, ensuring correct value access. In this work, we propose techniques to replace unions with tagged unions during C-to-Rust translation. We develop a static analysis that facilitates such replacement by identifying tag fields and the corresponding tag values. The analysis involves a must-points-to analysis computing struct field values and a heuristic interpreting these results. To enhance efficiency, we adopt intraprocedural function-wise analysis, allowing selective analysis of functions. Our evaluation on 36 real-world C programs shows that the proposed approach is (1) precise, identifying 74 tag fields with no false positives and only five false negatives, (2) mostly correct, with 17 out of 23 programs passing tests post-transformation, and (3) efficient, capable of analyzing and transforming 141k LOC in 4,910 seconds.",
        "Publication date": "16 September, 2024",
        "Link": "https://arxiv.org/pdf/2408.11418"
    },
    {
        "ID": 61,
        "Title": "Implementing OpenMP for Zig to enable its use in HPC context",
        "Authors": [
            "David Kacs",
            "Nick Brown",
            "Joseph Lee"
        ],
        "Abstract": "This extended abstract explores supporting OpenMP in the Zig programming language. Whilst, C and Fortran are currently the main languages used to implement HPC applications, Zig provides a similar level of performance complimented with several modern language features, such as enforcing memory safety. However, Zig lacks support for OpenMP which is the de facto threaded programming technology.\n  Leveraging Zig's LLVM compiler tooling, we have added partial support for OpenMP to the Zig compiler and demonstrated that the performance attained by using Zig with OpenMP is comparable to, and in come cases exceeds, that of conventional HPC languages. Consequently we demonstrate that Zig is a viable and important programming technology to use for HPC, and this work paves the way for more HPC features to be added to Zig, ultimately providing HPC developers with the option of using a safer, more modern language for creating high performance applications.",
        "Publication date": "19 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.09902"
    },
    {
        "ID": 62,
        "Title": "Impact of Large Language Models of Code on Fault Localization",
        "Authors": [
            "Suhwan Ji",
            "Sanghwa Lee",
            "Changsup Lee",
            "Hyeonseung Im",
            "Yo-Sub Han"
        ],
        "Abstract": "Identifying the point of error is imperative in software debugging. Traditional fault localization (FL) techniques rely on executing the program and using the code coverage matrix in tandem with test case results to calculate a suspiciousness score for each function or line. Recently, learning-based FL techniques have harnessed machine learning models to extract meaningful features from the code coverage matrix and improve FL performance. These techniques, however, require compilable source code, existing test cases, and specialized tools for generating the code coverage matrix for each programming language of interest.\n  In this paper, we propose, for the first time, a simple but effective sequence generation approach for fine-tuning large language models of code (LLMCs) for FL tasks. LLMCs have recently received much attention for various software engineering problems. In line with these, we leverage the innate understanding of code that LLMCs have acquired through pre-training on large code corpora. Specifically, we fine-tune representative encoder, encoder-decoder, and decoder-based 13 LLMCs for FL tasks. Unlike previous approaches, LLMCs can analyze code sequences even with syntactic errors, since they do not rely on compiled input. Still, they have a limitation on the length of the input data. Therefore, for a fair comparison with existing FL techniques, we extract methods with errors from the project-level benchmark, Defects4J, and analyze them at the line level. Experimental results show that LLMCs fine-tuned with our approach successfully pinpoint error positions in 50.6\\%, 64.2\\%, and 72.3\\% of 1,291 methods in Defects4J for Top-1/3/5 prediction, outperforming the best learning-based state-of-the-art technique by up to 1.35, 1.12, and 1.08 times, respectively. Our findings suggest promising research directions for FL and automated program repair tasks using LLMCs.",
        "Publication date": "18 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.09657"
    },
    {
        "ID": 63,
        "Title": "Galapagos: Automated N-Version Programming with LLMs",
        "Authors": [
            "Javier Ron",
            "Diogo Gaspar",
            "Javier Cabrera-Arteaga",
            "Benoit Baudry",
            "Martin Monperrus"
        ],
        "Abstract": "One of the main challenges of N-Version Programming is development cost: it requires paying multiple teams to develop variants of the same system. To address this issue, we propose the automated generation of variants using large language models. We design, develop and evaluate GalÃ¡pagos: a tool for generating program variants using LLMs, validating their correctness and equivalence, and using them to assemble N-Version binaries. We evaluate GalÃ¡pagos by creating N-Version components of real-world C code. Our original results show that GalÃ¡pagos can produce program variants that are proven to be functionally equivalent, even when the variants are written in a different programming language. Our systematic diversity measurement indicate that functionally equivalent variants produced by GalÃ¡pagos, are statically different after compilation, and present diverging internal behavior at runtime. We demonstrate that the variants produced by GalÃ¡pagos can protect C code against real miscompilation bugs which affect the Clang compiler. Overall, our paper shows that producing N-Version software can be drastically automated by advanced usage of practical formal verification and generative language models.",
        "Publication date": "18 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.09536"
    },
    {
        "ID": 64,
        "Title": "SeeWasm: An Efficient and Fully-Functional Symbolic Execution Engine for WebAssembly Binaries",
        "Authors": [
            "Ningyu He",
            "Zhehao Zhao",
            "Hanqin Guan",
            "Jikai Wang",
            "Shuo Peng",
            "Ding Li",
            "Haoyu Wang",
            "Xiangqun Chen",
            "Yao Guo"
        ],
        "Abstract": "WebAssembly (Wasm), as a compact, fast, and isolation-guaranteed binary format, can be compiled from more than 40 high-level programming languages. However, vulnerabilities in Wasm binaries could lead to sensitive data leakage and even threaten their hosting environments. To identify them, symbolic execution is widely adopted due to its soundness and the ability to automatically generate exploitations. However, existing symbolic executors for Wasm binaries are typically platform-specific, which means that they cannot support all Wasm features. They may also require significant manual interventions to complete the analysis and suffer from efficiency issues as well. In this paper, we propose an efficient and fully-functional symbolic execution engine, named SeeWasm. Compared with existing tools, we demonstrate that SeeWasm supports full-featured Wasm binaries without further manual intervention, while accelerating the analysis by 2 to 6 times. SeeWasm has been adopted by existing works to identify more than 30 0-day vulnerabilities or security issues in well-known C, Go, and SGX applications after compiling them to Wasm binaries.",
        "Publication date": "16 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.08537"
    },
    {
        "ID": 65,
        "Title": "LLM-Enhanced Static Analysis for Precise Identification of Vulnerable OSS Versions",
        "Authors": [
            "Yiran Cheng",
            "Lwin Khin Shar",
            "Ting Zhang",
            "Shouguo Yang",
            "Chaopeng Dong",
            "David Lo",
            "Shichao Lv",
            "Zhiqiang Shi",
            "Limin Sun"
        ],
        "Abstract": "Open-source software (OSS) has experienced a surge in popularity, attributed to its collaborative development model and cost-effective nature. However, the adoption of specific software versions in development projects may introduce security risks when these versions bring along vulnerabilities. Current methods of identifying vulnerable versions typically analyze and trace the code involved in vulnerability patches using static analysis with pre-defined rules. They then use syntactic-level code clone detection to identify the vulnerable versions. These methods are hindered by imprecisions due to (1) the inclusion of vulnerability-irrelevant code in the analysis and (2) the inadequacy of syntactic-level code clone detection. This paper presents Vercation, an approach designed to identify vulnerable versions of OSS written in C/C++. Vercation combines program slicing with a Large Language Model (LLM) to identify vulnerability-relevant code from vulnerability patches. It then backtraces historical commits to gather previous modifications of identified vulnerability-relevant code. We propose semantic-level code clone detection to compare the differences between pre-modification and post-modification code, thereby locating the vulnerability-introducing commit (vic) and enabling to identify the vulnerable versions between the patch commit and the vic. We curate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate Vercation. On this dataset, our approach achieves the F1 score of 92.4%, outperforming current state-of-the-art methods. More importantly, Vercation detected 134 incorrect vulnerable OSS versions in NVD reports.",
        "Publication date": "14 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.07321"
    },
    {
        "ID": 66,
        "Title": "HLSPilot: LLM-based High-Level Synthesis",
        "Authors": [
            "Chenwei Xiong",
            "Cheng Liu",
            "Huawei Li",
            "Xiaowei Li"
        ],
        "Abstract": "Large language models (LLMs) have catalyzed an upsurge in automatic code generation, garnering significant attention for register transfer level (RTL) code generation. Despite the potential of RTL code generation with natural language, it remains error-prone and limited to relatively small modules because of the substantial semantic gap between natural language expressions and hardware design intent. In response to the limitations, we propose a methodology that reduces the semantic gaps by utilizing C/C++ for generating hardware designs via High-Level Synthesis (HLS) tools. Basically, we build a set of C-to-HLS optimization strategies catering to various code patterns, such as nested loops and local arrays. Then, we apply these strategies to sequential C/C++ code through in-context learning, which provides the LLMs with exemplary C/C++ to HLS prompts. With this approach, HLS designs can be generated effectively. Since LLMs still face problems in determining the optimized pragma parameters precisely, we have a design space exploration (DSE) tool integrated for pragma parameter tuning. Furthermore, we also employ profiling tools to pinpoint the performance bottlenecks within a program and selectively convert bottleneck components to HLS code for hardware acceleration. By combining the LLM-based profiling, C/C++ to HLS translation, and DSE, we have established HLSPilot, the first LLM-enabled high-level synthesis framework, which can fully automate the high-level application acceleration on hybrid CPU-FPGA architectures. According to our experiments on real-world application benchmarks, HLSPilot achieve comparable performance in general and can even outperform manually crafted counterparts, thereby underscoring the substantial promise of LLM-assisted hardware designs.",
        "Publication date": "13 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.06810"
    },
    {
        "ID": 67,
        "Title": "Large Language Models for Secure Code Assessment: A Multi-Language Empirical Study",
        "Authors": [
            "Kohei Dozono",
            "Tiago Espinha Gasiba",
            "Andrea Stocco"
        ],
        "Abstract": "Most vulnerability detection studies focus on datasets of vulnerabilities in C/C++ code, offering limited language diversity. Thus, the effectiveness of deep learning methods, including large language models (LLMs), in detecting software vulnerabilities beyond these languages is still largely unexplored. In this paper, we evaluate the effectiveness of LLMs in detecting and classifying Common Weakness Enumerations (CWE) using different prompt and role strategies. Our experimental study targets six state-of-the-art pre-trained LLMs (GPT-3.5- Turbo, GPT-4 Turbo, GPT-4o, CodeLLama-7B, CodeLLama- 13B, and Gemini 1.5 Pro) and five programming languages: Python, C, C++, Java, and JavaScript. We compiled a multi-language vulnerability dataset from different sources, to ensure representativeness. Our results showed that GPT-4o achieves the highest vulnerability detection and CWE classification scores using a few-shot setting. Aside from the quantitative results of our study, we developed a library called CODEGUARDIAN integrated with VSCode which enables developers to perform LLM-assisted real-time vulnerability analysis in real-world security scenarios. We have evaluated CODEGUARDIAN with a user study involving 22 developers from the industry. Our study showed that, by using CODEGUARDIAN, developers are more accurate and faster at detecting vulnerabilities.",
        "Publication date": "12 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.06428"
    },
    {
        "ID": 68,
        "Title": "ViC: Virtual Compiler Is All You Need For Assembly Code Search",
        "Authors": [
            "Zeyu Gao",
            "Hao Wang",
            "Yuanda Wang",
            "Chao Zhang"
        ],
        "Abstract": "Assembly code search is vital for reducing the burden on reverse engineers, allowing them to quickly identify specific functions using natural language within vast binary programs. Despite its significance, this critical task is impeded by the complexities involved in building high-quality datasets. This paper explores training a Large Language Model (LLM) to emulate a general compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC), capable of compiling any source code of any language to assembly code. This approach allows for virtual compilation across a wide range of programming languages without the need for a real compiler, preserving semantic equivalency and expanding the possibilities for assembly code dataset construction. Furthermore, we use ViC to construct a sufficiently large dataset for assembly code search. Employing this extensive dataset, we achieve a substantial improvement in assembly code search performance, with our model surpassing the leading baseline by 26%.",
        "Publication date": "10 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.06385"
    },
    {
        "ID": 69,
        "Title": "Implementing and Executing Static Analysis Using LLVM and CodeChecker",
        "Authors": [
            "Gabor Horvath",
            "Reka Kovacs",
            "Richard Szalay",
            "Zoltan Porkolab"
        ],
        "Abstract": "Static analysis is a method of analyzing source code without executing it. It is widely used to find bugs and code smells in industrial software. Besides other methods, the most important techniques are those based on the abstract syntax tree and those performing symbolic execution. Both of these methods found their role in modern software development as they have different advantages and limitations. In this tutorial, we present two problems from the C++ programming language: the elimination of redundant pointers, and the reporting of dangling pointers originating from incorrect use of the std::string class. These two issues have different theoretical backgrounds and finding them requires different implementation techniques. We will provide a step-by-step guide to implement the checkers (software to identify the aforementioned problems) - one based on the abstract syntax analysis method, the other exploring the possibilities of symbolic execution. The methods are explained in great detail and supported by code examples. The intended audience for this tutorial are both architects of static analysis tools and developers who want to understand the advantages and constraints of the different methods.",
        "Publication date": "10 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.05657"
    },
    {
        "ID": 70,
        "Title": "Natural Language Outlines for Code: Literate Programming in the LLM Era",
        "Authors": [
            "Kensen Shi",
            "Deniz AltÄ±nbÃ¼ken",
            "Saswat Anand",
            "Mihai Christodorescu",
            "Katja GrÃ¼nwedel",
            "Alexa Koenings",
            "Sai Naidu",
            "Anurag Pathak",
            "Marc Rasi",
            "Fredde Ribeiro",
            "Brandon Ruffin",
            "Siddhant Sanyam",
            "Maxim Tabachnyk",
            "Sara Toth",
            "Roy Tu",
            "Tobias Welp",
            "Pengcheng Yin",
            "Manzil Zaheer",
            "Satish Chandra",
            "Charles Sutton"
        ],
        "Abstract": "We propose using natural language outlines as a novel modality and interaction surface for providing AI assistance to developers throughout the software development process. An NL outline for a code function comprises multiple statements written in concise prose, which partition the code and summarize its main ideas in the style of literate programming. Crucially, we find that modern LLMs can generate accurate and high-quality NL outlines in practice. Moreover, NL outlines enable a bidirectional sync between code and NL, allowing changes in one to be automatically reflected in the other. We discuss many use cases for NL outlines: they can accelerate understanding and navigation of code and diffs, simplify code maintenance, augment code search, steer code generation, and more. We then propose and compare multiple LLM prompting techniques for generating outlines and ask professional developers to judge outline quality. Finally, we present two case studies applying NL outlines toward code review and the difficult task of malware detection.",
        "Publication date": "8 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.04820"
    },
    {
        "ID": 71,
        "Title": "On some randomized algorithms and their evaluation",
        "Authors": [
            "Krasimir Yordzhev"
        ],
        "Abstract": "The paper considers implementations of some randomized algorithms in connection with obtaining a random $n^2 \\times n^2$ Sudoku matrix with programming language C++. For this purpose we describes the set $Î _n$ of all $(2n) \\times n$ matrices, consisting of elements of the set $\\mathbb{Z}_n =\\{ 1,2,\\ldots ,n\\}$, such that every row is a permutation. We emphasize the relationship between these matrices and the $n^2 \\times n^2$ Sudoku matrices. An algorithm to obtain random $Î _n$ matrices is presented in this paper. Several auxiliary algorithms that are related to the underlying problem have been described. We evaluated all algorithms according to two criteria - probability evaluation, and time for generation of random objects and checking of belonging to a specific set. This evaluations are interesting from both theoretical and practical point of view because they are particularly useful in the analysis of computer programs.",
        "Publication date": "8 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.04445"
    },
    {
        "ID": 72,
        "Title": "Semantic-Enhanced Indirect Call Analysis with Large Language Models",
        "Authors": [
            "Baijun Cheng",
            "Cen Zhang",
            "Kailong Wang",
            "Ling Shi",
            "Yang Liu",
            "Haoyu Wang",
            "Yao Guo",
            "Xiangqun Chen"
        ],
        "Abstract": "In contemporary software development, the widespread use of indirect calls to achieve dynamic features poses challenges in constructing precise control flow graphs (CFGs), which further impacts the performance of downstream static analysis tasks. To tackle this issue, various types of indirect call analyzers have been proposed. However, they do not fully leverage the semantic information of the program, limiting their effectiveness in real-world scenarios. To address these issues, this paper proposes Semantic-Enhanced Analysis (SEA), a new approach to enhance the effectiveness of indirect call analysis. Our fundamental insight is that for common programming practices, indirect calls often exhibit semantic similarity with their invoked targets. This semantic alignment serves as a supportive mechanism for static analysis techniques in filtering out false targets. Notably, contemporary large language models (LLMs) are trained on extensive code corpora, encompassing tasks such as code summarization, making them well-suited for semantic analysis. Specifically, SEA leverages LLMs to generate natural language summaries of both indirect calls and target functions from multiple perspectives. Through further analysis of these summaries, SEA can determine their suitability as caller-callee pairs. Experimental results demonstrate that SEA can significantly enhance existing static analysis methods by producing more precise target sets for indirect calls.",
        "Publication date": "8 September, 2024",
        "Link": "https://arxiv.org/pdf/2408.04344"
    },
    {
        "ID": 73,
        "Title": "Ownership in low-level intermediate representation",
        "Authors": [
            "Siddharth Priya",
            "Arie Gurfinkel"
        ],
        "Abstract": "The concept of ownership in high level languages can aid both the programmer and the compiler to reason about the validity of memory operations. Previously, ownership semantics has been used successfully in high level automatic program verification to model a reference to data by a first order logic (FOL) representation of data instead of maintaining an address map. However, ownership semantics is not used in low level program verification. We have identified two challenges. First, ownership information is lost when a program is compiled to a low level intermediate representation (e.g., in LLVM IR). Second, pointers in low level programs point to bytes using an address map (e.g., in unsafe Rust) and thus the verification condition (VC) cannot always replace a pointer by its FOL abstraction. To remedy the situation, we develop ownership semantics for an LLVM like low level intermediate representation. Using these semantics, the VC can opportunistically model some memory accesses by a direct access of a pointer cache that stores byte representation of data. This scheme reduces instances where an address map must be maintained, especially for mostly safe programs that follow ownership semantics. For unsafe functionality, memory accesses are modelled by operations on an address map and we provide mechanisms to keep the address map and pointer cache in sync. We implement these semantics in SEABMC, a bit precise bounded model checker for LLVM. For evaluation, the source programs are assumed to be written in C. Since C does not have ownership built in, suitable macros are added that introduce and preserve ownership during translation to LLVM like IR for verification. This approach is evaluated on mature open source C code. For both handcrafted benchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT solving.",
        "Publication date": "13 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.04043"
    },
    {
        "ID": 74,
        "Title": "LLM-Aided Compilation for Tensor Accelerators",
        "Authors": [
            "Charles Hong",
            "Sahil Bhatia",
            "Altan Haan",
            "Shengjun Kris Dong",
            "Dima Nikiforov",
            "Alvin Cheung",
            "Yakun Sophia Shao"
        ],
        "Abstract": "Hardware accelerators, in particular accelerators for tensor processing, have many potential application domains. However, they currently lack the software infrastructure to support the majority of domains outside of deep learning. Furthermore, a compiler that can easily be updated to reflect changes at both application and hardware levels would enable more agile development and design space exploration of accelerators, allowing hardware designers to realize closer-to-optimal performance. In this work, we discuss how large language models (LLMs) could be leveraged to build such a compiler. Specifically, we demonstrate the ability of GPT-4 to achieve high pass rates in translating code to the Gemmini accelerator, and prototype a technique for decomposing translation into smaller, more LLM-friendly steps. Additionally, we propose a 2-phase workflow for utilizing LLMs to generate hardware-optimized code.",
        "Publication date": "6 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.03408"
    },
    {
        "ID": 75,
        "Title": "Evaluating Large Language Models for Automatic Register Transfer Logic Generation via High-Level Synthesis",
        "Authors": [
            "Sneha Swaroopa",
            "Rijoy Mukherjee",
            "Anushka Debnath",
            "Rajat Subhra Chakraborty"
        ],
        "Abstract": "The ever-growing popularity of large language models (LLMs) has resulted in their increasing adoption for hardware design and verification. Prior research has attempted to assess the capability of LLMs to automate digital hardware design by producing superior-quality Register Transfer Logic (RTL) descriptions, particularly in Verilog. However, these tests have revealed that Verilog code production using LLMs at current state-of-the-art lack sufficient functional correctness to be practically viable, compared to automatic generation of programs in general-purpose programming languages such as C, C++, Python, etc. With this as the key insight, in this paper we assess the performance of a two-stage software pipeline for automated Verilog RTL generation: LLM based automatic generation of annotated C++ code suitable for high-level synthesis (HLS), followed by HLS to generate Verilog RTL. We have benchmarked the performance of our proposed scheme using the open-source VerilogEval dataset, for four different industry-scale LLMs, and the Vitis HLS tool. Our experimental results demonstrate that our two-step technique substantially outperforms previous proposed techniques of direct Verilog RTL generation by LLMs in terms of average functional correctness rates, reaching score of 0.86 in pass@1 metric.",
        "Publication date": "5 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.02793"
    },
    {
        "ID": 76,
        "Title": "Scaling Symbolic Execution to Large Software Systems",
        "Authors": [
            "Gabor Horvath",
            "Reka Kovacs",
            "Zoltan Porkolab"
        ],
        "Abstract": "Static analysis is the analysis of a program without executing it, usually carried out by an automated tool. Symbolic execution is a popular static analysis technique used both in program verification and in bug detection software. It works by interpreting the code, introducing a symbol for each value unknown at compile time (e.g. user-given inputs), and carrying out calculations symbolically. The analysis engine strives to explore multiple execution paths simultaneously, although checking all paths is an intractable problem, due to the vast number of possibilities.\n  We focus on an error finding framework called the Clang Static Analyzer, and the infrastructure built around it named CodeChecker. The emphasis is on achieving end-to-end scalability. This includes the run time and memory consumption of the analysis, bug presentation to the users, automatic false positive suppression, incremental analysis, pattern discovery in the results, and usage in continuous integration loops. We also outline future directions and open problems concerning these tools.\n  While a rich literature exists on program verification software, error finding tools normally need to settle for survey papers on individual techniques. In this paper, we not only discuss individual methods, but also how these decisions interact and reinforce each other, creating a system that is greater than the sum of its parts. Although the Clang Static Analyzer can only handle C-family languages, the techniques introduced in this paper are mostly language-independent and applicable to other similar static analysis tools.",
        "Publication date": "3 August, 2024",
        "Link": "https://arxiv.org/pdf/2408.01909"
    },
    {
        "ID": 77,
        "Title": "LLM-for-X: Application-agnostic Integration of Large Language Models to Support Personal Writing Workflows",
        "Authors": [
            "Lukas Teufelberger",
            "Xintong Liu",
            "Zhipeng Li",
            "Max Moebus",
            "Christian Holz"
        ],
        "Abstract": "To enhance productivity and to streamline workflows, there is a growing trend to embed large language model (LLM) functionality into applications, from browser-based web apps to native apps that run on personal computers. Here, we introduce LLM-for-X, a system-wide shortcut layer that seamlessly augments any application with LLM services through a lightweight popup dialog. Our native layer seamlessly connects front-end applications to popular LLM backends, such as ChatGPT and Gemini, using their uniform chat front-ends as the programming interface or their custom API calls. We demonstrate the benefits of LLM-for-X across a wide variety of applications, including Microsoft Office, VSCode, and Adobe Acrobat as well as popular web apps such as Overleaf. In our evaluation, we compared LLM-for-X with ChatGPT's web interface in a series of tasks, showing that our approach can provide users with quick, efficient, and easy-to-use LLM assistance without context switching to support writing and reading tasks that is agnostic of the specific application.",
        "Publication date": "31 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.21593"
    },
    {
        "ID": 78,
        "Title": "Chat-like Asserts Prediction with the Support of Large Language Model",
        "Authors": [
            "Han Wang",
            "Han Hu",
            "Chunyang Chen",
            "Burak Turhan"
        ],
        "Abstract": "Unit testing is an essential component of software testing, with the assert statements playing an important role in determining whether the tested function operates as expected. Although research has explored automated test case generation, generating meaningful assert statements remains an ongoing challenge. While several studies have investigated assert statement generation in Java, limited work addresses this task in popular dynamically-typed programming languages like Python. In this paper, we introduce Chat-like execution-based Asserts Prediction (\\tool), a novel Large Language Model-based approach for generating meaningful assert statements for Python projects. \\tool utilizes the persona, Chain-of-Thought, and one-shot learning techniques in the prompt design, and conducts rounds of communication with LLM and Python interpreter to generate meaningful assert statements. We also present a Python assert statement dataset mined from GitHub. Our evaluation demonstrates that \\tool achieves 64.7\\% accuracy for single assert statement generation and 62\\% for overall assert statement generation, outperforming the existing approaches. We also analyze the mismatched assert statements, which may still share the same functionality and discuss the potential help \\tool could offer to the automated Python unit test generation. The findings indicate that \\tool has the potential to benefit the SE community through more practical usage scenarios.",
        "Publication date": "31 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.21429"
    },
    {
        "ID": 79,
        "Title": "Multi-Robot System Architecture design in SysML and BPMN",
        "Authors": [
            "Ahmed R. Sadik",
            "Christian Goerick"
        ],
        "Abstract": "Multi-Robot System (MRS) is a complex system that contains many different software and hardware components. This main problem addressed in this article is the MRS design complexity. The proposed solution provides a modular modeling and simulation technique that is based on formal system engineering method, therefore the MRS design complexity is decomposed and reduced. Modeling the MRS has been achieved via two formal Architecture Description Languages (ADLs), which are Systems Modeling Language (SysML) and Business Process Model and Notation (BPMN), to design the system blueprints. By using those abstract design ADLs, the implementation of the project becomes technology agnostic. This allows to transfer the design concept from on programming language to another. During the simulation phase, a multi-agent environment is used to simulate the MRS blueprints. The simulation has been implemented in Java Agent Development (JADE) middleware. Therefore, its results can be used to analysis and verify the proposed MRS model in form of performance evaluation matrix.",
        "Publication date": "26 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.18749"
    },
    {
        "ID": 80,
        "Title": "Detecting and explaining (in)equivalence of context-free grammars",
        "Authors": [
            "Marko Schmellenkamp",
            "Thomas Zeume",
            "Sven Argo",
            "Sandra Kiefer",
            "Cedric Siems",
            "Fynn Stebel"
        ],
        "Abstract": "We propose a scalable framework for deciding, proving, and explaining (in)equivalence of context-free grammars. We present an implementation of the framework and evaluate it on large data sets collected within educational support systems. Even though the equivalence problem for context-free languages is undecidable in general, the framework is able to handle a large portion of these datasets. It introduces and combines techniques from several areas, such as an abstract grammar transformation language to identify equivalent grammars as well as sufficiently similar inequivalent grammars, theory-based comparison algorithms for a large class of context-free languages, and a graph-theory-inspired grammar canonization that allows to efficiently identify isomorphic grammars.",
        "Publication date": "25 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.18220"
    },
    {
        "ID": 81,
        "Title": "BLAZE: Cross-Language and Cross-Project Bug Localization via Dynamic Chunking and Hard Example Learning",
        "Authors": [
            "Partha Chakraborty",
            "Mahmoud Alfadel",
            "Meiyappan Nagappan"
        ],
        "Abstract": "Software bugs require developers to exert significant effort to identify and resolve them, often consuming about one-third of their time. Bug localization, the process of pinpointing the exact source code files that need modification, is crucial in reducing this effort. Existing bug localization tools, typically reliant on deep learning techniques, face limitations in cross-project applicability and effectiveness in multi-language environments. Recent advancements with Large Language Models (LLMs) offer detailed representations for bug localization. However, they encounter challenges with limited context windows and mapping accuracy. To address these issues, we propose BLAZE, an approach that employs dynamic chunking and hard example learning. First, BLAZE dynamically segments source code to minimize continuity loss. Then, BLAZE fine-tunes a GPT-based model using challenging bug cases, in order to enhance cross-project and cross-language bug localization. To support the capability of BLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29 large and thriving open-source projects across five different programming languages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on three benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate substantial improvements compared to six state-of-the-art baselines. Specifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144% in Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An extensive ablation study confirms the contributions of our pipeline components to the overall performance enhancement.",
        "Publication date": "19 August, 2024",
        "Link": "https://arxiv.org/pdf/2407.17631"
    },
    {
        "ID": 82,
        "Title": "SPLAT: A framework for optimised GPU code-generation for SParse reguLar ATtention",
        "Authors": [
            "Ahan Gupta",
            "Yueming Yuan",
            "Devansh Jain",
            "Yuhao Ge",
            "David Aponte",
            "Yanqi Zhou",
            "Charith Mendis"
        ],
        "Abstract": "Multi-head-self-attention (MHSA) mechanisms achieve state-of-the-art (SOTA) performance across natural language processing and vision tasks. However, their quadratic dependence on sequence lengths has bottlenecked inference speeds. To circumvent this bottleneck, researchers have proposed various sparse-MHSA models, where a subset of full attention is computed. Despite their promise, current sparse libraries and compilers do not support high-performance implementations for diverse sparse-MHSA patterns due to the underlying sparse formats they operate on. These formats, which are typically designed for high-performance & scientific computing applications, are either curated for extreme amounts of random sparsity (<1% non-zero values), or specific sparsity patterns. However, the sparsity patterns in sparse-MHSA are moderately sparse (10-50% non-zero values) and varied, resulting in existing sparse-formats trading off generality for performance.\n  We bridge this gap, achieving both generality and performance, by proposing a novel sparse format: affine-compressed-sparse-row (ACSR) and supporting code-generation scheme, SPLAT, that generates high-performance implementations for diverse sparse-MHSA patterns on GPUs. Core to our proposed format and code generation algorithm is the observation that common sparse-MHSA patterns have uniquely regular geometric properties. These properties, which can be analyzed just-in-time, expose novel optimizations and tiling strategies that SPLAT exploits to generate high-performance implementations for diverse patterns. To demonstrate SPLAT's efficacy, we use it to generate code for various sparse-MHSA models, achieving geomean speedups of 2.05x and 4.05x over hand-written kernels written in triton and TVM respectively on A100 GPUs. Moreover, its interfaces are intuitive and easy to use with existing implementations of MHSA in JAX.",
        "Publication date": "23 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.16847"
    },
    {
        "ID": 83,
        "Title": "Language-Based Security for Low-Level MPC",
        "Authors": [
            "Christian Skalka",
            "Joseph P. Near"
        ],
        "Abstract": "Secure Multi-Party Computation (MPC) is an important enabling technology for data privacy in modern distributed applications. Currently, proof methods for low-level MPC protocols are primarily manual and thus tedious and error-prone, and are also non-standardized and unfamiliar to most PL theorists. As a step towards better language support and language-based enforcement, we develop a new staged PL for defining a variety of low-level probabilistic MPC protocols. We also formulate a collection of confidentiality and integrity hyperproperties for our language model that are familiar from information flow, including conditional noninterference, gradual release, and robust declassification. We demonstrate their relation to standard MPC threat models of passive and malicious security, and how they can be leveraged in security verification of protocols. To prove these properties we develop automated tactics in $\\mathbb{F}_2$ that can be integrated with separation logic-style reasoning.",
        "Publication date": "23 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.16504"
    },
    {
        "ID": 84,
        "Title": "Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection",
        "Authors": [
            "Xin Zhou",
            "Duc-Manh Tran",
            "Thanh Le-Cong",
            "Ting Zhang",
            "Ivana Clairine Irsan",
            "Joshua Sumarlin",
            "Bach Le",
            "David Lo"
        ],
        "Abstract": "Software vulnerabilities pose significant security challenges and potential risks to society, necessitating extensive efforts in automated vulnerability detection. There are two popular lines of work to address automated vulnerability detection. On one hand, Static Application Security Testing (SAST) is usually utilized to scan source code for security vulnerabilities, especially in industries. On the other hand, deep learning (DL)-based methods, especially since the introduction of large language models (LLMs), have demonstrated their potential in software vulnerability detection. However, there is no comparative study between SAST tools and LLMs, aiming to determine their effectiveness in vulnerability detection, understand the pros and cons of both SAST and LLMs, and explore the potential combination of these two families of approaches.\n  In this paper, we compared 15 diverse SAST tools with 12 popular or state-of-the-art open-source LLMs in detecting software vulnerabilities from repositories of three popular programming languages: Java, C, and Python. The experimental results showed that SAST tools obtain low vulnerability detection rates with relatively low false positives, while LLMs can detect up 90\\% to 100\\% of vulnerabilities but suffer from high false positives. By further ensembling the SAST tools and LLMs, the drawbacks of both SAST tools and LLMs can be mitigated to some extent. Our analysis sheds light on both the current progress and future directions for software vulnerability detection.",
        "Publication date": "23 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.16235"
    },
    {
        "ID": 85,
        "Title": "Thoughts on Learning Human and Programming Languages",
        "Authors": [
            "Daniel S. Katz",
            "Jeffrey C. Carver"
        ],
        "Abstract": "This is a virtual dialog between Jeffrey C. Carver and Daniel S. Katz on how people learn programming languages. It's based on a talk Jeff gave at the first US-RSE Conference (US-RSE'23), which led Dan to think about human languages versus computer languages. Dan discussed this with Jeff at the conference, and this discussion continued asynchronous, with this column being a record of the discussion.",
        "Publication date": "22 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.15907"
    },
    {
        "ID": 86,
        "Title": "Learning to Compile Programs to Neural Networks",
        "Authors": [
            "Logan Weber",
            "Jesse Michel",
            "Alex Renda",
            "Michael Carbin"
        ],
        "Abstract": "A $\\textit{neural surrogate of a program}$ is a neural network that mimics the behavior of a program. Researchers have used these neural surrogates to automatically tune program inputs, adapt programs to new settings, and accelerate computations. Researchers traditionally develop neural surrogates by training on input-output examples from a single program. Alternatively, language models trained on a large dataset including many programs can consume program text, to act as a neural surrogate. Using a language model to both generate a surrogate and act as a surrogate, however, leading to a trade-off between resource consumption and accuracy. We present $\\textit{neural surrogate compilation}$, a technique for producing neural surrogates directly from program text without coupling neural surrogate generation and execution. We implement neural surrogate compilers using hypernetworks trained on a dataset of C programs and find that they produce neural surrogates that are $1.9$-$9.5\\times$ as data-efficient, produce visual results that are $1.0$-$1.3\\times$ more similar to ground truth, and train in $4.3$-$7.3\\times$ fewer epochs than neural surrogates trained from scratch.",
        "Publication date": "21 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.15078"
    },
    {
        "ID": 87,
        "Title": "Exploring the Evidence-Based Beliefs and Behaviors of LLM-Based Programming Assistants",
        "Authors": [
            "Chris Brown",
            "Jason Cusati"
        ],
        "Abstract": "Recent innovations in artificial intelligence (AI), primarily powered by large language models (LLMs), have transformed how programmers develop and maintain software -- leading to new frontiers in software engineering (SE). The advanced capabilities of LLM-based programming assistants to support software development tasks have led to a rise in the adoption of LLMs in SE. However, little is known about the evidenced-based practices, tools and processes verified by research findings, supported and adopted by AI programming assistants. To this end, our work conducts a preliminary evaluation exploring the beliefs and behaviors of LLM used to support software development tasks. We investigate 17 evidence-based claims posited by empirical SE research across five LLM-based programming assistants. Our findings show that LLM-based programming assistants have ambiguous beliefs regarding research claims, lack credible evidence to support responses, and are incapable of adopting practices demonstrated by empirical SE research to support development tasks. Based on our results, we provide implications for practitioners adopting LLM-based programming assistants in development contexts and shed light on future research directions to enhance the reliability and trustworthiness of LLMs -- aiming to increase awareness and adoption of evidence-based SE research findings in practice.",
        "Publication date": "18 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.13900"
    },
    {
        "ID": 88,
        "Title": "The Software Complexity of Nations",
        "Authors": [
            "SÃ¡ndor JuhÃ¡sz",
            "Johannes Wachs",
            "Jermain Kaminski",
            "CÃ©sar A. Hidalgo"
        ],
        "Abstract": "Despite the growing importance of the digital sector, research on economic complexity and its implications continues to rely mostly on administrative records, e.g. data on exports, patents, and employment, that fail to capture the nuances of the digital economy. In this paper we use data on the geography of programming languages used in open-source software projects to extend economic complexity ideas to the digital economy. We estimate a country's software economic complexity and show that it complements the ability of measures of complexity based on trade, patents, and research papers to account for international differences in GDP per capita, income inequality, and emissions. We also show that open-source software follows the principle of relatedness, meaning that a country's software entries and exits are explained by specialization in related programming languages. We conclude by exploring the diversification and development of countries in open-source software in the context of large language models. Together, these findings help extend economic complexity methods and their policy considerations to the digital sector.",
        "Publication date": "18 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.13880"
    },
    {
        "ID": 89,
        "Title": "Graphitron: A Domain Specific Language for FPGA-based Graph Processing Accelerator Generation",
        "Authors": [
            "Xinmiao Zhang",
            "Zheng Feng",
            "Shengwen Liang",
            "Xinyu Chen",
            "Cheng Liu",
            "Huawei Li",
            "Xiaowei Li"
        ],
        "Abstract": "FPGA-based graph processing accelerators, enabling extensive customization, have demonstrated significant energy efficiency over general computing engines like CPUs and GPUs. Nonetheless, customizing accelerators to diverse graph processing algorithms with distinct computational patterns remains challenging and error-prone for high-level application users. To this end, template-based approaches have been developed to automate the graph processing accelerator generation. Although these frameworks significantly enhance the design productivity, the templates often result in closely coupled algorithms, programming models, and architectures, severely limiting the versatility of the targeted graph processing algorithms and their applicability to high-level users. Furthermore, the limitations of the frameworks are usually ambiguous due to the absence of a rigorous grammar definition. To overcome these challenges, we introduce Graphitron, a domain-specific language (DSL), which allows users to generate customized accelerators for a wide range of graph processing algorithms on FPGAs without engaging with the complexities of low-level FPGA designs. Graphitron, by defining vertices and edges as primitive data types, naturally facilitates the description of graph algorithms using edge-centric or vertex-centric programming models. The Graphitron back-end employs a suite of hardware optimization techniques including pipelining, data shuffling, and memory access optimization that are independent with the specific algorithms, supporting the creation of versatile graph processing accelerators. Our experiments indicate that accelerators crafted using Graphitron achieve comparable performance to that generated with template-based design framework. Moreover, it exhibits exceptional flexibility in algorithm expression and significantly enhance accelerator design productivity.",
        "Publication date": "17 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.12575"
    },
    {
        "ID": 90,
        "Title": "WebAssembly and Security: a review",
        "Authors": [
            "Gaetano Perrone",
            "Simon Pietro Romano"
        ],
        "Abstract": "WebAssembly is revolutionizing the approach to developing modern applications. Although this technology was born to create portable and performant modules in web browsers, currently, its capabilities are extensively exploited in multiple and heterogeneous use-case scenarios. With the extensive effort of the community, new toolkits make the use of this technology more suitable for real-world applications. In this context, it is crucial to study the liaisons between the WebAssembly ecosystem and software security. Indeed, WebAssembly can be a medium for improving the security of a system, but it can also be exploited to evade detection systems or for performing cryptomining activities. In addition, programs developed in low-level languages such as C can be compiled in WebAssembly binaries, and it is interesting to evaluate the security impacts of executing programs vulnerable to attacks against memory in the WebAssembly sandboxed environment. Also, WebAssembly has been designed to provide a secure and isolated environment, but such capabilities should be assessed in order to analyze their weaknesses and propose new mechanisms for addressing them. Although some research works have provided surveys of the most relevant solutions aimed at discovering WebAssembly vulnerabilities or detecting attacks, at the time of writing, there is no comprehensive review of security-related literature in the WebAssembly ecosystem. We aim to fill this gap by proposing a comprehensive review of research works dealing with security in WebAssembly. We analyze 121 papers by identifying seven different security categories.\n  We hope that our work will provide insights into the complex landscape of WebAssembly and guide researchers, developers, and security professionals towards novel avenues in the realm of the WebAssembly ecosystem.",
        "Publication date": "16 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.12297"
    },
    {
        "ID": 91,
        "Title": "LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and Structured Data",
        "Authors": [
            "Liana Patel",
            "Siddharth Jha",
            "Carlos Guestrin",
            "Matei Zaharia"
        ],
        "Abstract": "The semantic capabilities of language models (LMs) have the potential to enable rich analytics and reasoning over vast knowledge corpora. Unfortunately, existing systems lack high-level abstractions to perform semantic queries at scale. We introduce semantic operators, a declarative programming interface that extends the relational model with composable AI-based operations for semantic queries over datasets (e.g., sorting or aggregating records using natural language criteria). Each operator can be implemented and optimized in multiple ways, opening a rich space for execution plans similar to relational operators. We implement our operators and several optimizations for them in LOTUS, an open-source query engine with a Pandas-like API.\n  We demonstrate LOTUS' effectiveness across a series of real applications, including fact-checking, extreme multi-label classification, and search. We find that LOTUS' programming model is highly expressive, capturing state-of-the-art query pipelines with low development overhead. Specifically, on the FEVER dataset, LOTUS' programs can reproduce FacTool, a recent state-of-the-art fact-checking pipeline, in few lines of code, and implement a new pipeline that improves accuracy by $9.5\\%$, while offering $7-34\\times$ lower execution time. In the extreme multi-label classification task on the BioDEX dataset, LOTUS reproduces state-of-the art result quality with its join operator, while providing an efficient algorithm that runs $800\\times$ faster than a naive join. In the search and ranking application, LOTUS allows a simple composition of operators to achieve $5.9 - 49.4\\%$ higher nDCG@10 than the vanilla retriever and re-ranker, while also providing query efficiency, with $1.67 - 10\\times$ lower execution time than LM-based ranking methods used by prior works. LOTUS is publicly available at https://github.com/stanford-futuredata/lotus.",
        "Publication date": "16 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.11418"
    },
    {
        "ID": 92,
        "Title": "CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization",
        "Authors": [
            "Yang Zhao",
            "Di Huang",
            "Chongxiao Li",
            "Pengwei Jin",
            "Ziyuan Nan",
            "Tianyun Ma",
            "Lei Qi",
            "Yansong Pan",
            "Zhenxing Zhang",
            "Rui Zhang",
            "Xishan Zhang",
            "Zidong Du",
            "Qi Guo",
            "Xing Hu",
            "Yunji Chen"
        ],
        "Abstract": "The increasing complexity and high costs associated with modern processor design have led to a surge in demand for processor design automation. Instruction-tuned large language models (LLMs) have demonstrated remarkable performance in automatically generating code for general-purpose programming languages like Python. However, these methods fail on hardware description languages (HDLs) like Verilog due to the scarcity of high-quality instruction tuning data, as even advanced LLMs like GPT-3.5 exhibit limited performance on Verilog generation. Regarding this issue, we observe that (1) Verilog code collected from the real world has higher quality than those generated by LLMs. (2) LLMs like GPT-3.5 excel in summarizing Verilog code rather than generating it. Based on these observations, this paper introduces CodeV, a series of open-source instruction-tuned Verilog generation LLMs. Instead of generating descriptions first and then getting the corresponding code from advanced LLMs, we prompt the LLM with Verilog code and let the LLM generate the corresponding natural language description by multi-level summarization. Experimental results show that CodeV relatively surpasses the previous open-source SOTA by 14.4% (BetterV in VerilogEval) and 11.3% (RTLCoder in RTLLM) respectively, and also relatively outperforms previous commercial SOTA GPT-4 by 22.1% in VerilogEval.",
        "Publication date": "20 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.10424"
    },
    {
        "ID": 93,
        "Title": "Defining Name Accessibility using Scope Graphs (Extended Edition)",
        "Authors": [
            "Aron Zwaan",
            "Casper Bach Poulsen"
        ],
        "Abstract": "Many programming languages allow programmers to regulate accessibility; i.e., annotating a declaration with keywords such as export and private to indicate where it can be accessed. Despite the importance of name accessibility for, e.g., compilers, editor auto-completion and tooling, and automated refactorings, few existing type systems provide a formal account of name accessibility.\n  We present a declarative, executable, and language-parametric model for name accessibility, which provides a formal specification of name accessibility in Java, C#, C++, Rust, and Eiffel. We achieve this by defining name accessibility as a predicate on resolution paths through scope graphs. Since scope graphs are a language-independent model of name resolution, our model provides a uniform approach to defining different accessibility policies for different languages.\n  Our model is implemented in Statix, a logic language for executable type system specification using scope graphs. We evaluate its correctness on a test suite that compares it with the C#, Java, and Rust compilers, and show we can synthesize access modifiers in programs with holes accurately.",
        "Publication date": "12 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.09320"
    },
    {
        "ID": 94,
        "Title": "Higher-Order Specifications for Deductive Synthesis of Programs with Pointers (Extended Version)",
        "Authors": [
            "David Young",
            "Ziyi Yang",
            "Ilya Sergey",
            "Alex Potanin"
        ],
        "Abstract": "Synthetic Separation Logic (SSL) is a formalism that powers SuSLik, the state-of-the-art approach for the deductive synthesis of provably-correct programs in C-like languages that manipulate Heap-based linked data structures. Despite its expressivity, SSL suffers from two shortcomings that hinder its utility. First, its main specification component, inductive predicates, only admits first-order definitions of data structure shapes, which leads to the proliferation of ''boiler-plate'' predicates for specifying common patterns. Second, SSL requires concrete definitions of data structures to synthesise programs that manipulate them, which results in the need to change a specification for a synthesis task every time changes are introduced into the layout of the involved structures.\n  We propose to significantly lift the level of abstraction used in writing Separation Logic specifications for synthesis -- both simplifying the approach and making the specifications more usable and easy to read and follow. We avoid the need to repetitively re-state low-level representation details throughout the specifications -- allowing the reuse of different implementations of the same data structure by abstracting away the details of a specific layout used in memory. Our novel high-level front-end language called Pika significantly improves the expressiveness of SuSLik.\n  We implemented a layout-agnostic synthesiser from Pika to SuSLik enabling push-button synthesis of C programs with in-place memory updates, along with the accompanying full proofs that they meet Separation Logic-style specifications, from high-level specifications that resemble ordinary functional programs. Our experiments show that our tool can produce C code that is comparable in its performance characteristics and is sometimes faster than Haskell.",
        "Publication date": "15 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.09143"
    },
    {
        "ID": 95,
        "Title": "Source Code Summarization in the Era of Large Language Models",
        "Authors": [
            "Weisong Sun",
            "Yun Miao",
            "Yuekang Li",
            "Hongyu Zhang",
            "Chunrong Fang",
            "Yi Liu",
            "Gelei Deng",
            "Yang Liu",
            "Zhenyu Chen"
        ],
        "Abstract": "To support software developers in understanding and maintaining programs, various automatic (source) code summarization techniques have been proposed to generate a concise natural language summary (i.e., comment) for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of code-related tasks. In this paper, we undertake a systematic and comprehensive study on code summarization in the era of LLMs, which covers multiple aspects involved in the workflow of LLM-based code summarization. Specifically, we begin by examining prevalent automated evaluation methods for assessing the quality of summaries generated by LLMs and find that the results of the GPT-4 evaluation method are most closely aligned with human evaluation. Then, we explore the effectiveness of five prompting techniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in adapting LLMs to code summarization tasks. Contrary to expectations, advanced prompting techniques may not outperform simple zero-shot prompting. Next, we investigate the impact of LLMs' model settings (including top\\_p and temperature parameters) on the quality of generated summaries. We find the impact of the two parameters on summary quality varies by the base LLM and programming language, but their impacts are similar. Moreover, we canvass LLMs' abilities to summarize code snippets in distinct types of programming languages. The results reveal that LLMs perform suboptimally when summarizing code written in logic programming languages compared to other language types. Finally, we unexpectedly find that CodeLlama-Instruct with 7B parameters can outperform advanced GPT-4 in generating summaries describing code implementation details and asserting code properties. We hope that our findings can provide a comprehensive understanding of code summarization in the era of LLMs.",
        "Publication date": "9 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.07959"
    },
    {
        "ID": 96,
        "Title": "Solving General Natural-Language-Description Optimization Problems with Large Language Models",
        "Authors": [
            "Jihai Zhang",
            "Wei Wang",
            "Siyan Guo",
            "Li Wang",
            "Fangquan Lin",
            "Cheng Yang",
            "Wotao Yin"
        ],
        "Abstract": "Optimization problems seek to find the best solution to an objective under a set of constraints, and have been widely investigated in real-world applications. Modeling and solving optimization problems in a specific domain typically require a combination of domain knowledge, mathematical skills, and programming ability, making it difficult for general users and even domain professionals. In this paper, we propose a novel framework called OptLLM that augments LLMs with external solvers. Specifically, OptLLM accepts user queries in natural language, convert them into mathematical formulations and programming codes, and calls the solvers to calculate the results for decision-making. In addition, OptLLM supports multi-round dialogues to gradually refine the modeling and solving of optimization problems. To illustrate the effectiveness of OptLLM, we provide tutorials on three typical optimization applications and conduct experiments on both prompt-based GPT models and a fine-tuned Qwen model using a large-scale selfdeveloped optimization dataset. Experimental results show that OptLLM works with various LLMs, and the fine-tuned model achieves an accuracy boost compared to the promptbased models. Some features of OptLLM framework have been available for trial since June 2023 (https://opt.alibabacloud.com/chat or https://opt.aliyun.com/chat).",
        "Publication date": "9 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.07924"
    },
    {
        "ID": 97,
        "Title": "Real-time adaptive optics control with a high level programming language",
        "Authors": [
            "William Thompson",
            "Darryl Gamroth",
            "Christian Marois",
            "Olivier LardiÃ¨re"
        ],
        "Abstract": "Adaptive optics systems are usually prototyped in a convenient but slow language like MATLAB or Python, and then re-written from scratch using high-performance C/C++ to perform real-time control. This duplication of effort adds costs and slows the experimentation process. We present instead a technical demonstration of performing real time, sub-millisecond latency control with an adaptive optics system using the high-level Julia programming language. This open-source software demonstrates support for multiple cameras, pixel streaming, and network-transparency distributed computing. Furthermore, it is easy to interface it with other programming languages as desired.",
        "Publication date": "9 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.07207"
    },
    {
        "ID": 98,
        "Title": "Prompting Techniques for Secure Code Generation: A Systematic Investigation",
        "Authors": [
            "Catherine Tony",
            "NicolÃ¡s E. DÃ­az Ferreyra",
            "Markus Mutas",
            "Salem Dhiff",
            "Riccardo Scandariato"
        ],
        "Abstract": "Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers to create code from natural language (NL) instructions. However, studies have questioned their ability to produce secure code and, thereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have emerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation remains under-explored and calls for further investigations. OBJECTIVE: In this study, we investigate the impact of different prompting techniques on the security of code generated from NL instructions by LLMs. METHOD: First we perform a systematic literature review to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL security-relevant code-generation prompts. RESULTS: Our work (i) classifies potential prompting techniques for code generation (ii) adapts and evaluates a subset of the identified techniques for secure code generation tasks and (iii) observes a reduction in security weaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI), contributing valuable insights to the ongoing discourse on LLM-generated code security.",
        "Publication date": "9 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.07064"
    },
    {
        "ID": 99,
        "Title": "Macaw: A Machine Code Toolbox for the Busy Binary Analyst",
        "Authors": [
            "Ryan G. Scott",
            "Brett Boston",
            "Benjamin Davis",
            "Iavor Diatchki",
            "Mike Dodds",
            "Joe Hendrix",
            "Daniel Matichuk",
            "Kevin Quick",
            "Tristan Ravitch",
            "Valentin Robert",
            "Benjamin Selfridge",
            "Andrei StefÄnescu",
            "Daniel Wagner",
            "Simon Winwood"
        ],
        "Abstract": "When attempting to understand the behavior of an executable, a binary analyst can make use of many different techniques. These include program slicing, dynamic instrumentation, binary-level rewriting, symbolic execution, and formal verification, all of which can uncover insights into how a piece of machine code behaves. As a result, there is no one-size-fits-all binary analysis tool, so a binary analysis researcher will often combine several different tools. Sometimes, a researcher will even need to design new tools to study problems that existing frameworks are not well equipped to handle. Designing such tools from complete scratch is rarely time- or cost-effective, however, given the scale and complexity of modern instruction set architectures.\n  We present Macaw, a modular framework that makes it possible to rapidly build reliable binary analysis tools across a range of use cases. Over a decade of development, we have used Macaw to support an industrial research team in building tools for machine code-related tasks. As such, the name \"Macaw\" refers not just to the framework itself, but also a suite of tools that are built on top of the framework. We describe Macaw in depth and describe the different static and dynamic analyses that it performs, many of which are powered by an SMT-based symbolic execution engine. We put a particular focus on interoperability between machine code and higher-level languages, including binary lifting from x86 to LLVM, as well verifying the correctness of mixed C and assembly code.",
        "Publication date": "8 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.06375"
    },
    {
        "ID": 100,
        "Title": "Studying the Impact of TensorFlow and PyTorch Bindings on Machine Learning Software Quality",
        "Authors": [
            "Hao Li",
            "Gopi Krishnan Rajbahadur",
            "Cor-Paul Bezemer"
        ],
        "Abstract": "Bindings for machine learning frameworks (such as TensorFlow and PyTorch) allow developers to integrate a framework's functionality using a programming language different from the framework's default language (usually Python). In this paper, we study the impact of using TensorFlow and PyTorch bindings in C#, Rust, Python and JavaScript on the software quality in terms of correctness (training and test accuracy) and time cost (training and inference time) when training and performing inference on five widely used deep learning models. Our experiments show that a model can be trained in one binding and used for inference in another binding for the same framework without losing accuracy. Our study is the first to show that using a non-default binding can help improve machine learning software quality from the time cost perspective compared to the default Python binding while still achieving the same level of correctness.",
        "Publication date": "7 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.05466"
    },
    {
        "ID": 101,
        "Title": "Harnessing the Power of LLMs: Automating Unit Test Generation for High-Performance Computing",
        "Authors": [
            "Rabimba Karanjai",
            "Aftab Hussain",
            "Md Rafiqul Islam Rabin",
            "Lei Xu",
            "Weidong Shi",
            "Mohammad Amin Alipour"
        ],
        "Abstract": "Unit testing is crucial in software engineering for ensuring quality. However, it's not widely used in parallel and high-performance computing software, particularly scientific applications, due to their smaller, diverse user base and complex logic. These factors make unit testing challenging and expensive, as it requires specialized knowledge and existing automated tools are often ineffective.\n  To address this, we propose an automated method for generating unit tests for such software, considering their unique features like complex logic and parallel processing. Recently, large language models (LLMs) have shown promise in coding and testing. We explored the capabilities of Davinci (text-davinci-002) and ChatGPT (gpt-3.5-turbo) in creating unit tests for C++ parallel programs. Our results show that LLMs can generate mostly correct and comprehensive unit tests, although they have some limitations, such as repetitive assertions and blank test cases.",
        "Publication date": "6 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.05202"
    },
    {
        "ID": 102,
        "Title": "Evaluating Language Models for Generating and Judging Programming Feedback",
        "Authors": [
            "Charles Koutcheme",
            "Nicola Dainese",
            "Arto Hellas",
            "Sami Sarsa",
            "Juho Leinonen",
            "Syed Ashraf",
            "Paul Denny"
        ],
        "Abstract": "The emergence of large language models (LLMs) has transformed research and practice in a wide range of domains. Within the computing education research (CER) domain, LLMs have received plenty of attention especially in the context of learning programming. Much of the work on LLMs in CER has however focused on applying and evaluating proprietary models. In this article, we evaluate the efficiency of open-source LLMs in generating high-quality feedback for programming assignments, and in judging the quality of the programming feedback, contrasting the results against proprietary models. Our evaluations on a dataset of students' submissions to Python introductory programming exercises suggest that the state-of-the-art open-source LLMs (Meta's Llama3) are almost on-par with proprietary models (GPT-4o) in both the generation and assessment of programming feedback. We further demonstrate the efficiency of smaller LLMs in the tasks, and highlight that there are a wide range of LLMs that are accessible even for free for educators and practitioners.",
        "Publication date": "5 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.04873"
    },
    {
        "ID": 103,
        "Title": "Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning",
        "Authors": [
            "Eric Pasewark",
            "Kyle Montgomery",
            "Kefei Duan",
            "Dawn Song",
            "Chenguang Wang"
        ],
        "Abstract": "We present a new method for large language models to solve compositional tasks. Although they have shown strong performance on traditional language understanding tasks, large language models struggle to solve compositional tasks, where the solution depends on solving smaller instances of the same problem. We propose a natural approach to solve compositional tasks recursively. Our method, Re-Tuning, tunes models to break down a problem into subproblems, solve those subproblems, and combine the results. We show that our method significantly improves model performance on three representative compositional tasks: integer addition, dynamic programming, and parity. Compared to state-of-the-art methods that keep intermediate steps towards solving the problems, Re-Tuning achieves significantly higher accuracy and is more GPU memory efficient.",
        "Publication date": "5 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.04787"
    },
    {
        "ID": 104,
        "Title": "Secure Rewind and Discard on ARM Morello",
        "Authors": [
            "Sacha Ruchlejmer"
        ],
        "Abstract": "Memory-unsafe programming languages such as C and C++ are the preferred languages for systems programming, embedded systems, and performance-critical applications. The widespread use of these languages makes the risk of memory-related attacks very high. There are well-known detection mechanisms, but they do not address software resilience. An earlier approach proposes the Secure Domain Rewind and Discard (SDRaD) of isolated domains as a method to enhance the resilience of software targeted by runtime attacks on x86 architecture, based on hardware-enforced Memory Protection Key (MPK). In this work, SDRaD has been adapted to work with the Capability Hardware Enhanced RISC Instructions (CHERI) architecture to be more lightweight and performant. The results obtained in this thesis show that CHERI-SDRaD, the prototype adaption that leverages the memory-safety properties inherent to the CHERI architecture, results in a solution with less performance degradation (2.2% in Nginx benchmarks) compared to earlier results obtained with the original SDRaD prototype on an Intel-based architecture. The adaption to CHERI additionally allowed limitations inherent to the MPK-based approach to be resolved.",
        "Publication date": "5 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.04757"
    },
    {
        "ID": 105,
        "Title": "Slice-100K: A Multimodal Dataset for Extrusion-based 3D Printing",
        "Authors": [
            "Anushrut Jignasu",
            "Kelly O. Marshall",
            "Ankush Kumar Mishra",
            "Lucas Nerone Rillo",
            "Baskar Ganapathysubramanian",
            "Aditya Balu",
            "Chinmay Hegde",
            "Adarsh Krishnamurthy"
        ],
        "Abstract": "G-code (Geometric code) or RS-274 is the most widely used computer numerical control (CNC) and 3D printing programming language. G-code provides machine instructions for the movement of the 3D printer, especially for the nozzle, stage, and extrusion of material for extrusion-based additive manufacturing. Currently there does not exist a large repository of curated CAD models along with their corresponding G-code files for additive manufacturing. To address this issue, we present SLICE-100K, a first-of-its-kind dataset of over 100,000 G-code files, along with their tessellated CAD model, LVIS (Large Vocabulary Instance Segmentation) categories, geometric properties, and renderings. We build our dataset from triangulated meshes derived from Objaverse-XL and Thingi10K datasets. We demonstrate the utility of this dataset by finetuning GPT-2 on a subset of the dataset for G-code translation from a legacy G-code format (Sailfish) to a more modern, widely used format (Marlin). SLICE-100K will be the first step in developing a multimodal foundation model for digital manufacturing.",
        "Publication date": "11 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.04180"
    },
    {
        "ID": 106,
        "Title": "Automated C/C++ Program Repair for High-Level Synthesis via Large Language Models",
        "Authors": [
            "Kangwei Xu",
            "Grace Li Zhang",
            "Xunzhao Yin",
            "Cheng Zhuo",
            "Ulf Schlichtmann",
            "Bing Li"
        ],
        "Abstract": "In High-Level Synthesis (HLS), converting a regular C/C++ program into its HLS-compatible counterpart (HLS-C) still requires tremendous manual effort. Various program scripts have been introduced to automate this process. But the resulting codes usually contain many issues that should be manually repaired by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated program repair in HLS. However, due to the limited training of LLMs considering hardware and software simultaneously, hallucinations may occur during program repair using LLMs, leading to compilation failures. Besides, using LLMs for iterative repair also incurs a high cost. To address these challenges, we propose an LLM-driven program repair framework that takes regular C/C++ code as input and automatically generates its corresponding HLS-C code for synthesis while minimizing human repair effort. To mitigate the hallucinations in LLMs and enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is introduced to guide the LLMs toward correct repair. In addition, we use LLMs to create a static bit width optimization program to identify the optimized bit widths for variables. Moreover, LLM-driven HLS optimization strategies are introduced to add/tune pragmas in HLS-C programs for circuit optimization. Experimental results demonstrate that the proposed LLM-driven automated framework can achieve much higher repair pass rates in 24 real-world applications compared with the traditional scripts and the direct application of LLMs for program repair.",
        "Publication date": "4 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.03889"
    },
    {
        "ID": 107,
        "Title": "The Mysterious Case of Neuron 1512: Injectable Realignment Architectures Reveal Internal Characteristics of Meta's Llama 2 Model",
        "Authors": [
            "Brenden Smith",
            "Dallin Baker",
            "Clayton Chase",
            "Myles Barney",
            "Kaden Parker",
            "Makenna Allred",
            "Peter Hu",
            "Alex Evans",
            "Nancy Fulda"
        ],
        "Abstract": "Large Language Models (LLMs) have an unrivaled and invaluable ability to \"align\" their output to a diverse range of human preferences, by mirroring them in the text they generate. The internal characteristics of such models, however, remain largely opaque. This work presents the Injectable Realignment Model (IRM) as a novel approach to language model interpretability and explainability. Inspired by earlier work on Neural Programming Interfaces, we construct and train a small network -- the IRM -- to induce emotion-based alignments within a 7B parameter LLM architecture. The IRM outputs are injected via layerwise addition at various points during the LLM's forward pass, thus modulating its behavior without changing the weights of the original model. This isolates the alignment behavior from the complex mechanisms of the transformer model. Analysis of the trained IRM's outputs reveals a curious pattern. Across more than 24 training runs and multiple alignment datasets, patterns of IRM activations align themselves in striations associated with a neuron's index within each transformer layer, rather than being associated with the layers themselves. Further, a single neuron index (1512) is strongly correlated with all tested alignments. This result, although initially counterintuitive, is directly attributable to design choices present within almost all commercially available transformer architectures, and highlights a potential weak point in Meta's pretrained Llama 2 models. It also demonstrates the value of the IRM architecture for language model analysis and interpretability. Our code and datasets are available at https://github.com/DRAGNLabs/injectable-alignment-model",
        "Publication date": "4 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.03621"
    },
    {
        "ID": 108,
        "Title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization",
        "Authors": [
            "Chris Cummins",
            "Volker Seeker",
            "Dejan Grubisic",
            "Baptiste Roziere",
            "Jonas Gehring",
            "Gabriel Synnaeve",
            "Hugh Leather"
        ],
        "Abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.",
        "Publication date": "27 June, 2024",
        "Link": "https://arxiv.org/pdf/2407.02524"
    },
    {
        "ID": 109,
        "Title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness",
        "Authors": [
            "Hung Le",
            "Yingbo Zhou",
            "Caiming Xiong",
            "Silvio Savarese",
            "Doyen Sahoo"
        ],
        "Abstract": "Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements. However, in many practical scenarios, it becomes increasingly challenging for these models to navigate the intricate boundary between helpfulness and safety, especially against highly complex yet potentially malicious instructions. In this work, we introduce INDICT: a new framework that empowers LLMs with Internal Dialogues of Critiques for both safety and helpfulness guidance. The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic. Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter. We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks, using LLMs from 7B to 70B parameters. We observed that our approach can provide an advanced level of critiques of both safety and helpfulness analysis, significantly improving the quality of output codes ($+10\\%$ absolute improvements in all models).",
        "Publication date": "23 June, 2024",
        "Link": "https://arxiv.org/pdf/2407.02518"
    },
    {
        "ID": 110,
        "Title": "Automated Text Scoring in the Age of Generative AI for the GPU-poor",
        "Authors": [
            "Christopher Michael Ormerod",
            "Alexander Kwako"
        ],
        "Abstract": "Current research on generative language models (GLMs) for automated text scoring (ATS) has focused almost exclusively on querying proprietary models via Application Programming Interfaces (APIs). Yet such practices raise issues around transparency and security, and these methods offer little in the way of efficiency or customizability. With the recent proliferation of smaller, open-source models, there is the option to explore GLMs with computers equipped with modest, consumer-grade hardware, that is, for the \"GPU poor.\" In this study, we analyze the performance and efficiency of open-source, small-scale GLMs for ATS. Results show that GLMs can be fine-tuned to achieve adequate, though not state-of-the-art, performance. In addition to ATS, we take small steps towards analyzing models' capacity for generating feedback by prompting GLMs to explain their scores. Model-generated feedback shows promise, but requires more rigorous evaluation focused on targeted use cases.",
        "Publication date": "1 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.01873"
    },
    {
        "ID": 111,
        "Title": "SCIF: A Language for Compositional Smart Contract Security",
        "Authors": [
            "Siqiu Yao",
            "Haobin Ni",
            "Andrew C. Myers",
            "Ethan Cecchetti"
        ],
        "Abstract": "Securing smart contracts remains a fundamental challenge. At its core, it is about building software that is secure in composition with untrusted code, a challenge that extends far beyond blockchains. We introduce SCIF, a language for building smart contracts that are compositionally secure. SCIF is based on the fundamentally compositional principle of secure information flow, but extends this core mechanism to include protection against reentrancy attacks, confused deputy attacks, and improper error handling, even in the presence of malicious contracts that do not follow SCIF's rules. SCIF supports a rich ecosystem of interacting principals with partial trust through its mechanisms for dynamic trust management. SCIF has been implemented as a compiler to Solidity. We describe the SCIF language, including its static checking rules and runtime. Finally, we implement several applications with intricate security reasoning, showing how SCIF supports building complex smart contracts securely and gives programmer accurate diagnostics about potential security bugs.",
        "Publication date": "1 July, 2024",
        "Link": "https://arxiv.org/pdf/2407.01204"
    },
    {
        "ID": 112,
        "Title": "One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving",
        "Authors": [
            "Archit Patke",
            "Dhemath Reddy",
            "Saurabh Jha",
            "Haoran Qiu",
            "Christian Pinto",
            "Shengkun Cui",
            "Chandra Narayanaswami",
            "Zbigniew Kalbarczyk",
            "Ravishankar Iyer"
        ],
        "Abstract": "Large language models (LLMs) have become an increasingly important workload for cloud providers catering to both enterprise and consumer applications. LLM inference requests from these applications have end-to-end latency SLOs that must be adhered to in production settings. However, existing LLM serving systems focus on optimization objectives such as request serving throughput or request execution latency rather than the end-to-end latency SLOs. Achieving end-to-end SLOs for latency-sensitive requests is challenging due to head-of-line (HOL) blocking in the request queue, which results from bursty arrival rates and insufficient resources.\n  To address the above challenge, we propose QLM, a multi-model queue management framework for LLM serving. QLM uses stochastic programming to orchestrate the actions of multiple LLM Serving Operations (LSOs) to reduce HOL blocking and maximize SLO attainment. Specifically, QLM uses the following LSOs: model swapping, request eviction, GPU-CPU state swapping, load balancing, and warm model start. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems.",
        "Publication date": "5 June, 2024",
        "Link": "https://arxiv.org/pdf/2407.00047"
    },
    {
        "ID": 113,
        "Title": "ROS-LLM: A ROS framework for embodied AI with task feedback and structured reasoning",
        "Authors": [
            "Christopher E. Mower",
            "Yuhui Wan",
            "Hongzhan Yu",
            "Antoine Grosnit",
            "Jonas Gonzalez-Billandon",
            "Matthieu Zimmer",
            "Jinlong Wang",
            "Xinyu Zhang",
            "Yao Zhao",
            "Anbang Zhai",
            "Puze Liu",
            "Daniel Palenicek",
            "Davide Tateo",
            "Cesar Cadena",
            "Marco Hutter",
            "Jan Peters",
            "Guangjian Tian",
            "Yuzheng Zhuang",
            "Kun Shao",
            "Xingyue Quan",
            "Jianye Hao",
            "Jun Wang",
            "Haitham Bou-Ammar"
        ],
        "Abstract": "We present a framework for intuitive robot programming by non-experts, leveraging natural language prompts and contextual information from the Robot Operating System (ROS). Our system integrates large language models (LLMs), enabling non-experts to articulate task requirements to the system through a chat interface. Key features of the framework include: integration of ROS with an AI agent connected to a plethora of open-source and commercial LLMs, automatic extraction of a behavior from the LLM output and execution of ROS actions/services, support for three behavior modes (sequence, behavior tree, state machine), imitation learning for adding new robot actions to the library of possible actions, and LLM reflection via human and environment feedback. Extensive experiments validate the framework, showcasing robustness, scalability, and versatility in diverse scenarios, including long-horizon tasks, tabletop rearrangements, and remote supervisory control. To facilitate the adoption of our framework and support the reproduction of our results, we have made our code open-source. You can access it at: https://github.com/huawei-noah/HEBO/tree/master/ROSLLM.",
        "Publication date": "12 July, 2024",
        "Link": "https://arxiv.org/pdf/2406.19741"
    },
    {
        "ID": 114,
        "Title": "Building multiscale models with PhysiBoSS, an agent-based modeling tool",
        "Authors": [
            "Marco Ruscone",
            "Andrea Checcoli",
            "Randy Heiland",
            "Emmanuel Barillot",
            "Paul Macklin",
            "Laurence Calzone",
            "Vincent NoÃ«l"
        ],
        "Abstract": "Multiscale models provide a unique tool for studying complex processes that study events occurring at different scales across space and time. In the context of biological systems, such models can simulate mechanisms happening at the intracellular level such as signaling, and at the extracellular level where cells communicate and coordinate with other cells. They aim to understand the impact of genetic or environmental deregulation observed in complex diseases, describe the interplay between a pathological tissue and the immune system, and suggest strategies to revert the diseased phenotypes. The construction of these multiscale models remains a very complex task, including the choice of the components to consider, the level of details of the processes to simulate, or the fitting of the parameters to the data. One additional difficulty is the expert knowledge needed to program these models in languages such as C++ or Python, which may discourage the participation of non-experts. Simplifying this process through structured description formalisms -- coupled with a graphical interface -- is crucial in making modeling more accessible to the broader scientific community, as well as streamlining the process for advanced users. This article introduces three examples of multiscale models which rely on the framework PhysiBoSS, an add-on of PhysiCell that includes intracellular descriptions as continuous time Boolean models to the agent-based approach. The article demonstrates how to easily construct such models, relying on PhysiCell Studio, the PhysiCell Graphical User Interface. A step-by-step tutorial is provided as a Supplementary Material and all models are provided at: https://physiboss.github.io/tutorial/.",
        "Publication date": "26 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.18371"
    },
    {
        "ID": 115,
        "Title": "Transforming Software Development: Evaluating the Efficiency and Challenges of GitHub Copilot in Real-World Projects",
        "Authors": [
            "Ruchika Pandey",
            "Prabhat Singh",
            "Raymond Wei",
            "Shaila Shankar"
        ],
        "Abstract": "Generative AI technologies promise to transform the product development lifecycle. This study evaluates the efficiency gains, areas for improvement, and emerging challenges of using GitHub Copilot, an AI-powered coding assistant. We identified 15 software development tasks and assessed Copilot's benefits through real-world projects on large proprietary code bases. Our findings indicate significant reductions in developer toil, with up to 50% time saved in code documentation and autocompletion, and 30-40% in repetitive coding tasks, unit test generation, debugging, and pair programming. However, Copilot struggles with complex tasks, large functions, multiple files, and proprietary contexts, particularly with C/C++ code. We project a 33-36% time reduction for coding-related tasks in a cloud-first software development lifecycle. This study aims to quantify productivity improvements, identify underperforming scenarios, examine practical benefits and challenges, investigate performance variations across programming languages, and discuss emerging issues related to code quality, security, and developer experience.",
        "Publication date": "25 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.17910"
    },
    {
        "ID": 116,
        "Title": "Specify What? Enhancing Neural Specification Synthesis by Symbolic Methods",
        "Authors": [
            "George Granberry",
            "Wolfgang Ahrendt",
            "Moa Johansson"
        ],
        "Abstract": "We investigate how combinations of Large Language Models (LLMs) and symbolic analyses can be used to synthesise specifications of C programs. The LLM prompts are augmented with outputs from two formal methods tools in the Frama-C ecosystem, Pathcrawler and EVA, to produce C program annotations in the specification language ACSL. We demonstrate how the addition of symbolic analysis to the workflow impacts the quality of annotations: information about input/output examples from Pathcrawler produce more context-aware annotations, while the inclusion of EVA reports yields annotations more attuned to runtime errors. In addition, we show that the method infers rather the programs intent than its behaviour, by generating specifications for buggy programs and observing robustness of the result against bugs.",
        "Publication date": "18 September, 2024",
        "Link": "https://arxiv.org/pdf/2406.15540"
    },
    {
        "ID": 117,
        "Title": "CS1-LLM: Integrating LLMs into CS1 Instruction",
        "Authors": [
            "Annapurna Vadaparty",
            "Daniel Zingaro",
            "David H. Smith IV",
            "Mounika Padala",
            "Christine Alvarado",
            "Jamie Gorson Benario",
            "Leo Porter"
        ],
        "Abstract": "The recent, widespread availability of Large Language Models (LLMs) like ChatGPT and GitHub Copilot may impact introductory programming courses (CS1) both in terms of what should be taught and how to teach it. Indeed, recent research has shown that LLMs are capable of solving the majority of the assignments and exams we previously used in CS1. In addition, professional software engineers are often using these tools, raising the question of whether we should be training our students in their use as well. This experience report describes a CS1 course at a large research-intensive university that fully embraces the use of LLMs from the beginning of the course. To incorporate the LLMs, the course was intentionally altered to reduce emphasis on syntax and writing code from scratch. Instead, the course now emphasizes skills needed to successfully produce software with an LLM. This includes explaining code, testing code, and decomposing large problems into small functions that are solvable by an LLM. In addition to frequent, formative assessments of these skills, students were given three large, open-ended projects in three separate domains (data science, image processing, and game design) that allowed them to showcase their creativity in topics of their choosing. In an end-of-term survey, students reported that they appreciated learning with the assistance of the LLM and that they interacted with the LLM in a variety of ways when writing code. We provide lessons learned for instructors who may wish to incorporate LLMs into their course.",
        "Publication date": "17 April, 2024",
        "Link": "https://arxiv.org/pdf/2406.15379"
    },
    {
        "ID": 118,
        "Title": "ASTERIX: Module for modelling the water flow on vegetated hillslopes",
        "Authors": [
            "Stelian Ion",
            "Dorin Marinescu",
            "Stefan-Gicu Cruceanu"
        ],
        "Abstract": "The paper presents ASTERIX, an open source software for numerical integration of an extended Saint-Venant system of equations used as a mathematical tool to model the water flow from laboratory up to large-scale spatial domains applying physically-based principles of fluid mechanics. Many in-situ observations have shown that the plant cover plays a key roll in controlling the hydrological flux at a catchment scale. The plant roots facilitate the infiltration processes, the canopy intercept some proportion of rain, and plant stems slow down the flow. In case of heavy rains, the infiltration and interception processes cease in a short time, the remaining rainfall gives rise to the Hortonian overland flow and the flash flood is thus initiated. In this context, the following problem is also addressed in the article: how do the gradient of soil surface and the density of the plant cover influence the water dynamics in the Hortonian flow? The mathematical model and ASTERIX were kept as simple as possible in order to be accessible to a wide range of stakeholders interested in understanding the complex processes behind the water flow on hillslopes covered by plants.\n  The software is written in C programming language and it is free under GNU license. It was tested on a series of benchmark problems, laboratory experiments, and theoretical problems; and the results have shown a good agreement with the theoretical or measured data.",
        "Publication date": "21 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.14933"
    },
    {
        "ID": 119,
        "Title": "CREF: An LLM-based Conversational Software Repair Framework for Programming Tutors",
        "Authors": [
            "Boyang Yang",
            "Haoye Tian",
            "Weiguo Pian",
            "Haoran Yu",
            "Haitao Wang",
            "Jacques Klein",
            "TegawendÃ© F. BissyandÃ©",
            "Shunfu Jin"
        ],
        "Abstract": "Program repair techniques offer cost-saving benefits for debugging within software development and programming education scenarios. With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, it is crucial to recognize that existing repair benchmarks may have influenced LLM training data, potentially causing data leakage. To evaluate LLMs' realistic repair capabilities, (1) we introduce an extensive, non-crawled benchmark, referred to as TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses the repair performance of 12 LLMs on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (2) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their performance in repairing defects. Among these types, tutor guidance was found to be the most effective information in enhancing LLM repair capabilities. To fully harness LLMs' conversational capabilities and the benefits of augmented information, (3) we introduce a novel conversational semi-automatic repair framework CREF assisting human tutor. It demonstrates a remarkable AVG-5 improvement of 17.2%-24.6% compared to the baseline, achieving an impressive AVG-5 of 76.6% when utilizing GPT-4. These results highlight the potential for enhancing LLMs' repair capabilities through interactions with tutors and historical conversations involving incorrect responses. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors' workload and improving students' learning experience, while also showcasing its promise for facilitating other software engineering tasks, such as code review.",
        "Publication date": "8 July, 2024",
        "Link": "https://arxiv.org/pdf/2406.13972"
    },
    {
        "ID": 120,
        "Title": "Computationally efficient multi-level Gaussian process regression for functional data observed under completely or partially regular sampling designs",
        "Authors": [
            "Adam Gorm Hoffmann",
            "Claus Thorn EkstrÃ¸m",
            "Andreas Kryger Jensen"
        ],
        "Abstract": "Gaussian process regression is a frequently used statistical method for flexible yet fully probabilistic non-linear regression modeling. A common obstacle is its computational complexity which scales poorly with the number of observations. This is especially an issue when applying Gaussian process models to multiple functions simultaneously in various applications of functional data analysis.\n  We consider a multi-level Gaussian process regression model where a common mean function and individual subject-specific deviations are modeled simultaneously as latent Gaussian processes. We derive exact analytic and computationally efficient expressions for the log-likelihood function and the posterior distributions in the case where the observations are sampled on either a completely or partially regular grid. This enables us to fit the model to large data sets that are currently computationally inaccessible using a standard implementation. We show through a simulation study that our analytic expressions are several orders of magnitude faster compared to a standard implementation, and we provide an implementation in the probabilistic programming language Stan.",
        "Publication date": "7 August, 2024",
        "Link": "https://arxiv.org/pdf/2406.13691"
    },
    {
        "ID": 121,
        "Title": "APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts",
        "Authors": [
            "Honghua Dong",
            "Qidong Su",
            "Yubo Gao",
            "Zhaoyu Li",
            "Yangjun Ruan",
            "Gennady Pekhimenko",
            "Chris J. Maddison",
            "Xujie Si"
        ],
        "Abstract": "Large Language Models (LLMs) have become increasingly capable of handling diverse tasks with the aid of well-crafted prompts and integration of external tools, but as task complexity rises, the workflow involving LLMs can be complicated and thus challenging to implement and maintain. To address this challenge, we propose APPL, A Prompt Programming Language that acts as a bridge between computer programs and LLMs, allowing seamless embedding of prompts into Python functions, and vice versa. APPL provides an intuitive and Python-native syntax, an efficient parallelized runtime with asynchronous semantics, and a tracing module supporting effective failure diagnosis and replaying without extra costs. We demonstrate that APPL programs are intuitive, concise, and efficient through three representative scenarios: Chain-of-Thought with self-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat. Experiments on three parallelizable workflows further show that APPL can effectively parallelize independent LLM calls, with a significant speedup ratio that almost matches the estimation.",
        "Publication date": "18 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.13161"
    },
    {
        "ID": 122,
        "Title": "Can AI Beat Undergraduates in Entry-level Java Assignments? Benchmarking Large Language Models on JavaBench",
        "Authors": [
            "Jialun Cao",
            "Zhiyong Chen",
            "Jiarong Wu",
            "Shing-chi Cheung",
            "Chang Xu"
        ],
        "Abstract": "Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks involve Python, while only 5 benchmarks involve Java. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills, while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism).\n  To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench.",
        "Publication date": "10 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.12902"
    },
    {
        "ID": 123,
        "Title": "Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction",
        "Authors": [
            "Atharva Naik",
            "Kexun Zhang",
            "Nathaniel Robinson",
            "Aravind Mysore",
            "Clayton Marr",
            "Hong Sng Rebecca Byrnes",
            "Anna Cai",
            "Kalvin Chang",
            "David Mortensen"
        ],
        "Abstract": "Historical linguists have long written a kind of incompletely formalized ''program'' that converts reconstructed words in an ancestor language into words in one of its attested descendants that consist of a series of ordered string rewrite functions (called sound laws). They do this by observing pairs of words in the reconstructed language (protoforms) and the descendent language (reflexes) and constructing a program that transforms protoforms into reflexes. However, writing these programs is error-prone and time-consuming. Prior work has successfully scaffolded this process computationally, but fewer researchers have tackled Sound Law Induction (SLI), which we approach in this paper by casting it as Programming by Examples. We propose a language-agnostic solution that utilizes the programming ability of Large Language Models (LLMs) by generating Python sound law programs from sound change examples. We evaluate the effectiveness of our approach for various LLMs, propose effective methods to generate additional language-agnostic synthetic data to fine-tune LLMs for SLI, and compare our method with existing automated SLI methods showing that while LLMs lag behind them they can complement some of their weaknesses.",
        "Publication date": "18 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.12725"
    },
    {
        "ID": 124,
        "Title": "MegaVul: A C/C++ Vulnerability Dataset with Comprehensive Code Representation",
        "Authors": [
            "Chao Ni",
            "Liyu Shen",
            "Xiaohu Yang",
            "Yan Zhu",
            "Shaohua Wang"
        ],
        "Abstract": "We constructed a newly large-scale and comprehensive C/C++ vulnerability dataset named MegaVul by crawling the Common Vulnerabilities and Exposures (CVE) database and CVE-related open-source projects. Specifically, we collected all crawlable descriptive information of the vulnerabilities from the CVE database and extracted all vulnerability-related code changes from 28 Git-based websites. We adopt advanced tools to ensure the extracted code integrality and enrich the code with four different transformed representations. In total, MegaVul contains 17,380 vulnerabilities collected from 992 open-source repositories spanning 169 different vulnerability types disclosed from January 2006 to October 2023. Thus, MegaVul can be used for a variety of software security-related tasks including detecting vulnerabilities and assessing vulnerability severity. All information is stored in the JSON format for easy usage. MegaVul is publicly available on GitHub and will be continuously updated. It can be easily extended to other programming languages.",
        "Publication date": "18 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.12415"
    },
    {
        "ID": 125,
        "Title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
        "Authors": [
            "DeepSeek-AI",
            "Qihao Zhu",
            "Daya Guo",
            "Zhihong Shao",
            "Dejian Yang",
            "Peiyi Wang",
            "Runxin Xu",
            "Y. Wu",
            "Yukun Li",
            "Huazuo Gao",
            "Shirong Ma",
            "Wangding Zeng",
            "Xiao Bi",
            "Zihui Gu",
            "Hanwei Xu",
            "Damai Dai",
            "Kai Dong",
            "Liyue Zhang",
            "Yishi Piao",
            "Zhibin Gou",
            "Zhenda Xie",
            "Zhewen Hao",
            "Bingxuan Wang",
            "Junxiao Song",
            "Deli Chen"
        ],
        "Abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.",
        "Publication date": "17 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.11931"
    },
    {
        "ID": 126,
        "Title": "Practical Applications of Unidimensional Optimization Using Octave",
        "Authors": [
            "Erick Clapton de Lima Silva",
            "Francisco MÃ¡rcio Barboza"
        ],
        "Abstract": "This paper suggests integrating one-dimensional optimization methods to tackle diverse problems, emphasizing their significance in resolving practical issues and applying mathematical principles to real-world contexts. It focuses on employing the Octave programming language, backed by specific examples, to simplify the practical application of mathematical concepts and improve problem-solving abilities. The research aims to assess the effect on students' comprehension of one-dimensional optimization.",
        "Publication date": "16 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.11910"
    },
    {
        "ID": 127,
        "Title": "Data Petri Nets meet Probabilistic Programming (Extended version)",
        "Authors": [
            "Martin Kuhn",
            "Joscha GrÃ¼ger",
            "Christoph Matheja",
            "Andrey Rivkin"
        ],
        "Abstract": "Probabilistic programming (PP) is a programming paradigm that allows for writing statistical models like ordinary programs, performing simulations by running those programs, and analyzing and refining their statistical behavior using powerful inference engines. This paper takes a step towards leveraging PP for reasoning about data-aware processes. To this end, we present a systematic translation of Data Petri Nets (DPNs) into a model written in a PP language whose features are supported by most PP systems. We show that our translation is sound and provides statistical guarantees for simulating DPNs. Furthermore, we discuss how PP can be used for process mining tasks and report on a prototype implementation of our translation. We also discuss further analysis scenarios that could be easily approached based on the proposed translation and available PP tools.",
        "Publication date": "12 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.11883"
    },
    {
        "ID": 128,
        "Title": "A platform for lightweight deployment of IoT applications based on a Function-as-a-Service model",
        "Authors": [
            "SebastiÃ  SansÃ³",
            "Carlos Guerrero",
            "Isaac Lera",
            "Carlos Juiz"
        ],
        "Abstract": "This paper presents a platform to facilitate the deployment of applications in Internet of Things (IoT) devices. The platform allows to the programmers to use a Function-as-a-Service programming paradigm that are managed and configured in a Platform-as-a-Service web tool. The tool also allows to establish interoperability between the functions of the applications. The proposed platform obtained faster and easier deployments of the applications and the resource usages of the IoT devices also were lower in relation to a deployment process based in containers of Docker.",
        "Publication date": "17 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.11413"
    },
    {
        "ID": 129,
        "Title": "Program Synthesis Benchmark for Visual Programming in XLogoOnline Environment",
        "Authors": [
            "Chao Wen",
            "Jacqueline Staub",
            "Adish Singla"
        ],
        "Abstract": "Large language and multimodal models have shown remarkable successes on various benchmarks focused on specific skills such as general-purpose programming, natural language understanding, math word problem-solving, and visual question answering. However, it is unclear how well these models perform on tasks that require a combination of these skills. In this paper, we curate a novel program synthesis benchmark based on the XLogoOnline visual programming environment. The benchmark comprises 85 real-world tasks from the Mini-level of the XLogoOnline environment, each requiring a combination of different skills such as spatial planning, basic programming, and logical reasoning. Our evaluation shows that current state-of-the-art models like GPT-4V and Llama3-70B struggle to solve these tasks, achieving only 20% and 2.35% success rates. Next, we develop a fine-tuning pipeline to boost the performance of models by leveraging a large-scale synthetic training dataset with over 80000 tasks. Moreover, we showcase how emulator-driven feedback can be used to design a curriculum over training data distribution. We showcase that a fine-tuned Llama3-8B drastically outperforms GPT-4V and Llama3-70B models, and provide an in-depth analysis of the models' expertise across different skill dimensions. We will publicly release the benchmark for future research on program synthesis in visual programming.",
        "Publication date": "17 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.11334"
    },
    {
        "ID": 130,
        "Title": "GitHub Copilot: the perfect Code compLeeter?",
        "Authors": [
            "Ilja SiroÅ¡",
            "Dave SingelÃ©e",
            "Bart Preneel"
        ],
        "Abstract": "This paper aims to evaluate GitHub Copilot's generated code quality based on the LeetCode problem set using a custom automated framework. We evaluate the results of Copilot for 4 programming languages: Java, C++, Python3 and Rust. We aim to evaluate Copilot's reliability in the code generation stage, the correctness of the generated code and its dependency on the programming language, problem's difficulty level and problem's topic. In addition to that, we evaluate code's time and memory efficiency and compare it to the average human results. In total, we generate solutions for 1760 problems for each programming language and evaluate all the Copilot's suggestions for each problem, resulting in over 50000 submissions to LeetCode spread over a 2-month period. We found that Copilot successfully solved most of the problems. However, Copilot was rather more successful in generating code in Java and C++ than in Python3 and Rust. Moreover, in case of Python3 Copilot proved to be rather unreliable in the code generation phase. We also discovered that Copilot's top-ranked suggestions are not always the best. In addition, we analysed how the topic of the problem impacts the correctness rate. Finally, based on statistics information from LeetCode, we can conclude that Copilot generates more efficient code than an average human.",
        "Publication date": "17 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.11326"
    },
    {
        "ID": 131,
        "Title": "Investigating Video Reasoning Capability of Large Language Models with Tropes in Movies",
        "Authors": [
            "Hung-Ting Su",
            "Chun-Tong Chao",
            "Ya-Ching Hsu",
            "Xudong Lin",
            "Yulei Niu",
            "Hung-Yi Lee",
            "Winston H. Hsu"
        ],
        "Abstract": "Large Language Models (LLMs) have demonstrated effectiveness not only in language tasks but also in video reasoning. This paper introduces a novel dataset, Tropes in Movies (TiM), designed as a testbed for exploring two critical yet previously overlooked video reasoning skills: (1) Abstract Perception: understanding and tokenizing abstract concepts in videos, and (2) Long-range Compositional Reasoning: planning and integrating intermediate reasoning steps for understanding long-range videos with numerous frames. Utilizing tropes from movie storytelling, TiM evaluates the reasoning capabilities of state-of-the-art LLM-based approaches. Our experiments show that current methods, including Captioner-Reasoner, Large Multimodal Model Instruction Fine-tuning, and Visual Programming, only marginally outperform a random baseline when tackling the challenges of Abstract Perception and Long-range Compositional Reasoning. To address these deficiencies, we propose Face-Enhanced Viper of Role Interactions (FEVoRI) and Context Query Reduction (ConQueR), which enhance Visual Programming by fostering role interaction awareness and progressively refining movie contexts and trope queries during reasoning processes, significantly improving performance by 15 F1 points. However, this performance still lags behind human levels (40 vs. 65 F1). Additionally, we introduce a new protocol to evaluate the necessity of Abstract Perception and Long-range Compositional Reasoning for task resolution. This is done by analyzing the code generated through Visual Programming using an Abstract Syntax Tree (AST), thereby confirming the increased complexity of TiM. The dataset and code are available at: https://ander1119.github.io/TiM",
        "Publication date": "16 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.10923"
    },
    {
        "ID": 132,
        "Title": "Correctness is Demanding, Performance is Frustrating",
        "Authors": [
            "Artjoms Sinkarovs",
            "Thomas Koopman",
            "Sven-Bodo Scholz"
        ],
        "Abstract": "In this paper we demonstrate a technique for developing high performance applications with strong correctness guarantees. We use a theorem prover to derive a high-level specification of the application that includes correctness invariants of our choice. After that, within the same theorem prover, we implement an extraction of the specified application into a high-performance language of our choice. Concretely, we are using Agda to specify a framework for automatic differentiation (reverse mode) that is focused on index-safe tensors. This framework comes with an optimiser for tensor expressions and the ability to translate these expressions into SaC and C. We specify a canonical convolutional neural network within the proposed framework, compute the derivatives needed for the training phase and then demonstrate that the generated code matches the performance of hand-written code when running on a multi-core machine.",
        "Publication date": "14 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.10405"
    },
    {
        "ID": 133,
        "Title": "Out of style: Misadventures with LLMs and code style transfer",
        "Authors": [
            "Karl Munson",
            "Chih-Kai Ting",
            "Serenity Wade",
            "Anish Savla",
            "Julian Dolby",
            "Kiran Kate",
            "Kavitha Srinivas"
        ],
        "Abstract": "Like text, programs have styles, and certain programming styles are more desirable than others for program readability, maintainability, and performance. Code style transfer, however, is difficult to automate except for trivial style guidelines such as limits on line length. Inspired by the success of using language models for text style transfer, we investigate if code language models can perform code style transfer. Code style transfer, unlike text transfer, has rigorous requirements: the system needs to identify lines of code to change, change them correctly, and leave the rest of the program untouched. We designed CSB (Code Style Benchmark), a benchmark suite of code style transfer tasks across five categories including converting for-loops to list comprehensions, eliminating duplication in code, adding decorators to methods, etc. We then used these tests to see if large pre-trained code language models or fine-tuned models perform style transfer correctly, based on rigorous metrics to test that the transfer did occur, and the code still passes functional tests. Surprisingly, language models failed to perform all of the tasks, suggesting that they perform poorly on tasks that require code understanding. We will make available the large-scale corpora to help the community build better code models.",
        "Publication date": "14 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.10320"
    },
    {
        "ID": 134,
        "Title": "Enhancing non-Perl bioinformatic applications with Perl: Building novel, component based applications using Object Orientation, PDL, Alien, FFI, Inline and OpenMP",
        "Authors": [
            "Christos Argyropoulos"
        ],
        "Abstract": "Component-Based Software Engineering (CBSE) is a methodology that assembles pre-existing, re-usable software components into new applications, which is particularly relevant for fast moving, data-intensive fields such as bioinformatics. While Perl was used extensively in this field until a decade ago, more recent applications opt for a Bioconductor/R or Python. This trend represents a significantly missed opportunity for the rapid generation of novel bioinformatic applications out of pre-existing components since Perl offers a variety of abstractions that can facilitate composition. In this paper, we illustrate the utility of Perl for CBSE through a combination of Object Oriented frameworks, the Perl Data Language and facilities for interfacing with non-Perl code through Foreign Function Interfaces and inlining of foreign source code. To do so, we enhance Polyester, a RNA sequencing simulator written in R, and edlib a fast sequence similarity search library based on the edit distance. The first case study illustrates the near effortless authoring of new, highly performant Perl modules for the simulation of random numbers using the GNU Scientific Library and PDL, and proposes Perl and Perl/C alternatives to the Python tool cutadapt that is used to \"trim\" polyA tails from biological sequences. For the edlib case, we leverage the power of metaclass programming to endow edlib with coarse, process based parallelism, through the Many Core Engine (MCE) module and fine grained parallelism through OpenMP, a C/C++/Fortran Application Programming Interface for shared memory multithreaded processing. These use cases provide proof-of-concept for the Bio::SeqAlignment framework, which can organize heterogeneous components in complex memory and command-line based workflows for the construction of novel bionformatic tools to analyze data from long-read sequencing, e.g. Nanopore, sequencing platforms.",
        "Publication date": "11 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.10271"
    },
    {
        "ID": 135,
        "Title": "How and Why LLMs Use Deprecated APIs in Code Completion? An Empirical Study",
        "Authors": [
            "Chong Wang",
            "Kaifeng Huang",
            "Jian Zhang",
            "Yebo Feng",
            "Lyuye Zhang",
            "Yang Liu",
            "Xin Peng"
        ],
        "Abstract": "Large language models (LLMs), pre-trained or fine-tuned on large code corpora, have shown effectiveness in generating code completions. However, in LLM-based code completion, LLMs may struggle to use correct and up-to-date Application Programming Interfaces (APIs) due to the rapid and continuous evolution of libraries. While existing studies have highlighted issues with predicting incorrect APIs, the specific problem of deprecated API usage in LLM-based code completion has not been thoroughly investigated.\n  To address this gap, we conducted the first evaluation study on deprecated API usage in LLM-based code completion. This study involved seven advanced LLMs, 145 API mappings from eight popular Python libraries, and 28,125 completion prompts. The study results reveal the \\textit{status quo} and \\textit{root causes} of deprecated API usage in LLM-based code completion from the perspectives of \\textit{model}, \\textit{prompt}, and \\textit{library}. Based on these findings, we propose two lightweight fixing approaches, \\textsc{ReplaceAPI} and \\textsc{InsertPrompt}, which can serve as baseline approaches for future research on mitigating deprecated API usage in LLM-based completion. Additionally, we provide implications for future research on integrating library evolution with LLM-driven software development.",
        "Publication date": "3 July, 2024",
        "Link": "https://arxiv.org/pdf/2406.09834"
    },
    {
        "ID": 136,
        "Title": "Cross-Modality Program Representation Learning for Electronic Design Automation with High-Level Synthesis",
        "Authors": [
            "Zongyue Qin",
            "Yunsheng Bai",
            "Atefeh Sohrabizadeh",
            "Zijian Ding",
            "Ziniu Hu",
            "Yizhou Sun",
            "Jason Cong"
        ],
        "Abstract": "In recent years, domain-specific accelerators (DSAs) have gained popularity for applications such as deep learning and autonomous driving. To facilitate DSA designs, programmers use high-level synthesis (HLS) to compile a high-level description written in C/C++ into a design with low-level hardware description languages that eventually synthesize DSAs on circuits. However, creating a high-quality HLS design still demands significant domain knowledge, particularly in microarchitecture decisions expressed as \\textit{pragmas}. Thus, it is desirable to automate such decisions with the help of machine learning for predicting the quality of HLS designs, requiring a deeper understanding of the program that consists of original code and pragmas. Naturally, these programs can be considered as sequence data. In addition, these programs can be compiled and converted into a control data flow graph (CDFG). But existing works either fail to leverage both modalities or combine the two in shallow or coarse ways. We propose ProgSG, a model that allows interaction between the source code sequence modality and the graph modality in a deep and fine-grained way. To alleviate the scarcity of labeled designs, a pre-training method is proposed based on a suite of compiler's data flow analysis tasks. Experimental results show that ProgSG reduces the RMSE of design performance predictions by up to $22\\%$, and identifies designs with an average of $1.10\\times$ and $1.26\\times$ (up to $8.17\\times$ and $13.31\\times$) performance improvement in design space exploration (DSE) task compared to HARP and AutoDSE, respectively.",
        "Publication date": "17 July, 2024",
        "Link": "https://arxiv.org/pdf/2406.09606"
    },
    {
        "ID": 137,
        "Title": "A Symbolic Computing Perspective on Software Systems",
        "Authors": [
            "Arthur C. Norman",
            "Stephen M. Watt"
        ],
        "Abstract": "Symbolic mathematical computing systems have served as a canary in the coal mine of software systems for more than sixty years. They have introduced or have been early adopters of programming language ideas such ideas as dynamic memory management, arbitrary precision arithmetic and dependent types. These systems have the feature of being highly complex while at the same time operating in a domain where results are well-defined and clearly verifiable. These software systems span multiple layers of abstraction with concerns ranging from instruction scheduling and cache pressure up to algorithmic complexity of constructions in algebraic geometry. All of the major symbolic mathematical computing systems include low-level code for arithmetic, memory management and other primitives, a compiler or interpreter for a bespoke programming language, a library of high level mathematical algorithms, and some form of user interface. Each of these parts invokes multiple deep issues.\n  We present some lessons learned from this environment and free flowing opinions on topics including:\n  * Portability of software across architectures and decades;\n  * Infrastructure to embrace and infrastructure to avoid;\n  * Choosing base abstractions upon which to build;\n  * How to get the most out of a small code base;\n  * How developments in compilers both to optimise and to validate code have always been and remain of critical importance, with plenty of remaining challenges;\n  * The way in which individuals including in particular Alan Mycroft who has been able to span from hand-crafting Z80 machine code up to the most abstruse high level code analysis techniques are needed, and\n  * Why it is important to teach full-stack thinking to the next generation.",
        "Publication date": "13 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.09085"
    },
    {
        "ID": 138,
        "Title": "CGP++ : A Modern C++ Implementation of Cartesian Genetic Programming",
        "Authors": [
            "Roman Kalkreuth",
            "Thomas Baeck"
        ],
        "Abstract": "The reference implementation of Cartesian Genetic Programming (CGP) was written in the C programming language. C inherently follows a procedural programming paradigm, which entails challenges in providing a reusable and scalable implementation model for complex structures and methods. Moreover, due to the limiting factors of C, the reference implementation of CGP does not provide a generic framework and is therefore restricted to a set of predefined evaluation types. Besides the reference implementation, we also observe that other existing implementations are limited with respect to the features provided. In this work, we therefore propose the first version of a modern C++ implementation of CGP that pursues object-oriented design and generic programming paradigm to provide an efficient implementation model that can facilitate the discovery of new problem domains and the implementation of complex advanced methods that have been proposed for CGP over time. With the proposal of our new implementation, we aim to generally promote interpretability, accessibility and reproducibility in the field of CGP.",
        "Publication date": "13 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.09038"
    },
    {
        "ID": 139,
        "Title": "McEval: Massively Multilingual Code Evaluation",
        "Authors": [
            "Linzheng Chai",
            "Shukai Liu",
            "Jian Yang",
            "Yuwei Yin",
            "Ke Jin",
            "Jiaheng Liu",
            "Tao Sun",
            "Ge Zhang",
            "Changyu Ren",
            "Hongcheng Guo",
            "Zekun Wang",
            "Boyang Wang",
            "Xianjie Wu",
            "Bing Wang",
            "Tongliang Li",
            "Liqun Yang",
            "Sufeng Duan",
            "Zhoujun Li"
        ],
        "Abstract": "Code large language models (LLMs) have shown remarkable advances in code understanding, completion, and generation tasks. Programming benchmarks, comprised of a selection of code challenges and corresponding test cases, serve as a standard to evaluate the capability of different LLMs in such tasks. However, most existing benchmarks primarily focus on Python and are still restricted to a limited number of languages, where other languages are translated from the Python samples (e.g. MultiPL-E) degrading the data diversity. To further facilitate the research of code LLMs, we propose a massively multilingual code benchmark covering 40 programming languages (McEval) with 16K test samples, which substantially pushes the limits of code LLMs in multilingual scenarios. The benchmark contains challenging code completion, understanding, and generation evaluation tasks with finely curated massively multilingual instruction corpora McEval-Instruct. In addition, we introduce an effective multilingual coder mCoder trained on McEval-Instruct to support multilingual programming language generation. Extensive experimental results on McEval show that there is still a difficult journey between open-source models and closed-source LLMs (e.g. GPT-series models) in numerous languages. The instruction corpora, evaluation benchmark, and leaderboard are available at \\url{https://mceval.github.io/}.",
        "Publication date": "11 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.07436"
    },
    {
        "ID": 140,
        "Title": "Towards a Translation Framework To Bridge The Classical-Quantum Programming Gap",
        "Authors": [
            "Matteo Esposito",
            "Maryam Tavassoli Sabzevari",
            "Boshuai Ye",
            "Davide Falessi",
            "Arif Ali Khan",
            "Davide Taibi"
        ],
        "Abstract": "Quantum computing, albeit readily available as hardware or emulated on the cloud, is still far from being available in general regarding complex programming paradigms and learning curves. This vision paper introduces $Classi|Q\\rangle$, a translation framework idea to bridge Classical and Quantum Computing by translating high-level programming languages, e.g., Python or C++, into a low-level language, e.g., Quantum Assembly. Our idea paper serves as a blueprint for ongoing efforts in quantum software engineering, offering a roadmap for further $Classi|Q\\rangle$ development to meet the diverse needs of researchers and practitioners. $Classi|Q\\rangle$ is designed to empower researchers and practitioners with no prior quantum experience to harness the potential of hybrid quantum computation. We also discuss future enhancements to $Classi|Q\\rangle$, including support for additional quantum languages, improved optimization strategies, and integration with emerging quantum computing platforms.",
        "Publication date": "1 July, 2024",
        "Link": "https://arxiv.org/pdf/2406.06764"
    },
    {
        "ID": 141,
        "Title": "PretVM: Predictable, Efficient Virtual Machine for Real-Time Concurrency",
        "Authors": [
            "Shaokai Lin",
            "Erling Jellum",
            "Mirco Theile",
            "Tassilo Tanneberger",
            "Binqi Sun",
            "Chadlia Jerad",
            "Ruomu Xu",
            "Guangyu Feng",
            "Christian Menard",
            "Marten Lohstroh",
            "Jeronimo Castrillon",
            "Sanjit Seshia",
            "Edward Lee"
        ],
        "Abstract": "This paper introduces the Precision-Timed Virtual Machine (PretVM), an intermediate platform facilitating the execution of quasi-static schedules compiled from a subset of programs written in the Lingua Franca (LF) coordination language. The subset consists of those programs that in principle should have statically verifiable and predictable timing behavior. The PretVM provides a schedule with well-defined worst-case timing bounds. The PretVM provides a clean separation between application logic and coordination logic, yielding more analyzable program executions. Experiments compare the PretVM against the default (more dynamic) LF scheduler and show that it delivers time-accurate deterministic execution.",
        "Publication date": "25 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.06253"
    },
    {
        "ID": 142,
        "Title": "An extension of C++ with memory-centric specifications for HPC to reduce memory footprints and streamline MPI development",
        "Authors": [
            "Pawel K. Radtke",
            "Cristian G. Barrera-Hinojosa",
            "Mladen Ivkovic",
            "Tobias Weinzierl"
        ],
        "Abstract": "The C++ programming language and its cousins lean towards a memory-inefficient storage of structs: The compiler inserts helper bits into the struct such that individual attributes align with bytes, and it adds additional bytes aligning attributes with cache lines, while it is not able to exploit knowledge about the range of integers, enums or bitsets to bring the memory footprint down. Furthermore, the language provides neither support for data exchange via MPI nor for arbitrary floating-point precision formats. If developers need to have a low memory footprint and MPI datatypes over structs which exchange only minimal data, they have to manipulate the data and to write MPI datatypes manually. We propose a C++ language extension based upon C++ attributes through which developers can guide the compiler what memory arrangements would be beneficial: Can multiple booleans be squeezed into one bit field, do floats hold fewer significant bits than in the IEEE standard, or does the code require a user-defined MPI datatype for certain subsets of attributes? The extension offers the opportunity to fall back to normal alignment and padding rules via plain C++ assignments, no dependencies upon external libraries are introduced, and the resulting code remains standard C++. Our work implements the language annotations within LLVM and demonstrates their potential impact, both upon the runtime and the memory footprint, through smoothed particle hydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of performance and development productivity.",
        "Publication date": "25 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.06095"
    },
    {
        "ID": 143,
        "Title": "Coconut Libtool: Bridging Textual Analysis Gaps for Non-Programmers",
        "Authors": [
            "Faizhal Arif Santosa",
            "Manika Lamba",
            "Crissandra George",
            "J. Stephen Downie"
        ],
        "Abstract": "In the era of big and ubiquitous data, professionals and students alike are finding themselves needing to perform a number of textual analysis tasks. Historically, the general lack of statistical expertise and programming skills has stopped many with humanities or social sciences backgrounds from performing and fully benefiting from such analyses. Thus, we introduce Coconut Libtool (www.coconut-libtool.com/), an open-source, web-based application that utilizes state-of-the-art natural language processing (NLP) technologies. Coconut Libtool analyzes text data from customized files and bibliographic databases such as Web of Science, Scopus, and Lens. Users can verify which functions can be performed with the data they have. Coconut Libtool deploys multiple algorithmic NLP techniques at the backend, including topic modeling (LDA, Biterm, and BERTopic algorithms), network graph visualization, keyword lemmatization, and sunburst visualization. Coconut Libtool is the people-first web application designed to be used by professionals, researchers, and students in the information sciences, digital humanities, and computational social sciences domains to promote transparency, reproducibility, accessibility, reciprocity, and responsibility in research practices.",
        "Publication date": "9 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.05949"
    },
    {
        "ID": 144,
        "Title": "Compilation Quotient (CQ): A Metric for the Compilation Hardness of Programming Languages",
        "Authors": [
            "Vince Szabo",
            "Dominik Winterer",
            "Zhendong Su"
        ],
        "Abstract": "Today's programmers can choose from an exceptional range of programming languages, each with its own traits, purpose, and complexity. A key aspect of a language's complexity is how hard it is to compile programs in the language. While most programmers have an intuition about compilation hardness for different programming languages, no metric exists to quantify it. We introduce the compilation quotient (CQ), a metric to quantify the compilation hardness of compiled programming languages. The key idea is to measure the compilation success rates of programs sampled from context-free grammars. To this end, we fairly sample over 12 million programs in total. CQ ranges between 0 and 100, where 0 indicates that no programs compile, and 100 means that all programs compile. Our findings on 12 popular compiled programming languages show high variation in CQ. C has a CQ of 48.11, C++ has 0.60, Java has 0.27 and Haskell has 0.13. Strikingly, Rust's CQ is nearly 0, and for C, even a large fraction of very sizable programs compile. We believe CQ can help understand the differences of compiled programming languages better and help language designers.",
        "Publication date": "7 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.04778"
    },
    {
        "ID": 145,
        "Title": "StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation",
        "Authors": [
            "Weike Fang",
            "Zhejian Zhou",
            "Junzhou He",
            "Weihang Wang"
        ],
        "Abstract": "WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.",
        "Publication date": "6 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.04568"
    },
    {
        "ID": 146,
        "Title": "TESTEVAL: Benchmarking Large Language Models for Test Case Generation",
        "Authors": [
            "Wenhan Wang",
            "Chenyuan Yang",
            "Zhijie Wang",
            "Yuheng Huang",
            "Zhaoyang Chu",
            "Da Song",
            "Lingming Zhang",
            "An Ran Chen",
            "Lei Ma"
        ],
        "Abstract": "Testing plays a crucial role in the software development cycle, enabling the detection of bugs, vulnerabilities, and other undesirable behaviors. To perform software testing, testers need to write code snippets that execute the program under test. Recently, researchers have recognized the potential of large language models (LLMs) in software testing. However, there remains a lack of fair comparisons between different LLMs in terms of test case generation capabilities.\n  In this paper, we propose TESTEVAL, a novel benchmark for test case generation with LLMs. We collect 210 Python programs from an online programming platform, LeetCode, and design three different tasks: overall coverage, targeted line/branch coverage, and targeted path coverage. We further evaluate sixteen popular LLMs, including both commercial and open-source ones, on TESTEVAL. We find that generating test cases to cover specific program lines/branches/paths is still challenging for current LLMs, indicating a lack of ability to comprehend program logic and execution paths. We have open-sourced our dataset and benchmark pipelines at https://llm4softwaretesting.github.io to contribute and accelerate future research on LLMs for software testing.",
        "Publication date": "6 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.04531"
    },
    {
        "ID": 147,
        "Title": "VHDL-Eval: A Framework for Evaluating Large Language Models in VHDL Code Generation",
        "Authors": [
            "Prashanth Vijayaraghavan",
            "Luyao Shi",
            "Stefano Ambrogio",
            "Charles Mackin",
            "Apoorva Nitsure",
            "David Beymer",
            "Ehsan Degan"
        ],
        "Abstract": "With the unprecedented advancements in Large Language Models (LLMs), their application domains have expanded to include code generation tasks across various programming languages. While significant progress has been made in enhancing LLMs for popular programming languages, there exists a notable gap in comprehensive evaluation frameworks tailored for Hardware Description Languages (HDLs), particularly VHDL. This paper addresses this gap by introducing a comprehensive evaluation framework designed specifically for assessing LLM performance in VHDL code generation task. We construct a dataset for evaluating LLMs on VHDL code generation task. This dataset is constructed by translating a collection of Verilog evaluation problems to VHDL and aggregating publicly available VHDL problems, resulting in a total of 202 problems. To assess the functional correctness of the generated VHDL code, we utilize a curated set of self-verifying testbenches specifically designed for those aggregated VHDL problem set. We conduct an initial evaluation of different LLMs and their variants, including zero-shot code generation, in-context learning (ICL), and Parameter-efficient fine-tuning (PEFT) methods. Our findings underscore the considerable challenges faced by existing LLMs in VHDL code generation, revealing significant scope for improvement. This study emphasizes the necessity of supervised fine-tuning code generation models specifically for VHDL, offering potential benefits to VHDL designers seeking efficient code generation solutions.",
        "Publication date": "5 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.04379"
    },
    {
        "ID": 148,
        "Title": "Advancing The Robotics Software Development Experience: Bridging Julia's Performance and Python's Ecosystem",
        "Authors": [
            "Gustavo Nunes Goretkin",
            "Joseph Carpinelli",
            "Andy Park"
        ],
        "Abstract": "Robotics programming typically involves a trade-off between the ease of use offered by Python and the run-time performance of C++. While multi-language architectures address this trade-off by coupling Python's ergonomics with C++'s speed, they introduce complexity at the language interface. This paper proposes using Julia for performance-critical tasks within Python ROS 2 applications, providing an elegant solution that streamlines the development process without disrupting the existing Python workflow.",
        "Publication date": "5 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.03677"
    },
    {
        "ID": 149,
        "Title": "MESS: Modern Electronic Structure Simulations",
        "Authors": [
            "Hatem Helal",
            "Andrew Fitzgibbon"
        ],
        "Abstract": "Electronic structure simulation (ESS) has been used for decades to provide quantitative scientific insights on an atomistic scale, enabling advances in chemistry, biology, and materials science, among other disciplines. Following standard practice in scientific computing, the software packages driving these studies have been implemented in compiled languages such as FORTRAN and C. However, the recent introduction of machine learning (ML) into these domains has meant that ML models must be coded in these languages, or that complex software bridges have to be built between ML models in Python and these large compiled software systems. This is in contrast with recent progress in modern ML frameworks which aim to optimise both ease of use and high performance by harnessing hardware acceleration of tensor programs defined in Python. We introduce MESS: a modern electronic structure simulation package implemented in JAX; porting the ESS code to the ML world. We outline the costs and benefits of following the software development practices used in ML for this important scientific workload. MESS shows significant speedups n widely available hardware accelerators and simultaneously opens a clear pathway towards combining ESS with ML. MESS is available at https://github.com/graphcore-research/mess.",
        "Publication date": "5 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.03121"
    },
    {
        "ID": 150,
        "Title": "ShadowBound: Efficient Heap Memory Protection Through Advanced Metadata Management and Customized Compiler Optimization",
        "Authors": [
            "Zheng Yu",
            "Ganxiang Yang",
            "Xinyu Xing"
        ],
        "Abstract": "In software development, the prevalence of unsafe languages such as C and C++ introduces potential vulnerabilities, especially within the heap, a pivotal component for dynamic memory allocation. Despite its significance, heap management complexities have made heap corruption pervasive, posing severe threats to system security. While prior solutions aiming for temporal and spatial memory safety exhibit overheads deemed impractical, we present ShadowBound, a unique heap memory protection design. At its core, ShadowBound is an efficient out-of-bounds defense that can work with various use-after-free defenses (e.g. MarkUs, FFMalloc, PUMM) without compatibility constraints. We harness a shadow memory-based metadata management mechanism to store heap chunk boundaries and apply customized compiler optimizations tailored for boundary checking. We implemented ShadowBound atop the LLVM framework and integrated three state-of-the-art use-after-free defenses. Our evaluations show that ShadowBound provides robust heap protection with minimal time and memory overhead, suggesting its effectiveness and efficiency in safeguarding real-world programs against prevalent heap vulnerabilities.",
        "Publication date": "23 September, 2024",
        "Link": "https://arxiv.org/pdf/2406.02023"
    },
    {
        "ID": 151,
        "Title": "From Effectiveness to Efficiency: Comparative Evaluation of Code Generated by LCGMs for Bilingual Programming Questions",
        "Authors": [
            "Weipeng Jiang",
            "Xuanqi Gao",
            "Juan Zhai",
            "Shiqing Ma",
            "Xiaoyu Zhang",
            "Chao Shen"
        ],
        "Abstract": "Large Code Generation Models (LCGMs) have garnered significant attention and achieved promising results across various programming tasks. However, concerns arise regarding performance when using non-English prompts, as these models are primarily trained on English-centric corpora, and most programming language tokens resemble English. Existing benchmarks often rely on English programming questions and limited manual unit test cases, inadequately assessing LCGM-generated code quality. This paper investigates code quality differences, specifically effectiveness and efficiency, when employing different natural languages as inputs, focusing on Chinese and English due to their prominent corpora and LCGM availability. Evaluating LCGM-generated code quality under bilingual inputs presents three challenges: (1) lack of high-quality bilingual programming question datasets, (2) insufficient unit test cases for comprehensive correctness verification, and (3) limited support for comparing generated code performance. To address these challenges, we curated a test suite of 52 bilingual programming questions and developed automated input generators for each. We enhanced correctness verification by sampling larger unit test cases and estimated code performance by profiling execution time relative to input size growth. Using this framework, we conducted an empirical study on six state-of-the-art LCGMs. The results revealed that LCGM-generated code exhibits varying bilingual correctness on an average of 10.5% of tasks, with 39.5% of correct code showing diverse bilingual performance differences. Our findings suggested LCGMs may not consistently generate high-quality code across different languages, providing insights for future research directions.",
        "Publication date": "1 June, 2024",
        "Link": "https://arxiv.org/pdf/2406.00602"
    },
    {
        "ID": 152,
        "Title": "Towards LLM-Powered Verilog RTL Assistant: Self-Verification and Self-Correction",
        "Authors": [
            "Hanxian Huang",
            "Zhenghan Lin",
            "Zixuan Wang",
            "Xin Chen",
            "Ke Ding",
            "Jishen Zhao"
        ],
        "Abstract": "We explore the use of Large Language Models (LLMs) to generate high-quality Register-Transfer Level (RTL) code with minimal human interference. The traditional RTL design workflow requires human experts to manually write high-quality RTL code, which is time-consuming and error-prone. With the help of emerging LLMs, developers can describe their requirements to LLMs which then generate corresponding code in Python, C, Java, and more. Adopting LLMs to generate RTL design in hardware description languages is not trivial, given the complex nature of hardware design and the generated design has to meet the timing and physical constraints.\n  We propose VeriAssist, an LLM-powered programming assistant for Verilog RTL design workflow. VeriAssist takes RTL design descriptions as input and generates high-quality RTL code with corresponding test benches. VeriAssist enables the LLM to self-correct and self-verify the generated code by adopting an automatic prompting system and integrating RTL simulator in the code generation loop. To generate an RTL design, VeriAssist first generates the initial RTL code and corresponding test benches, followed by a self-verification step that walks through the code with test cases to reason the code behavior at different time steps, and finally it self-corrects the code by reading the compilation and simulation results and generating final RTL code that fixes errors in compilation and simulation. This design fully leverages the LLMs' capabilities on multi-turn interaction and chain-of-thought reasoning to improve the quality of the generated code. We evaluate VeriAssist with various benchmark suites and find it significantly improves both syntax and functionality correctness over existing LLM implementations, thus minimizing human intervention and making RTL design more accessible to novice designers.",
        "Publication date": "31 May, 2024",
        "Link": "https://arxiv.org/pdf/2406.00115"
    },
    {
        "ID": 153,
        "Title": "A Survey Study on the State of the Art of Programming Exercise Generation using Large Language Models",
        "Authors": [
            "Eduard Frankford",
            "Ingo HÃ¶hn",
            "Clemens Sauerwein",
            "Ruth Breu"
        ],
        "Abstract": "This paper analyzes Large Language Models (LLMs) with regard to their programming exercise generation capabilities. Through a survey study, we defined the state of the art, extracted their strengths and weaknesses and finally proposed an evaluation matrix, helping researchers and educators to decide which LLM is the best fitting for the programming exercise generation use case. We also found that multiple LLMs are capable of producing useful programming exercises. Nevertheless, there exist challenges like the ease with which LLMs might solve exercises generated by LLMs. This paper contributes to the ongoing discourse on the integration of LLMs in education.",
        "Publication date": "30 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.20183"
    },
    {
        "ID": 154,
        "Title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
        "Authors": [
            "Zhaoxi Zhang",
            "Xiaomei Zhang",
            "Yanjun Zhang",
            "Leo Yu Zhang",
            "Chao Chen",
            "Shengshan Hu",
            "Asif Gill",
            "Shirui Pan"
        ],
        "Abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
        "Publication date": "30 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.19677"
    },
    {
        "ID": 155,
        "Title": "SpecTra: Enhancing the Code Translation Ability of Language Models by Generating Multi-Modal Specifications",
        "Authors": [
            "Vikram Nitin",
            "Rahul Krishna",
            "Baishakhi Ray"
        ],
        "Abstract": "Large language models (LLMs) are increasingly being used for the task of automated code translation, which has important real-world applications. However, most existing approaches use only the source code of a program as an input to an LLM, and do not consider the different kinds of specifications that can be extracted from a program. In this paper, we propose SpecTra, a multi-stage approach that uses a novel self-consistency filter to first generate high-quality static specifications, test cases, and natural language descriptions from a given program, and then uses these along with the source code to improve the quality of LLM-generated translations. We evaluate SpecTra on three code translation tasks - C to Rust, C to Go, and JavaScript to TypeScript - and show that it can enhance the performance of six popular LLMs on these tasks by up to 10 percentage points and a relative improvement of 26\\%. Our research suggests that generating high-quality specifications could be a promising and efficient way to improve the performance of LLMs for code translation. We make our code and data available, anonymized for review.",
        "Publication date": "10 July, 2024",
        "Link": "https://arxiv.org/pdf/2405.18574"
    },
    {
        "ID": 156,
        "Title": "DSDL: Data Set Description Language for Bridging Modalities and Tasks in AI Data",
        "Authors": [
            "Bin Wang",
            "Linke Ouyang",
            "Fan Wu",
            "Wenchang Ning",
            "Xiao Han",
            "Zhiyuan Zhao",
            "Jiahui Peng",
            "Yiying Jiang",
            "Dahua Lin",
            "Conghui He"
        ],
        "Abstract": "In the era of artificial intelligence, the diversity of data modalities and annotation formats often renders data unusable directly, requiring understanding and format conversion before it can be used by researchers or developers with different needs. To tackle this problem, this article introduces a framework called Dataset Description Language (DSDL) that aims to simplify dataset processing by providing a unified standard for AI datasets. DSDL adheres to the three basic practical principles of generic, portable, and extensible, using a unified standard to express data of different modalities and structures, facilitating the dissemination of AI data, and easily extending to new modalities and tasks. The standardized specifications of DSDL reduce the workload for users in data dissemination, processing, and usage. To further improve user convenience, we provide predefined DSDL templates for various tasks, convert mainstream datasets to comply with DSDL specifications, and provide comprehensive documentation and DSDL tools. These efforts aim to simplify the use of AI data, thereby improving the efficiency of AI development.",
        "Publication date": "28 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.18315"
    },
    {
        "ID": 157,
        "Title": "Bringing Rust to Safety-Critical Systems in Space",
        "Authors": [
            "Lukas Seidel",
            "Julian Beier"
        ],
        "Abstract": "The development of safety-critical aerospace systems is traditionally dominated by the C language. Its language characteristics make it trivial to accidentally introduce memory safety issues resulting in undefined behavior or security vulnerabilities. The Rust language aims to drastically reduce the chance of introducing bugs and consequently produces overall more secure and safer code. However, due to its relatively short lifespan, industry adaption in safety-critical environments is still lacking. This work provides a set of recommendations for the development of safety-critical space systems in Rust. Our recommendations are based on insights from our multi-fold contributions towards safer and more secure aerospace systems: We provide a comprehensive overview of ongoing efforts to adapt Rust for safety-critical system programming, highlighting its potential to enhance system robustness. Next, we introduce a procedure for partially rewriting C-based systems in Rust, offering a pragmatic pathway to improving safety without necessitating a full system overhaul. During the execution of our rewriting case study, we identify and fix three previously undiscovered vulnerabilities in a popular open-source satellite communication protocol. Finally, we introduce a new Rust compiler target configuration for bare metal PowerPC. With this, we aim to broaden Rust's applicability in space-oriented projects, as the architecture is commonly encountered in the domain, e.g., in the James Webb Space Telescope.",
        "Publication date": "28 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.18135"
    },
    {
        "ID": 158,
        "Title": "Towards Integrating Emerging AI Applications in SE Education",
        "Authors": [
            "Michael Vierhauser",
            "Iris Groher",
            "Tobias Antensteiner",
            "Clemens Sauerwein"
        ],
        "Abstract": "Artificial Intelligence (AI) approaches have been incorporated into modern learning environments and software engineering (SE) courses and curricula for several years. However, with the significant rise in popularity of large language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in particular in the last year, educators are faced with rapidly changing classroom environments and disrupted teaching principles. Examples range from programming assignment solutions that are fully generated via ChatGPT, to various forms of cheating during exams. However, despite these negative aspects and emerging challenges, AI tools in general, and LLM applications in particular, can also provide significant opportunities in a wide variety of SE courses, supporting both students and educators in meaningful ways. In this early research paper, we present preliminary results of a systematic analysis of current trends in the area of AI, and how they can be integrated into university-level SE curricula, guidelines, and approaches to support both instructors and learners. We collected both teaching and research papers and analyzed their potential usage in SE education, using the ACM Computer Science Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of opportunities for AI applications and further research areas.",
        "Publication date": "3 June, 2024",
        "Link": "https://arxiv.org/pdf/2405.18062"
    },
    {
        "ID": 159,
        "Title": "AbstractBeam: Enhancing Bottom-Up Program Synthesis using Library Learning",
        "Authors": [
            "Janis Zenkner",
            "Lukas Dierkes",
            "Tobias Sesterhenn",
            "Chrisitan Bartelt"
        ],
        "Abstract": "LambdaBeam is a state-of-the-art, execution-guided algorithm for program synthesis that utilizes higher-order functions, lambda functions, and iterative loops within a Domain-Specific Language (DSL). LambdaBeam generates each program from scratch but does not take advantage of the frequent recurrence of program blocks or subprograms commonly found in specific domains, such as loops for list traversal. To address this limitation, we introduce AbstractBeam: a novel program synthesis framework designed to enhance LambdaBeam by leveraging Library Learning. AbstractBeam identifies and integrates recurring program structures into the DSL, optimizing the synthesis process. Our experimental evaluations demonstrate that AbstractBeam statistically significantly (p < 0.05) outperforms LambdaBeam in the integer list manipulation domain. Beyond solving more tasks, AbstractBeam's program synthesis is also more efficient, requiring less time and fewer candidate programs to generate a solution. Furthermore, our findings indicate that Library Learning effectively enhances program synthesis in domains that are not explicitly designed to showcase its advantages, thereby highlighting the broader applicability of Library Learning.",
        "Publication date": "12 September, 2024",
        "Link": "https://arxiv.org/pdf/2405.17514"
    },
    {
        "ID": 160,
        "Title": "Cost-Effective Online Multi-LLM Selection with Versatile Reward Models",
        "Authors": [
            "Xiangxiang Dai",
            "Jin Li",
            "Xutong Liu",
            "Anqi Yu",
            "John C. S. Lui"
        ],
        "Abstract": "With the rapid advancement of large language models (LLMs), the diversity of multi-LLM tasks and the variability in their pricing structures have become increasingly important, as costs can vary greatly between different LLMs. To tackle these challenges, we introduce the \\textit{C2MAB-V}, a \\underline{C}ost-effective \\underline{C}ombinatorial \\underline{M}ulti-armed \\underline{B}andit with \\underline{V}ersatile reward models for optimal LLM selection and usage. This online model differs from traditional static approaches or those reliant on a single LLM without cost consideration. With multiple LLMs deployed on a scheduling cloud and a local server dedicated to handling user queries, \\textit{C2MAB-V} facilitates the selection of multiple LLMs over a combinatorial search space, specifically tailored for various collaborative task types with different reward models. Based on our designed online feedback mechanism and confidence bound technique, \\textit{C2MAB-V} can effectively address the multi-LLM selection challenge by managing the exploration-exploitation trade-off across different models, while also balancing cost and reward for diverse tasks. The NP-hard integer linear programming problem for selecting multiple LLMs with trade-off dilemmas is addressed by: i) decomposing the integer problem into a relaxed form by the local server, ii) utilizing a discretization rounding scheme that provides optimal LLM combinations by the scheduling cloud, and iii) continual online updates based on feedback. Theoretically, we prove that \\textit{C2MAB-V} offers strict guarantees over versatile reward models, matching state-of-the-art results for regret and violations in some degenerate cases. Empirically, we show that \\textit{C2MAB-V} effectively balances performance and cost-efficiency with nine LLMs for three application scenarios.",
        "Publication date": "2 October, 2024",
        "Link": "https://arxiv.org/pdf/2405.16587"
    },
    {
        "ID": 161,
        "Title": "Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search",
        "Authors": [
            "Max Liu",
            "Chan-Hung Yu",
            "Wei-Hsu Lee",
            "Cheng-Wei Hung",
            "Yen-Chun Chen",
            "Shao-Hua Sun"
        ],
        "Abstract": "Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization. Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of program-environment interactions. To tackle this challenge, we introduce a novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the programming expertise and common sense reasoning of LLMs to enhance the efficiency of assumption-free, random-guessing search methods. We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy - an LLM is instructed to initially generate Python codes and then convert them into DSL programs. To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to consistently improve the programs. Experimental results in the Karel domain demonstrate the superior effectiveness and efficiency of our LLM-GS framework. Extensive ablation studies further verify the critical role of our Pythonic-DSL strategy and Scheduled Hill Climbing algorithm.",
        "Publication date": "26 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.16450"
    },
    {
        "ID": 162,
        "Title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code",
        "Authors": [
            "Marc Oedingen",
            "Raphael C. Engelhardt",
            "Robin Denz",
            "Maximilian Hammer",
            "Wolfgang Konen"
        ],
        "Abstract": "In recent times, large language models (LLMs) have made significant strides in generating computer code, blurring the lines between code created by humans and code produced by artificial intelligence (AI). As these technologies evolve rapidly, it is crucial to explore how they influence code generation, especially given the risk of misuse in areas like higher education. This paper explores this issue by using advanced classification techniques to differentiate between code written by humans and that generated by ChatGPT, a type of LLM. We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms - including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting - to achieve this differentiation with an impressive accuracy of 98%. For the successful combinations, we also examine their model calibration, showing that some of the models are extremely well calibrated. Additionally, we present white-box features and an interpretable Bayes classifier to elucidate critical differences between the code sources, enhancing the explainability and transparency of our approach. Both approaches work well but provide at most 85-88% accuracy. We also show that untrained humans solve the same task not better than random guessing. This study is crucial in understanding and mitigating the potential risks associated with using AI in code generation, particularly in the context of higher education, software development, and competitive programming.",
        "Publication date": "3 July, 2024",
        "Link": "https://arxiv.org/pdf/2405.15512"
    },
    {
        "ID": 163,
        "Title": "Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces",
        "Authors": [
            "Tommaso Calo",
            "Christopher J. MacLellan"
        ],
        "Abstract": "Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.",
        "Publication date": "23 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.14713"
    },
    {
        "ID": 164,
        "Title": "GPU Implementations for Midsize Integer Addition and Multiplication",
        "Authors": [
            "Cosmin E. Oancea",
            "Stephen M. Watt"
        ],
        "Abstract": "This paper explores practical aspects of using a high-level functional language for GPU-based arithmetic on ``midsize'' integers. By this we mean integers of up to about a quarter million bits, which is sufficient for most practical purposes. The goal is to understand whether it is possible to support efficient nested-parallel programs with a small, flexible code base. We report on GPU implementations for addition and multiplication of integers that fit in one CUDA block, thus leveraging temporal reuse from scratchpad memories. Our key contribution resides in the simplicity of the proposed solutions: We recognize that addition is a straightforward application of scan, which is known to allow efficient GPU implementation. For quadratic multiplication we employ a simple work-partitioning strategy that offers good temporal locality. For FFT multiplication, we efficiently map the computation in the domain of integral fields by finding ``good'' primes that enable almost-full utilization of machine words. In comparison, related work uses complex tiling strategies -- which feel too big a hammer for the job -- or uses the computational domain of reals, which may degrade the magnitude of the base in which the computation is carried. We evaluate the performance in comparison to the state-of-the-art CGBN library, authored by NvidiaLab, and report that our CUDA prototype outperforms CGBN for integer sizes higher than 32K bits, while offering comparable performance for smaller sizes. Moreover, we are, to our knowledge, the first to report that FFT multiplication outperforms the classical one on the larger sizes that still fit in a CUDA block. Finally, we examine Futhark's strengths and weaknesses for efficiently supporting such computations and find out that a compiler pass aimed at efficient sequentialization of excess parallelism would significantly improve performance.",
        "Publication date": "23 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.14642"
    },
    {
        "ID": 165,
        "Title": "Generating Exceptional Behavior Tests with Reasoning Augmented Large Language Models",
        "Authors": [
            "Jiyang Zhang",
            "Yu Liu",
            "Pengyu Nie",
            "Junyi Jessy Li",
            "Milos Gligoric"
        ],
        "Abstract": "Many popular programming languages, including C#, Java, and Python, support exceptions. Exceptions are thrown during program execution if an unwanted event happens, e.g., a method is invoked with an illegal argument value. Software developers write exceptional behavior tests (EBTs) to check that their code detects unwanted events and throws appropriate exceptions. Prior research studies have shown the importance of EBTs, but those studies also highlighted that developers put most of their efforts on \"happy paths\", e.g., paths without unwanted events. To help developers fill the gap, we present the first framework, dubbed exLong, that automatically generates EBTs. exLong is a large language model instruction-tuned from CodeLlama and embeds reasoning about traces that lead to throw statements, conditional expressions that guard throw statements, and non-exceptional behavior tests that execute similar traces. We compare exLong with the state-of-the-art models for test generation (CAT-LM) and one of the strongest foundation models (GPT3.5), as well as with analysis-based tools for test generation (Randoop and EvoSuite). Our results show that exLong outperforms existing models and tools. Furthermore, we contributed several pull requests to open-source projects and 23 EBTs generated by exLong were already accepted.",
        "Publication date": "24 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.14619"
    },
    {
        "ID": 166,
        "Title": "Evaluation of the Programming Skills of Large Language Models",
        "Authors": [
            "Luc Bryan Heitz",
            "Joun Chamas",
            "Christopher Scherb"
        ],
        "Abstract": "The advent of Large Language Models (LLM) has revolutionized the efficiency and speed with which tasks are completed, marking a significant leap in productivity through technological innovation. As these chatbots tackle increasingly complex tasks, the challenge of assessing the quality of their outputs has become paramount. This paper critically examines the output quality of two leading LLMs, OpenAI's ChatGPT and Google's Gemini AI, by comparing the quality of programming code generated in both their free versions. Through the lens of a real-world example coupled with a systematic dataset, we investigate the code quality produced by these LLMs. Given their notable proficiency in code generation, this aspect of chatbot capability presents a particularly compelling area for analysis. Furthermore, the complexity of programming code often escalates to levels where its verification becomes a formidable task, underscoring the importance of our study. This research aims to shed light on the efficacy and reliability of LLMs in generating high-quality programming code, an endeavor that has significant implications for the field of software development and beyond.",
        "Publication date": "23 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.14388"
    },
    {
        "ID": 167,
        "Title": "AI-Assisted Assessment of Coding Practices in Modern Code Review",
        "Authors": [
            "Manushree Vijayvergiya",
            "MaÅgorzata Salawa",
            "Ivan BudiseliÄ",
            "Dan Zheng",
            "Pascal Lamblin",
            "Marko IvankoviÄ",
            "Juanjo Carin",
            "Mateusz Lewko",
            "Jovan Andonov",
            "Goran PetroviÄ",
            "Daniel Tarlow",
            "Petros Maniatis",
            "RenÃ© Just"
        ],
        "Abstract": "Modern code review is a process in which an incremental code contribution made by a code author is reviewed by one or more peers before it is committed to the version control system. An important element of modern code review is verifying that code contributions adhere to best practices. While some of these best practices can be automatically verified, verifying others is commonly left to human reviewers. This paper reports on the development, deployment, and evaluation of AutoCommenter, a system backed by a large language model that automatically learns and enforces coding best practices. We implemented AutoCommenter for four programming languages (C++, Java, Python, and Go) and evaluated its performance and adoption in a large industrial setting. Our evaluation shows that an end-to-end system for learning and enforcing coding best practices is feasible and has a positive impact on the developer workflow. Additionally, this paper reports on the challenges associated with deploying such a system to tens of thousands of developers and the corresponding lessons learned.",
        "Publication date": "22 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.13565"
    },
    {
        "ID": 168,
        "Title": "Source-level reasoning for quantitative information flow",
        "Authors": [
            "Chris Chen",
            "Annabelle McIver",
            "Carroll Morgan"
        ],
        "Abstract": "We present a novel formal system for proving quantitative-leakage properties of programs. Based on a theory of Quantitative Information Flow (QIF) that models information leakage as a noisy communication channel, it uses \"gain-functions\" for the description and measurement of expected leaks.\n  We use a small imperative programming language, augmented with leakage features, and with it express adversaries' activities in the style of, but more generally than, the Hoare triples or expectation transformers that traditionally express deterministic or probabilistic correctness but without information flow.\n  The programs are annotated with \"gain-expressions\" that capture simple adversarial settings such as \"Guess the secret in one try.\" but also much more general ones; and our formal syntax and logic -based framework enables us to transform such gain-expressions that apply after a program has finished to ones that equivalently apply before the program has begun.\n  In that way we enable a formal proof-based reasoning system for QIF at the source level. We apply it to the %programming language we have chosen, and demonstrate its effectiveness in a number of small but sometimes intricate situations.",
        "Publication date": "22 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.13416"
    },
    {
        "ID": 169,
        "Title": "Fully Randomized Pointers",
        "Authors": [
            "Gregory J. Duck",
            "Sai Dhawal Phaye",
            "Roland H. C. Yap",
            "Trevor E. Carlson"
        ],
        "Abstract": "Software security continues to be a critical concern for programs implemented in low-level programming languages such as C and C++. Many defenses have been proposed in the current literature, each with different trade-offs including performance, compatibility, and attack resistance. One general class of defense is pointer randomization or authentication, where invalid object access (e.g., memory errors) is obfuscated or denied. Many defenses rely on the program termination (e.g., crashing) to abort attacks, with the implicit assumption that an adversary cannot \"brute force\" the defense with multiple attack attempts. However, such assumptions do not always hold, such as hardware speculative execution attacks or network servers configured to restart on error. In such cases, we argue that most existing defenses provide only weak effective security.\n  In this paper, we propose Fully Randomized Pointers (FRP) as a stronger memory error defense that is resistant to even brute force attacks. The key idea is to fully randomize pointer bits -- as much as possible while also preserving binary compatibility -- rendering the relationships between pointers highly unpredictable. Furthermore, the very high degree of randomization renders brute force attacks impractical -- providing strong effective security compared to existing work. We design a new FRP encoding that is: (1) compatible with existing binary code (without recompilation); (2) decoupled from the underlying object layout; and (3) can be efficiently decoded on-the-fly to the underlying memory address. We prototype FRP in the form of a software implementation (BlueFat) to test security and compatibility, and a proof-of-concept hardware implementation (GreenFat) to evaluate performance. We show that FRP is secure, practical, and compatible at the binary level, while a hardware implementation can achieve low performance overheads (<10%).",
        "Publication date": "21 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.12513"
    },
    {
        "ID": 170,
        "Title": "Compiler support for semi-manual AoS-to-SoA conversions with data views",
        "Authors": [
            "Pawel K. Radtke",
            "Tobias Weinzierl"
        ],
        "Abstract": "The C programming language and its cousins such as C++ stipulate the static storage of sets of structured data: Developers have to commit to one, invariant data model -- typically a structure-of-arrays (SoA) or an array-of-structs (AoS) -- unless they manually rearrange, i.e.~convert it throughout the computation. Whether AoS or SoA is favourable depends on the execution context and algorithm step. We propose a language extension based upon C++ attributes through which developers can guide the compiler what memory arrangements are to be used. The compiler can then automatically convert (parts of) the data into the format of choice prior to a calculation and convert results back afterwards. As all conversions are merely annotations, it is straightforward for the developer to experiment with different storage formats and to pick subsets of data that are subject to memory rearrangements. Our work implements the annotations within Clang and demonstrates their potential impact through a smoothed particle hydrodynamics (SPH) code.",
        "Publication date": "5 September, 2024",
        "Link": "https://arxiv.org/pdf/2405.12507"
    },
    {
        "ID": 171,
        "Title": "Efficacy of static analysis tools for software defect detection on open-source projects",
        "Authors": [
            "Jones Yeboah",
            "Saheed Popoola"
        ],
        "Abstract": "In software practice, static analysis tools remain an integral part of detecting defects in software and there have been various tools designed to run the analysis in different programming languages like Java, C++, and Python. This paper presents an empirical comparison of popular static analysis tools for identifying software defects using several datasets using Java, C++, and Python code. The study used popular analysis tools such as SonarQube, PMD, Checkstyle, and FindBugs to perform the comparison based on using the datasets. The study also used various evaluation metrics such as Precision, Recall, and F1-score to determine the performance of each analysis tool. The study results show that SonarQube performs considerably well than all other tools in terms of its defect detection across the various three programming languages. These findings remain consistent with other existing studies that also agree on SonarQube being an effective tool for defect detection in software. The study contributes to much insight on static analysis tools with different programming languages and additional information to understand the strengths and weaknesses of each analysis tool. The study also discusses the implications for software development researchers and practitioners, and future directions in this area. Our research approach aim is to provide a recommendation guideline to enable software developers, practitioners, and researchers to make the right choice on static analysis tools to detect errors in their software codes. Also, for researchers to embark on investigating and improving software analysis tools to enhance the quality and reliability of the software systems and its software development processes practice.",
        "Publication date": "20 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.12333"
    },
    {
        "ID": 172,
        "Title": "Towards Translating Real-World Code with LLMs: A Study of Translating to Rust",
        "Authors": [
            "Hasan Ferit Eniser",
            "Hanliang Zhang",
            "Cristina David",
            "Meng Wang",
            "Maria Christakis",
            "Brandon Paulsen",
            "Joey Dodds",
            "Daniel Kroening"
        ],
        "Abstract": "Large language models (LLMs) show promise in code translation - the task of translating code written in one programming language to another language - due to their ability to write code in most programming languages. However, LLM's effectiveness on translating real-world code remains largely unstudied. In this work, we perform the first substantial study on LLM-based translation to Rust by assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude 2.1, Gemini Pro, and Mixtral. We conduct our study on code extracted from real-world open source projects. To enable our study, we develop FLOURINE, an end-to-end code translation tool that uses differential fuzzing to check if a Rust translation is I/O equivalent to the original source program, eliminating the need for pre-existing test cases. As part of our investigation, we assess both the LLM's ability to produce an initially successful translation, as well as their capacity to fix a previously generated buggy one. If the original and the translated programs are not I/O equivalent, we apply a set of automated feedback strategies, including feedback to the LLM with counterexamples. Our results show that the most successful LLM can translate 47% of our benchmarks, and also provides insights into next steps for improvements.",
        "Publication date": "21 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.11514"
    },
    {
        "ID": 173,
        "Title": "A Classification-by-Retrieval Framework for Few-Shot Anomaly Detection to Detect API Injection Attacks",
        "Authors": [
            "Udi Aharon",
            "Ran Dubin",
            "Amit Dvir",
            "Chen Hajaj"
        ],
        "Abstract": "Application Programming Interface (API) Injection attacks refer to the unauthorized or malicious use of APIs, which are often exploited to gain access to sensitive data or manipulate online systems for illicit purposes. Identifying actors that deceitfully utilize an API poses a demanding problem. Although there have been notable advancements and contributions in the field of API security, there remains a significant challenge when dealing with attackers who use novel approaches that don't match the well-known payloads commonly seen in attacks. Also, attackers may exploit standard functionalities unconventionally and with objectives surpassing their intended boundaries. Thus, API security needs to be more sophisticated and dynamic than ever, with advanced computational intelligence methods, such as machine learning models that can quickly identify and respond to abnormal behavior. In response to these challenges, we propose a novel unsupervised few-shot anomaly detection framework composed of two main parts: First, we train a dedicated generic language model for API based on FastText embedding. Next, we use Approximate Nearest Neighbor search in a classification-by-retrieval approach. Our framework allows for training a fast, lightweight classification model using only a few examples of normal API requests. We evaluated the performance of our framework using the CSIC 2010 and ATRDF 2023 datasets. The results demonstrate that our framework improves API attack detection accuracy compared to the state-of-the-art (SOTA) unsupervised anomaly detection baselines.",
        "Publication date": "15 September, 2024",
        "Link": "https://arxiv.org/pdf/2405.11247"
    },
    {
        "ID": 174,
        "Title": "The Cost of Garbage Collection for State Machine Replication",
        "Authors": [
            "Zhiying Liang",
            "Vahab Jabrayilov",
            "Aleksey Charapko",
            "Abutalib Aghayev"
        ],
        "Abstract": "State Machine Replication (SMR) protocols form the backbone of many distributed systems. Enterprises and startups increasingly build their distributed systems on the cloud due to its many advantages, such as scalability and cost-effectiveness. One of the first technical questions companies face when building a system on the cloud is which programming language to use. Among many factors that go into this decision is whether to use a language with garbage collection (GC), such as Java or Go, or a language with manual memory management, such as C++ or Rust. Today, companies predominantly prefer languages with GC, like Go, Kotlin, or even Python, due to ease of development; however, there is no free lunch: GC costs resources (memory and CPU) and performance (long tail latencies due to GC pauses). While there have been anecdotal reports of reduced cloud cost and improved tail latencies when switching from a language with GC to a language with manual memory management, so far, there has not been a systematic study of the GC overhead of running an SMR-based cloud system.\n  This paper studies the overhead of running an SMR-based cloud system written in a language with GC. To this end, we design from scratch a canonical SMR system -- a MultiPaxos-based replicated in-memory key-value store -- and we implement it in C++, Java, Rust, and Go. We compare the performance and resource usage of these implementations when running on the cloud under different workloads and resource constraints and report our results. Our findings have implications for the design of cloud systems.",
        "Publication date": "18 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.11182"
    },
    {
        "ID": 175,
        "Title": "A Certified Proof Checker for Deep Neural Network Verification",
        "Authors": [
            "Remi Desmartin",
            "Omri Isac",
            "Ekaterina Komendantskaya",
            "Kathrin Stark",
            "Grant Passmore",
            "Guy Katz"
        ],
        "Abstract": "Recent advances in the verification of deep neural networks (DNNs) have opened the way for broader usage of DNN verification technology in many application areas, including safety-critical ones. DNN verifiers are themselves complex programs that have been shown to be susceptible to errors and imprecisions; this in turn has raised the question of trust in DNN verifiers. One prominent attempt to address this issue is enhancing DNN verifiers with the capability of producing proofs of their results that are subject to independent algorithmic certification (proof checking). Formulations of proof production and proof checking already exist on top of the state-of-the-art Marabou DNN verifier. The native implementation of the proof checking algorithm for Marabou was done in C++ and itself raised the question of trust in the code (e.g., in the precision of floating point calculations or guarantees for implementation soundness). Here, we present an alternative implementation of the Marabou proof checking algorithm in Imandra -- an industrial functional programming language and prover -- that allows us to obtain an implementation with formal guarantees, including proofs of mathematical results underlying the algorithm, such as the use of the Farkas lemma.",
        "Publication date": "17 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.10611"
    },
    {
        "ID": 176,
        "Title": "PyOptInterface: Design and implementation of an efficient modeling language for mathematical optimization",
        "Authors": [
            "Yue Yang",
            "Chenhui Lin",
            "Luo Xu",
            "Wenchuan Wu"
        ],
        "Abstract": "This paper introduces the design and implementation of PyOptInterface, a modeling language for mathematical optimization embedded in Python programming language. PyOptInterface uses lightweight and compact data structure to bridge high-level entities in optimization models like variables and constraints to internal indices of optimizers efficiently. It supports a variety of optimization solvers and a range of common problem classes. We provide benchmarks to exhibit the competitive performance of PyOptInterface compared with other state-of-the-art modeling languages.",
        "Publication date": "16 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.10130"
    },
    {
        "ID": 177,
        "Title": "LoRA Learns Less and Forgets Less",
        "Authors": [
            "Dan Biderman",
            "Jacob Portes",
            "Jose Javier Gonzalez Ortiz",
            "Mansheej Paul",
            "Philip Greengard",
            "Connor Jennings",
            "Daniel King",
            "Sam Havens",
            "Vitaliy Chiley",
            "Jonathan Frankle",
            "Cody Blakeney",
            "John P. Cunningham"
        ],
        "Abstract": "Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning (approximately 100K prompt-response pairs) and continued pretraining (20B unstructured tokens) data regimes. Our results show that, in the standard low-rank settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA better maintains the base model's performance on tasks outside the target domain. We show that LoRA mitigates forgetting more than common regularization techniques such as weight decay and dropout; it also helps maintain more diverse generations. Finally, we show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with LoRA.",
        "Publication date": "20 September, 2024",
        "Link": "https://arxiv.org/pdf/2405.09673"
    },
    {
        "ID": 178,
        "Title": "AKN_Regie: a bridge between digital and performing arts",
        "Authors": [
            "Georges GagnerÃ©"
        ],
        "Abstract": "In parallel with the dissemination of information technology, we note the persistence of frontiers within creative practices, in particular between the digital arts and the performing arts. Crossings of these frontiers brought to light the need for a common appropriation of digital issues. As a result of this appropriation, the AvatarStaging platform and its software dimension AKN_Regie will be described in their use to direct avatars on a mixed theatre stage. Developed with the Blueprint visual language within Epic Games' Unreal Engine, AKN_Regie offers a user interface accessible to non-programming artists. This feature will be used to describe two perspectives of appropriation of the tool: the Plugin perspective for these users and the Blueprint perspective for programming artists who want to improve the tool. These two perspectives are then completed by a C++ perspective that aligns AKN_Regie with the language with which the engine itself is programmed. The circulations between these three perspectives are finally studied by drawing on work on the ecology of collective intelligence.",
        "Publication date": "14 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.09574"
    },
    {
        "ID": 179,
        "Title": "LLMs are Meaning-Typed Code Constructs",
        "Authors": [
            "Jason Mars",
            "Yiping Kang",
            "Jayanaka Dantanarayana",
            "Chandra Irugalbandara",
            "Kugesan Sivasothynathan",
            "Lingjia Tang"
        ],
        "Abstract": "Programming with Generative AI (GenAI) models is a type of Neurosymbolic programming and has seen tremendous adoption across many domains. However, leveraging GenAI models in code today can be complex, counter-intuitive and often require specialized frameworks, leading to increased complexity. This is because it is currently unclear as to the right abstractions through which we should marry GenAI models with the nature of traditional programming code constructs. In this paper, we introduce a set of novel abstractions to help bridge the gap between Neuro- and symbolic programming. We introduce Meaning, a new specialized type that represents the underlying semantic value of traditional types (e.g., string). We make the case that GenAI models, LLMs in particular, should be reasoned as a meaning-type wrapped code construct at the language level. We formulate the problem of translation between meaning and traditional types and propose Automatic Meaning-Type Transformation (A-MTT), a runtime feature that abstracts this translation away from the developers by automatically converting between M eaning and types at the interface of LLM invocation. Leveraging this new set of code constructs and OTT, we demonstrate example implementation of neurosymbolic programs that seamlessly utilizes LLMs to solve problems in place of potentially complex traditional programming logic.",
        "Publication date": "14 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.08965"
    },
    {
        "ID": 180,
        "Title": "Transforming C++11 Code to C++03 to Support Legacy Compilation Environments",
        "Authors": [
            "GÃ¡bor Antal",
            "DÃ¡vid Havas",
            "IstvÃ¡n Siket",
            "ÃrpÃ¡d BeszÃ©des",
            "Rudolf Ferenc",
            "JÃ³zsef Mihalicza"
        ],
        "Abstract": "Newer technologies - programming languages, environments, libraries - change very rapidly. However, various internal and external constraints often prevent projects from quickly adopting to these changes. Customers may require specific platform compatibility from a software vendor, for example. In this work, we deal with such an issue in the context of the C++ programming language. Our industrial partner is required to use SDKs that support only older C++ language editions. They, however, would like to allow their developers to use the newest language constructs in their code. To address this problem, we created a source code transformation framework to automatically backport source code written according to the C++11 standard to its functionally equivalent C++03 variant. With our framework developers are free to exploit the latest language features, while production code is still built by using a restricted set of available language constructs. This paper reports on the technical details of the transformation engine, and our experiences in applying it on two large industrial code bases and four open-source systems. Our solution is freely available and open-source.",
        "Publication date": "12 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.07204"
    },
    {
        "ID": 181,
        "Title": "SonifyAR: Context-Aware Sound Generation in Augmented Reality",
        "Authors": [
            "Xia Su",
            "Jon E. Froehlich",
            "Eunyee Koh",
            "Chang Xiao"
        ],
        "Abstract": "Sound plays a crucial role in enhancing user experience and immersiveness in Augmented Reality (AR). However, current platforms lack support for AR sound authoring due to limited interaction types, challenges in collecting and specifying context information, and difficulty in acquiring matching sound assets. We present SonifyAR, an LLM-based AR sound authoring system that generates context-aware sound effects for AR experiences. SonifyAR expands the current design space of AR sound and implements a Programming by Demonstration (PbD) pipeline to automatically collect contextual information of AR events, including virtual content semantics and real world context. This context information is then processed by a large language model to acquire sound effects with Recommendation, Retrieval, Generation, and Transfer methods. To evaluate the usability and performance of our system, we conducted a user study with eight participants and created five example applications, including an AR-based science experiment, an improving case for AR headset safety, and an assisting example for low vision AR users.",
        "Publication date": "11 August, 2024",
        "Link": "https://arxiv.org/pdf/2405.07089"
    },
    {
        "ID": 182,
        "Title": "MEIC: Re-thinking RTL Debug Automation using LLMs",
        "Authors": [
            "Ke Xu",
            "Jialin Sun",
            "Yuchen Hu",
            "Xinwei Fang",
            "Weiwei Shan",
            "Xi Wang",
            "Zhe Jiang"
        ],
        "Abstract": "The deployment of Large Language Models (LLMs) for code debugging (e.g., C and Python) is widespread, benefiting from their ability to understand and interpret intricate concepts. However, in the semiconductor industry, utilising LLMs to debug Register Transfer Level (RTL) code is still insufficient, largely due to the underrepresentation of RTL-specific data in training sets. This work introduces a novel framework, Make Each Iteration Count (MEIC), which contrasts with traditional one-shot LLM-based debugging methods that heavily rely on prompt engineering, model tuning, and model training. MEIC utilises LLMs in an iterative process to overcome the limitation of LLMs in RTL code debugging, which is suitable for identifying and correcting both syntax and function errors, while effectively managing the uncertainties inherent in LLM operations. To evaluate our framework, we provide an open-source dataset comprising 178 common RTL programming errors. The experimental results demonstrate that the proposed debugging framework achieves fix rate of 93% for syntax errors and 78% for function errors, with up to 48x speedup in debugging processes when compared with experienced engineers. The Repo. of dataset and code: https://anonymous.4open.science/r/Verilog-Auto-Debug-6E7F/.",
        "Publication date": "10 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.06840"
    },
    {
        "ID": 183,
        "Title": "Benchmarking Educational Program Repair",
        "Authors": [
            "Charles Koutcheme",
            "Nicola Dainese",
            "Sami Sarsa",
            "Juho Leinonen",
            "Arto Hellas",
            "Paul Denny"
        ],
        "Abstract": "The emergence of large language models (LLMs) has sparked enormous interest due to their potential application across a range of educational tasks. For example, recent work in programming education has used LLMs to generate learning resources, improve error messages, and provide feedback on code. However, one factor that limits progress within the field is that much of the research uses bespoke datasets and different evaluation metrics, making direct comparisons between results unreliable. Thus, there is a pressing need for standardization and benchmarks that facilitate the equitable comparison of competing approaches. One task where LLMs show great promise is program repair, which can be used to provide debugging support and next-step hints to students. In this article, we propose a novel educational program repair benchmark. We curate two high-quality publicly available programming datasets, present a unified evaluation procedure introducing a novel evaluation metric rouge@k for approximating the quality of repairs, and evaluate a set of five recent models to establish baseline performance.",
        "Publication date": "8 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.05347"
    },
    {
        "ID": 184,
        "Title": "Smart Portable Computer",
        "Authors": [
            "Niladri Das"
        ],
        "Abstract": "Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the \"Portable Smart Computer,\" a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.",
        "Publication date": "8 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.05292"
    },
    {
        "ID": 185,
        "Title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
        "Authors": [
            "Charles Koutcheme",
            "Nicola Dainese",
            "Sami Sarsa",
            "Arto Hellas",
            "Juho Leinonen",
            "Paul Denny"
        ],
        "Abstract": "Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings.",
        "Publication date": "8 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.05253"
    },
    {
        "ID": 186,
        "Title": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence",
        "Authors": [
            "Mayank Mishra",
            "Matt Stallone",
            "Gaoyuan Zhang",
            "Yikang Shen",
            "Aditya Prasad",
            "Adriana Meza Soria",
            "Michele Merler",
            "Parameswaran Selvam",
            "Saptha Surendran",
            "Shivdeep Singh",
            "Manish Sethi",
            "Xuan-Hong Dang",
            "Pengyuan Li",
            "Kun-Lung Wu",
            "Syed Zawad",
            "Andrew Coleman",
            "Matthew White",
            "Mark Lewis",
            "Raju Pavuluri",
            "Yan Koyfman",
            "Boris Lublinsky",
            "Maximilien de Bayser",
            "Ibrahim Abdelaziz",
            "Kinjal Basu",
            "Mayank Agarwal"
        ],
        "Abstract": "Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.",
        "Publication date": "7 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.04324"
    },
    {
        "ID": 187,
        "Title": "A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama",
        "Authors": [
            "Vlad-Andrei Cursaru",
            "Laura Duits",
            "Joel Milligan",
            "Damla Ural",
            "Berta Rodriguez Sanchez",
            "Vincenzo Stoico",
            "Ivano Malavolta"
        ],
        "Abstract": "Context. Nowadays, 83% of software developers use Large Language Models (LLMs) to generate code. LLMs recently became essential to increase the productivity of software developers and decrease the time and cost of software development. Developers ranging from novices to experts use LLM tools not only to detect and patch bugs, but also to integrate generated code into their software. However, as of today there is no objective assessment of the energy efficiency of the source code generated by LLM tools. Released in August 2023, Code Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy efficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks implemented in C++, JavaScript, and Python. We ask Code Llama to generate the code of the benchmarks using different prompts and temperatures. Therefore, we execute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code Llama is heavily-dependent on the chosen programming language and the specific code problem at hand. Also, human implementations tend to be more energy efficient overall, with generated JavaScript code outperforming its human counterpart. Moreover, explicitly asking Code Llama to generate energy-efficient code results in an equal or worse energy efficiency, as well as using different temperatures seems not to affect the energy efficiency of generated code.\n  Conclusions. According to our results, code generated using Code Llama does not guarantee energy efficiency, even when prompted to do so. Therefore, software developers should evaluate the energy efficiency of generated code before integrating it into the software system under development.",
        "Publication date": "6 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.03616"
    },
    {
        "ID": 188,
        "Title": "Traffic Performance GPT (TP-GPT): Real-Time Data Informed Intelligent ChatBot for Transportation Surveillance and Management",
        "Authors": [
            "Bingzhang Wang",
            "Zhiyu Cai",
            "Muhammad Monjurul Karim",
            "Chenxi Liu",
            "Yinhai Wang"
        ],
        "Abstract": "The digitization of traffic sensing infrastructure has significantly accumulated an extensive traffic data warehouse, which presents unprecedented challenges for transportation analytics. The complexities associated with querying large-scale multi-table databases require specialized programming expertise and labor-intensive development. Additionally, traditional analysis methods have focused mainly on numerical data, often neglecting the semantic aspects that could enhance interpretability and understanding. Furthermore, real-time traffic data access is typically limited due to privacy concerns. To bridge this gap, the integration of Large Language Models (LLMs) into the domain of traffic management presents a transformative approach to addressing the complexities and challenges inherent in modern transportation systems. This paper proposes an intelligent online chatbot, TP-GPT, for efficient customized transportation surveillance and management empowered by a large real-time traffic database. The innovative framework leverages contextual and generative intelligence of language models to generate accurate SQL queries and natural language interpretations by employing transportation-specialized prompts, Chain-of-Thought prompting, few-shot learning, multi-agent collaboration strategy, and chat memory. Experimental study demonstrates that our approach outperforms state-of-the-art baselines such as GPT-4 and PaLM 2 on a challenging traffic-analysis benchmark TransQuery. TP-GPT would aid researchers and practitioners in real-time transportation surveillance and management in a privacy-preserving, equitable, and customizable manner.",
        "Publication date": "5 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.03076"
    },
    {
        "ID": 189,
        "Title": "CodeGRAG: Bridging the Gap between Natural Language and Programming Language via Graphical Retrieval Augmented Generation",
        "Authors": [
            "Kounianhua Du",
            "Jizheng Chen",
            "Renting Rui",
            "Huacan Chai",
            "Lingyue Fu",
            "Wei Xia",
            "Yasheng Wang",
            "Ruiming Tang",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "Abstract": "Utilizing large language models to generate codes has shown promising meaning in software development revolution. Despite the intelligence shown by the general large language models, their specificity in code generation can still be improved due to the syntactic gap and mismatched vocabulary existing among natural language and different programming languages. In this paper, we propose CodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance the performance of LLMs. CodeGRAG builds the graphical view of code blocks based on the control flow and data flow of them to fill the gap between programming languages and natural language, which can facilitate natural language based LLMs for better understanding of code syntax and serve as a bridge among different programming languages. To take the extracted structural knowledge into the foundation models, we propose 1) a hard meta-graph prompt template to transform the challenging graphical representation into informative knowledge for tuning-free models and 2) a soft prompting technique that injects the domain knowledge of programming languages into the model parameters via finetuning the models with the help of a pretrained GNN expert model. Various experiments and ablations are done on four datasets including both the C++ and python languages to validate the hard meta-graph prompt, the soft prompting technique, and the effectiveness of the objectives for pretrained GNN expert. CodeGRAG improves the code generation ability of LLMs and can even offer performance gain for cross-lingual code generation. The implementation is available at https://anonymous.4open.science/r/Code-5970/.",
        "Publication date": "2 October, 2024",
        "Link": "https://arxiv.org/pdf/2405.02355"
    },
    {
        "ID": 190,
        "Title": "Task Synthesis for Elementary Visual Programming in XLogoOnline Environment",
        "Authors": [
            "Chao Wen",
            "Ahana Ghosh",
            "Jacqueline Staub",
            "Adish Singla"
        ],
        "Abstract": "In recent years, the XLogoOnline programming platform has gained popularity among novice learners. It integrates the Logo programming language with visual programming, providing a visual interface for learning computing concepts. However, XLogoOnline offers only a limited set of tasks, which are inadequate for learners to master the computing concepts that require sufficient practice. To address this, we introduce XLogoSyn, a novel technique for synthesizing high-quality tasks for varying difficulty levels. Given a reference task, XLogoSyn can generate practice tasks at varying difficulty levels that cater to the varied needs and abilities of different learners. XLogoSyn achieves this by combining symbolic execution and constraint satisfaction techniques. Our expert study demonstrates the effectiveness of XLogoSyn. We have also deployed synthesized practice tasks into XLogoOnline, highlighting the educational benefits of these synthesized practice tasks.",
        "Publication date": "3 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.02173"
    },
    {
        "ID": 191,
        "Title": "Automated Control Logic Test Case Generation using Large Language Models",
        "Authors": [
            "Heiko Koziolek",
            "Virendra Ashiwal",
            "Soumyadip Bandyopadhyay",
            "Chandrika K R"
        ],
        "Abstract": "Testing PLC and DCS control logic in industrial automation is laborious and challenging since appropriate test cases are often complex and difficult to formulate. Researchers have previously proposed several automated test case generation approaches for PLC software applying symbolic execution and search-based techniques. Often requiring formal specifications and performing a mechanical analysis of programs, these approaches may uncover specific programming errors but sometimes suffer from state space explosion and cannot process rather informal specifications. We proposed a novel approach for the automatic generation of PLC test cases that queries a Large Language Model (LLM) to synthesize test cases for code provided in a prompt. Experiments with ten open-source function blocks from the OSCAT automation library showed that the approach is fast, easy to use, and can yield test cases with high statement coverage for low-to-medium complex programs. However, we also found that LLM-generated test cases suffer from erroneous assertions in many cases, which still require manual adaption.",
        "Publication date": "3 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.01874"
    },
    {
        "ID": 192,
        "Title": "WitheredLeaf: Finding Entity-Inconsistency Bugs with LLMs",
        "Authors": [
            "Hongbo Chen",
            "Yifan Zhang",
            "Xing Han",
            "Huanyao Rong",
            "Yuheng Zhang",
            "Tianhao Mao",
            "Hang Zhang",
            "XiaoFeng Wang",
            "Luyi Xing",
            "Xun Chen"
        ],
        "Abstract": "Originating from semantic bugs, Entity-Inconsistency Bugs (EIBs) involve misuse of syntactically valid yet incorrect program entities, such as variable identifiers and function names, which often have security implications. Unlike straightforward syntactic vulnerabilities, EIBs are subtle and can remain undetected for years. Traditional detection methods, such as static analysis and dynamic testing, often fall short due to the versatile and context-dependent nature of EIBs. However, with advancements in Large Language Models (LLMs) like GPT-4, we believe LLM-powered automatic EIB detection becomes increasingly feasible through these models' semantics understanding abilities. This research first undertakes a systematic measurement of LLMs' capabilities in detecting EIBs, revealing that GPT-4, while promising, shows limited recall and precision that hinder its practical application. The primary problem lies in the model's tendency to focus on irrelevant code snippets devoid of EIBs. To address this, we introduce a novel, cascaded EIB detection system named WitheredLeaf, which leverages smaller, code-specific language models to filter out most negative cases and mitigate the problem, thereby significantly enhancing the overall precision and recall. We evaluated WitheredLeaf on 154 Python and C GitHub repositories, each with over 1,000 stars, identifying 123 new flaws, 45% of which can be exploited to disrupt the program's normal operations. Out of 69 submitted fixes, 27 have been successfully merged.",
        "Publication date": "2 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.01668"
    },
    {
        "ID": 193,
        "Title": "Class-Level Code Generation from Natural Language Using Iterative, Tool-Enhanced Reasoning over Repository",
        "Authors": [
            "Ajinkya Deshpande",
            "Anmol Agarwal",
            "Shashank Shet",
            "Arun Iyer",
            "Aditya Kanade",
            "Ramakrishna Bairi",
            "Suresh Parthasarathy"
        ],
        "Abstract": "LLMs have demonstrated significant potential in code generation tasks, achieving promising results at the function or statement level across various benchmarks. However, the complexities associated with creating code artifacts like classes, particularly within the context of real-world software repositories, remain underexplored. Prior research treats class-level generation as an isolated task, neglecting the intricate dependencies & interactions that characterize real-world software environments. To address this gap, we introduce RepoClassBench, a comprehensive benchmark designed to rigorously evaluate LLMs in generating complex, class-level code within real-world repositories. RepoClassBench includes \"Natural Language to Class generation\" tasks across Java, Python & C# from a selection of repositories. We ensure that each class in our dataset not only has cross-file dependencies within the repository but also includes corresponding test cases to verify its functionality. We find that current models struggle with the realistic challenges posed by our benchmark, primarily due to their limited exposure to relevant repository contexts. To address this shortcoming, we introduce Retrieve-Repotools-Reflect (RRR), a novel approach that equips LLMs with static analysis tools to iteratively navigate & reason about repository-level context in an agent-based framework. Our experiments demonstrate that RRR significantly outperforms existing baselines on RepoClassBench, showcasing its effectiveness across programming languages & under various settings. Our findings emphasize the critical need for code-generation benchmarks to incorporate repo-level dependencies to more accurately reflect the complexities of software development. Our work shows the benefits of leveraging specialized tools to enhance LLMs' understanding of repository context. We plan to make our dataset & evaluation harness public.",
        "Publication date": "5 June, 2024",
        "Link": "https://arxiv.org/pdf/2405.01573"
    },
    {
        "ID": 194,
        "Title": "From Keyboard to Chatbot: An AI-powered Integration Platform with Large-Language Models for Teaching Computational Thinking for Young Children",
        "Authors": [
            "Changjae Lee",
            "Jinjun Xiong"
        ],
        "Abstract": "Teaching programming in early childhood (4-9) to enhance computational thinking has gained popularity in the recent movement of computer science for all. However, current practices ignore some fundamental issues resulting from young children's developmental readiness, such as the sustained capability to keyboarding, the decomposition of complex tasks to small tasks, the need for intuitive mapping from abstract programming to tangible outcomes, and the limited amount of screen time exposure. To address these issues in this paper, we present a novel methodology with an AI-powered integration platform to effectively teach computational thinking for young children. The system features a hybrid pedagogy that supports both the top-down and bottom-up approach for teaching computational thinking. Young children can describe their desired task in natural language, while the system can respond with an easy-to-understand program consisting of the right level of decomposed sub-tasks. A tangible robot can immediately execute the decomposed program and demonstrate the program's outcomes to young children. The system is equipped with an intelligent chatbot that can interact with young children through natural languages, and children can speak to the chatbot to complete all the needed programming tasks, while the chatbot orchestrates the execution of the program onto the robot. This would completely eliminates the need of keyboards for young children to program. By developing such a system, we aim to make the concept of computational thinking more accessible to young children, fostering a natural understanding of programming concepts without the need of explicit programming skills. Through the interactive experience provided by the robotic agent, our system seeks to engage children in an effective manner, contributing to the field of educational technology for early childhood computer science education.",
        "Publication date": "1 May, 2024",
        "Link": "https://arxiv.org/pdf/2405.00750"
    },
    {
        "ID": 195,
        "Title": "Exploring Multi-Lingual Bias of Large Code Models in Code Generation",
        "Authors": [
            "Chaozheng Wang",
            "Zongjie Li",
            "Cuiyun Gao",
            "Wenxuan Wang",
            "Ting Peng",
            "Hailiang Huang",
            "Yuetang Deng",
            "Shuai Wang",
            "Michael R. Lyu"
        ],
        "Abstract": "Code generation aims to synthesize code and fulfill functional requirements based on natural language (NL) specifications, which can greatly improve development efficiency. In the era of large language models (LLMs), large code models (LCMs) have been recently proposed to generate source code. LCMs can generate highly feasible solutions for programming problems described in natural language. Despite the effectiveness, we observe a noticeable multilingual bias in the generation performance of LCMs. Specifically, LCMs demonstrate proficiency in generating solutions when provided with instructions in English, yet may falter when faced with semantically equivalent instructions in other NLs such as Chinese. Moreover, the ability of LCMs to generate code exhibits variety across different programming languages (PLs), such as Python and C++. The observed phenomenon indicates the presence of multi-lingual bias within the generative capabilities of LCMs, which has remained unexplored.\n  In this paper, we aim to investigate the multi-lingual bias that exists in current LCMs. First, we initiate our investigation by constructing the first multi-lingual evaluation benchmark X-HumanEval-X, enabling us to systematically evaluate the extent of multi-lingual bias that exists in current LCMs. In our large-scale experiments on nine popular LCMs, we observe a pronounced multi-lingual bias of LCMs in code generation, including multi-NL and multi-PL bias. Specifically, when using Chinese instructions, the code generation capabilities of LCMs decrease by at least 13% in terms of the Pass@1 metric. Furthermore, LCMs perform variously across different programming languages, e.g., the performance gap between Python and C++ reaches as high as 20.9%. ...",
        "Publication date": "30 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.19368"
    },
    {
        "ID": 196,
        "Title": "VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners",
        "Authors": [
            "Aidan Z. H. Yang",
            "Yoshiki Takashima",
            "Brandon Paulsen",
            "Josiah Dodds",
            "Daniel Kroening"
        ],
        "Abstract": "Rust is a programming language that combines memory safety and low-level control, providing C-like performance while guaranteeing the absence of undefined behaviors by default. Rust's growing popularity has prompted research on safe and correct transpiling of existing code-bases to Rust. Existing work falls into two categories: rule-based and large language model (LLM)-based. While rule-based approaches can theoretically produce correct transpilations that maintain input-output equivalence to the original, they often yield unreadable Rust code that uses unsafe subsets of the Rust language. On the other hand, while LLM-based approaches typically produce more readable, maintainable, and safe code, they do not provide any guarantees about correctness. In this work, we present VERT, a tool that can produce readable Rust transpilations with formal guarantees of correctness. VERT's only requirement is that there is Web Assembly compiler for the source language, which is true for most major languages. VERT first uses the Web Assembly compiler to obtain an oracle Rust program. In parallel, VERT uses an LLM to generate a readable candidate Rust program. This candidate is verified against the oracle, and if verification fails, we regenerate a new candidate transpilation until verification succeeds. We evaluate VERT by transpiling a suite of 1,394 programs taken from competitive programming style benchmarks. Combining Anthropic's Claude-2 and VERT increases Rust transpilations passing property-based testing from 31% to 54% and bounded model-checking from 1% to 42% compared to using Claude alone. In addition, we evaluate VERT's ability to generate non-trivial safe Rust on programs taken from real-world C projects that make significant use of pointers. Our results provide insights into the limitations of LLMs to write safe Rust.",
        "Publication date": "25 May, 2024",
        "Link": "https://arxiv.org/pdf/2404.18852"
    },
    {
        "ID": 197,
        "Title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
        "Authors": [
            "Parshin Shojaee",
            "Kazem Meidani",
            "Shashank Gupta",
            "Amir Barati Farimani",
            "Chandan K Reddy"
        ],
        "Abstract": "Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely high-dimensional combinatorial and nonlinear hypothesis spaces. Traditional methods of equation discovery, commonly known as symbolic regression, largely focus on extracting equations from data alone, often neglecting the rich domain-specific prior knowledge that scientists typically depend on. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data in an efficient manner. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its physical understanding, which are then optimized against data to estimate skeleton parameters. We demonstrate LLM-SR's effectiveness across three diverse scientific domains, where it discovers physically accurate equations that provide significantly better fits to in-domain and out-of-domain data compared to the well-established symbolic regression baselines. Incorporating scientific prior knowledge also enables LLM-SR to search the equation space more efficiently than baselines. Code is available at: https://github.com/deep-symbolic-mathematics/LLM-SR",
        "Publication date": "2 June, 2024",
        "Link": "https://arxiv.org/pdf/2404.18400"
    },
    {
        "ID": 198,
        "Title": "Do Neutral Prompts Produce Insecure Code? FormAI-v2 Dataset: Labelling Vulnerabilities in Code Generated by Large Language Models",
        "Authors": [
            "Norbert Tihanyi",
            "Tamas Bisztray",
            "Mohamed Amine Ferrag",
            "Ridhi Jain",
            "Lucas C. Cordeiro"
        ],
        "Abstract": "This study provides a comparative analysis of state-of-the-art large language models (LLMs), analyzing how likely they generate vulnerabilities when writing simple C programs using a neutral zero-shot prompt. We address a significant gap in the literature concerning the security properties of code produced by these models without specific directives. N. Tihanyi et al. introduced the FormAI dataset at PROMISE '23, containing 112,000 GPT-3.5-generated C programs, with over 51.24% identified as vulnerable. We expand that work by introducing the FormAI-v2 dataset comprising 265,000 compilable C programs generated using various LLMs, including robust models such as Google's GEMINI-pro, OpenAI's GPT-4, and TII's 180 billion-parameter Falcon, to Meta's specialized 13 billion-parameter CodeLLama2 and various other compact models. Each program in the dataset is labelled based on the vulnerabilities detected in its source code through formal verification using the Efficient SMT-based Context-Bounded Model Checker (ESBMC). This technique eliminates false positives by delivering a counterexample and ensures the exclusion of false negatives by completing the verification process. Our study reveals that at least 63.47% of the generated programs are vulnerable. The differences between the models are minor, as they all display similar coding errors with slight variations. Our research highlights that while LLMs offer promising capabilities for code generation, deploying their output in a production environment requires risk assessment and validation.",
        "Publication date": "28 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.18353"
    },
    {
        "ID": 199,
        "Title": "Generating Situated Reflection Triggers about Alternative Solution Paths: A Case Study of Generative AI for Computer-Supported Collaborative Learning",
        "Authors": [
            "Atharva Naik",
            "Jessica Ruhan Yin",
            "Anusha Kamath",
            "Qianou Ma",
            "Sherry Tongshuang Wu",
            "Charles Murray",
            "Christopher Bogart",
            "Majd Sakr",
            "Carolyn P. Rose"
        ],
        "Abstract": "An advantage of Large Language Models (LLMs) is their contextualization capability - providing different responses based on student inputs like solution strategy or prior discussion, to potentially better engage students than standard feedback. We present a design and evaluation of a proof-of-concept LLM application to offer students dynamic and contextualized feedback. Specifically, we augment an Online Programming Exercise bot for a college-level Cloud Computing course with ChatGPT, which offers students contextualized reflection triggers during a collaborative query optimization task in database design. We demonstrate that LLMs can be used to generate highly situated reflection triggers that incorporate details of the collaborative discussion happening in context. We discuss in depth the exploration of the design space of the triggers and their correspondence with the learning objectives as well as the impact on student learning in a pilot study with 34 students.",
        "Publication date": "28 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.18262"
    },
    {
        "ID": 200,
        "Title": "Tenspiler: A Verified Lifting-Based Compiler for Tensor Operations",
        "Authors": [
            "Jie Qiu",
            "Colin Cai",
            "Sahil Bhatia",
            "Niranjan Hasabnis",
            "Sanjit A. Seshia",
            "Alvin Cheung"
        ],
        "Abstract": "Tensor processing infrastructures such as deep learning frameworks and specialized hardware accelerators have revolutionized how computationally intensive code from domains such as deep learning and image processing is executed and optimized. These infrastructures provide powerful and expressive abstractions while ensuring high performance. However, to utilize them, code must be written specifically using the APIs / ISAs of such software frameworks or hardware accelerators. Importantly, given the fast pace of innovation in these domains, code written today quickly becomes legacy as new frameworks and accelerators are developed, and migrating such legacy code manually is a considerable effort.\n  To enable developers in leveraging such DSLs while preserving their current programming paradigm, we introduce Tenspiler, a verified lifting-based compiler that uses program synthesis to translate sequential programs written in general-purpose programming languages (e.g., C++ or Python code) into tensor operations. Central to Tenspiler is our carefully crafted yet simple intermediate language, named TensIR, that expresses tensor operations. TensIR enables efficient lifting, verification, and code generation.\n  Currently, Tenspiler already supports \\textbf{six} DSLs, spanning a broad spectrum of software and hardware environments. Furthermore, we show that new backends can be easily supported by Tenspiler by adding simple pattern-matching rules for TensIR. Using 10 real-world code benchmark suites, our experimental evaluation shows that by translating code to be executed on \\textbf{6} different software frameworks and hardware devices, Tenspiler offers on average 105$\\times$ kernel and 9.65$\\times$ end-to-end execution time improvement over the fully-optimized sequential implementation of the same benchmarks.",
        "Publication date": "28 July, 2024",
        "Link": "https://arxiv.org/pdf/2404.18249"
    },
    {
        "ID": 201,
        "Title": "Automatic Build Repair for Test Cases using Incompatible Java Versions",
        "Authors": [
            "Ching Hang Mak",
            "Shing-Chi Cheung"
        ],
        "Abstract": "Context: Bug bisection is a common technique used to identify a revision that introduces a bug or indirectly fixes a bug, and often involves executing multiple revisions of a project to determine whether the bug is present within the revision. However, many legacy revisions often cannot be successfully compiled due to changes in the programming language or tools used in the compilation process, adding complexity and preventing automation in the bisection process.\n  Objective: In this paper, we introduce an approach to repair test cases of Java projects by performing dependency minimization. Our approach aims to remove classes and methods that are not required for the execution of one or more test cases. Unlike existing state-of-the-art techniques, our approach performs minimization at source-level, which allows compile-time errors to be fixed.\n  Method: A standalone Java tool implementing our technique was developed, and we evaluated our technique using subjects from Defects4J retargeted against Java 8 and 17.\n  Results: Our evaluation showed that a majority of subjects can be repaired solely by performing minimization, including replicating the test results of the original version. Furthermore, our technique is also shown to achieve accurate minimized results, while only adding a small overhead to the bisection process.\n  Conclusion: Our proposed technique is shown to be effective for repairing build failures with minimal overhead, making it suitable for use in automated bug bisection. Our tool can also be adapted for use cases such as bug corpus creation and refactoring.",
        "Publication date": "3 May, 2024",
        "Link": "https://arxiv.org/pdf/2404.17818"
    },
    {
        "ID": 202,
        "Title": "Probabilistic Interval Analysis of Unreliable Programs",
        "Authors": [
            "Dibyendu Das",
            "Soumyajit Dey"
        ],
        "Abstract": "Advancement of chip technology will make future computer chips faster. Power consumption of such chips shall also decrease. But this speed gain shall not come free of cost, there is going to be a trade-off between speed and efficiency, i.e accuracy of the computation. In order to achieve this extra speed we will simply have to let our computers make more mistakes in computations. Consequently, systems built with these type of chips will possess an innate unreliability lying within. Programs written for these systems will also have to incorporate this unreliability. Researchers have already started developing programming frameworks for unreliable architectures as such.\n  In the present work, we use a restricted version of C-type languages to model the programs written for unreliable architectures. We propose a technique for statically analyzing codes written for these kind of architectures. Our technique, which primarily focuses on Interval/Range Analysis of this type of programs, uses the well established theory of abstract interpretation. While discussing unreliability of hardware, there comes scope of failure of the hardware components implicitly. There are two types of failure models, namely: 1) permanent failure model, where the hardware stops execution on failure and 2) transient failure model, where on failure, the hardware continues subsequent operations with wrong operand values. In this paper, we've only taken transient failure model into consideration. The goal of this analysis is to predict the probability with which a program variable assumes values from a given range at a given program point.",
        "Publication date": "25 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.16997"
    },
    {
        "ID": 203,
        "Title": "Finch: Sparse and Structured Array Programming with Control Flow",
        "Authors": [
            "Willow Ahrens",
            "Teodoro Fields Collin",
            "Radha Patel",
            "Kyle Deeds",
            "Changwan Hong",
            "Saman Amarasinghe"
        ],
        "Abstract": "From FORTRAN to NumPy, arrays have revolutionized how we express computation. However, arrays in these, and almost all prominent systems, can only handle dense rectilinear integer grids. Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Support for structured data is fragmented and incomplete. Existing frameworks limit the array structures and program control flow they support to better simplify the problem.\n  In this work, we propose a new programming language, Finch, which supports both flexible control flow and diverse data structures. Finch facilitates a programming model which resolves the challenges of computing over structured arrays by combining control flow and data structures into a common representation where they can be co-optimized. Finch automatically specializes control flow to data so that performance engineers can focus on experimenting with many algorithms. Finch supports a familiar programming language of loops, statements, ifs, breaks, etc., over a wide variety of array structures, such as sparsity, run-length-encoding, symmetry, triangles, padding, or blocks. Finch reliably utilizes the key properties of structure, such as structural zeros, repeated values, or clustered non-zeros. We show that this leads to dramatic speedups in operations such as SpMV and SpGEMM, image processing, graph analytics, and a high-level tensor operator fusion interface.",
        "Publication date": "25 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.16730"
    },
    {
        "ID": 204,
        "Title": "VulEval: Towards Repository-Level Evaluation of Software Vulnerability Detection",
        "Authors": [
            "Xin-Cheng Wen",
            "Xinchen Wang",
            "Yujia Chen",
            "Ruida Hu",
            "David Lo",
            "Cuiyun Gao"
        ],
        "Abstract": "Deep Learning (DL)-based methods have proven to be effective for software vulnerability detection, with a potential for substantial productivity enhancements for detecting vulnerabilities. Current methods mainly focus on detecting single functions (i.e., intra-procedural vulnerabilities), ignoring the more complex inter-procedural vulnerability detection scenarios in practice. For example, developers routinely engage with program analysis to detect vulnerabilities that span multiple functions within repositories. In addition, the widely-used benchmark datasets generally contain only intra-procedural vulnerabilities, leaving the assessment of inter-procedural vulnerability detection capabilities unexplored.\n  To mitigate the issues, we propose a repository-level evaluation system, named \\textbf{VulEval}, aiming at evaluating the detection performance of inter- and intra-procedural vulnerabilities simultaneously. Specifically, VulEval consists of three interconnected evaluation tasks: \\textbf{(1) Function-Level Vulnerability Detection}, aiming at detecting intra-procedural vulnerability given a code snippet; \\textbf{(2) Vulnerability-Related Dependency Prediction}, aiming at retrieving the most relevant dependencies from call graphs for providing developers with explanations about the vulnerabilities; and \\textbf{(3) Repository-Level Vulnerability Detection}, aiming at detecting inter-procedural vulnerabilities by combining with the dependencies identified in the second task. VulEval also consists of a large-scale dataset, with a total of 4,196 CVE entries, 232,239 functions, and corresponding 4,699 repository-level source code in C/C++ programming languages. Our analysis highlights the current progress and future directions for software vulnerability detection.",
        "Publication date": "23 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.15596"
    },
    {
        "ID": 205,
        "Title": "NExT: Teaching Large Language Models to Reason about Code Execution",
        "Authors": [
            "Ansong Ni",
            "Miltiadis Allamanis",
            "Arman Cohan",
            "Yinlin Deng",
            "Kensen Shi",
            "Charles Sutton",
            "Pengcheng Yin"
        ],
        "Abstract": "A fundamental skill among human developers is the ability to understand and reason about program execution. As an example, a programmer can mentally simulate code execution in natural language to debug and repair code (aka. rubber duck debugging). However, large language models (LLMs) of code are typically trained on the surface textual form of programs, thus may lack a semantic understanding of how programs execute at run-time. To address this issue, we propose NExT, a method to teach LLMs to inspect the execution traces of programs (variable states of executed lines) and reason about their run-time behavior through chain-of-thought (CoT) rationales. Specifically, NExT uses self-training to bootstrap a synthetic training set of execution-aware rationales that lead to correct task solutions (e.g., fixed programs) without laborious manual annotation. Experiments on program repair tasks based on MBPP and HumanEval demonstrate that NExT improves the fix rate of a PaLM 2 model, by 26.1% and 14.3% absolute, respectively, with significantly improved rationale quality as verified by automated metrics and human raters. Our model can also generalize to scenarios where program traces are absent at test-time.",
        "Publication date": "22 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.14662"
    },
    {
        "ID": 206,
        "Title": "SIGY: Breaking Intel SGX Enclaves with Malicious Exceptions & Signals",
        "Authors": [
            "Supraja Sridhara",
            "Andrin Bertschi",
            "Benedict SchlÃ¼ter",
            "Shweta Shinde"
        ],
        "Abstract": "User programs recover from hardware exceptions and respond to signals by executing custom handlers that they register specifically for such events. We present SIGY attack, which abuses this programming model on Intel SGX to break the confidentiality and integrity guarantees of enclaves. SIGY uses the untrusted OS to deliver fake hardware events and injects fake signals in an enclave at any point. Such unintended execution of benign program-defined handlers in an enclave corrupts its state and violates execution integrity. 7 runtimes and library OSes (OpenEnclave, Gramine, Scone, Asylo, Teaclave, Occlum, EnclaveOS) are vulnerable to SIGY. 8 languages supported in Intel SGX have programming constructs that are vulnerable to SIGY. We use SIGY to demonstrate 4 proof of concept exploits on webservers (Nginx, Node.js) to leak secrets and data analytics workloads in different languages (C and Java) to break execution integrity.",
        "Publication date": "22 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.13998"
    },
    {
        "ID": 207,
        "Title": "Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs",
        "Authors": [
            "Boyang Yang",
            "Haoye Tian",
            "Jiadong Ren",
            "Hongyu Zhang",
            "Jacques Klein",
            "TegawendÃ© F. BissyandÃ©",
            "Claire Le Goues",
            "Shunfu Jin"
        ],
        "Abstract": "Large language models (LLMs) have demonstrated remarkable capabilities on a broad spectrum of downstream tasks. Within the realm of software engineering, specialized tasks on code, such as program repair, present unique challenges, necessitating fine-tuning to unlock state-of-the-art performance. Fine-tuning approaches proposed in the literature for LLMs on program repair tasks are however generally overlooking the need to reason about the logic behind code changes, beyond syntactic patterns in the data. High-performing fine-tuning experiments also usually come at very high computational costs. With MORepair, we propose a novel perspective on the learning focus of LLM fine-tuning for program repair: we not only adapt the LLM parameters to the syntactic nuances of the task of code transformation (objective 1), but we also specifically fine-tune the LLM with respect to the logical reason behind the code change in the training data (objective 2). Such a multi-objective fine-tuning will instruct LLMs to generate high-quality patches.\n  We apply MORepair to fine-tune four open-source LLMs with different sizes and architectures. Experimental results on C++ and Java repair benchmarks show that the implemented fine-tuning effectively boosts LLM repair performance by 7.6% to 10% in Top-10 repair suggestions. We further show that our fine-tuning strategy yields superior performance compared to the incumbent state-of-the-art in fine-tuned models for program repair, Fine-tune-CoT and RepairLLaMA.",
        "Publication date": "22 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.12636"
    },
    {
        "ID": 208,
        "Title": "Let's Ask AI About Their Programs: Exploring ChatGPT's Answers To Program Comprehension Questions",
        "Authors": [
            "Teemu Lehtinen",
            "Charles Koutcheme",
            "Arto Hellas"
        ],
        "Abstract": "Recent research has explored the creation of questions from code submitted by students. These Questions about Learners' Code (QLCs) are created through program analysis, exploring execution paths, and then creating code comprehension questions from these paths and the broader code structure. Responding to the questions requires reading and tracing the code, which is known to support students' learning. At the same time, computing education researchers have witnessed the emergence of Large Language Models (LLMs) that have taken the community by storm. Researchers have demonstrated the applicability of these models especially in the introductory programming context, outlining their performance in solving introductory programming problems and their utility in creating new learning resources. In this work, we explore the capability of the state-of-the-art LLMs (GPT-3.5 and GPT-4) in answering QLCs that are generated from code that the LLMs have created. Our results show that although the state-of-the-art LLMs can create programs and trace program execution when prompted, they easily succumb to similar errors that have previously been recorded for novice programmers. These results demonstrate the fallibility of these models and perhaps dampen the expectations fueled by the recent LLM hype. At the same time, we also highlight future research possibilities such as using LLMs to mimic students as their behavior can indeed be similar for some specific tasks.",
        "Publication date": "17 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.11734"
    },
    {
        "ID": 209,
        "Title": "An Empirical Evaluation of Pre-trained Large Language Models for Repairing Declarative Formal Specifications",
        "Authors": [
            "Mohannad Alhanahnah",
            "Md Rashedul Hasan",
            "Hamid Bagheri"
        ],
        "Abstract": "Automatic Program Repair (APR) has garnered significant attention as a practical research domain focused on automatically fixing bugs in programs. While existing APR techniques primarily target imperative programming languages like C and Java, there is a growing need for effective solutions applicable to declarative software specification languages. This paper presents a systematic investigation into the capacity of Large Language Models (LLMs) for repairing declarative specifications in Alloy, a declarative formal language used for software specification. We propose a novel repair pipeline that integrates a dual-agent LLM framework, comprising a Repair Agent and a Prompt Agent. Through extensive empirical evaluation, we compare the effectiveness of LLM-based repair with state-of-the-art Alloy APR techniques on a comprehensive set of benchmarks. Our study reveals that LLMs, particularly GPT-4 variants, outperform existing techniques in terms of repair efficacy, albeit with a marginal increase in runtime and token usage. This research contributes to advancing the field of automatic repair for declarative specifications and highlights the promising potential of LLMs in this domain.",
        "Publication date": "16 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.11050"
    },
    {
        "ID": 210,
        "Title": "Automating Personalized Parsons Problems with Customized Contexts and Concepts",
        "Authors": [
            "Andre del Carpio Gutierrez",
            "Paul Denny",
            "Andrew Luxton-Reilly"
        ],
        "Abstract": "Parsons problems provide useful scaffolding for introductory programming students learning to write code. However, generating large numbers of high-quality Parsons problems that appeal to the diverse range of interests in a typical introductory course is a significant challenge for educators. Large language models (LLMs) may offer a solution, by allowing students to produce on-demand Parsons problems for topics covering the breadth of the introductory programming curriculum, and targeting thematic contexts that align with their personal interests. In this paper, we introduce PuzzleMakerPy, an educational tool that uses an LLM to generate unlimited contextualized drag-and-drop programming exercises in the form of Parsons Problems, which introductory programmers can use as a supplemental learning resource. We evaluated PuzzleMakerPy by deploying it in a large introductory programming course, and found that the ability to personalize the contextual framing used in problem descriptions was highly engaging for students, and being able to customize the programming topics was reported as being useful for their learning.",
        "Publication date": "16 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.10990"
    },
    {
        "ID": 211,
        "Title": "Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance",
        "Authors": [
            "Yewei Song",
            "Cedric Lothritz",
            "Daniel Tang",
            "TegawendÃ© F. BissyandÃ©",
            "Jacques Klein"
        ],
        "Abstract": "This paper revisits recent code similarity evaluation metrics, particularly focusing on the application of Abstract Syntax Tree (AST) editing distance in diverse programming languages. In particular, we explore the usefulness of these metrics and compare them to traditional sequence similarity metrics. Our experiments showcase the effectiveness of AST editing distance in capturing intricate code structures, revealing a high correlation with established metrics. Furthermore, we explore the strengths and weaknesses of AST editing distance and prompt-based GPT similarity scores in comparison to BLEU score, execution match, and Jaccard Similarity. We propose, optimize, and publish an adaptable metric that demonstrates effectiveness across all tested languages, representing an enhanced version of Tree Similarity of Edit Distance (TSED).",
        "Publication date": "3 June, 2024",
        "Link": "https://arxiv.org/pdf/2404.08817"
    },
    {
        "ID": 212,
        "Title": "Towards Robustness of Text-to-Visualization Translation against Lexical and Phrasal Variability",
        "Authors": [
            "Jinwei Lu",
            "Yuanfeng Song",
            "Haodi Zhang",
            "Chen Zhang",
            "Raymond Chi-Wing Wong"
        ],
        "Abstract": "Text-to-Vis is an emerging task in the natural language processing (NLP) area that aims to automatically generate data visualizations from natural language questions (NLQs). Despite their progress, existing text-to-vis models often heavily rely on lexical matching between words in the questions and tokens in data schemas. This overreliance on lexical matching may lead to a diminished level of model robustness against input variations. In this study, we thoroughly examine the robustness of current text-to-vis models, an area that has not previously been explored. In particular, we construct the first robustness dataset nvBench-Rob, which contains diverse lexical and phrasal variations based on the original text-to-vis benchmark nvBench. Then, we found that the performance of existing text-to-vis models on this new dataset dramatically drops, implying that these methods exhibit inadequate robustness overall. Finally, we propose a novel framework based on Retrieval-Augmented Generation (RAG) technique, named GRED, specifically designed to address input perturbations in these two variants. The framework consists of three parts: NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and Annotation-based Debugger, which are used to tackle the challenges posed by natural language variants, programming style differences and data schema variants, respectively. Extensive experimental evaluations show that, compared to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs better in terms of model robustness, with a 32% increase in accuracy on the proposed nvBench-Rob dataset.",
        "Publication date": "11 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.07135"
    },
    {
        "ID": 213,
        "Title": "On Evaluating the Efficiency of Source Code Generated by LLMs",
        "Authors": [
            "Changan Niu",
            "Ting Zhang",
            "Chuanyi Li",
            "Bin Luo",
            "Vincent Ng"
        ],
        "Abstract": "Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.",
        "Publication date": "9 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.06041"
    },
    {
        "ID": 214,
        "Title": "KaMPIng: Flexible and (Near) Zero-Overhead C++ Bindings for MPI",
        "Authors": [
            "Tim Niklas Uhl",
            "Matthias Schimek",
            "Lukas HÃ¼bner",
            "Demian Hespe",
            "Florian Kurpicz",
            "Christoph Stelz",
            "Peter Sanders"
        ],
        "Abstract": "The Message-Passing Interface (MPI) and C++ form the backbone of high-performance computing, but MPI only provides C and Fortran bindings. While this offers great language interoperability, high-level programming languages like C++ make software development quicker and less error-prone.\n  We propose novel C++ language bindings that cover all abstraction levels from low-level MPI calls to convenient STL-style bindings, where most parameters are inferred from a small subset of parameters, by bringing named parameters to C++. This enables rapid prototyping and fine-tuning runtime behavior and memory management. A flexible type system and additional safety guarantees help to prevent programming errors.\n  By exploiting C++'s template metaprogramming capabilities, this has (near) zero overhead, as only required code paths are generated at compile time.\n  We demonstrate that our library is a strong foundation for a future distributed standard library using multiple application benchmarks, ranging from text-book sorting algorithms to phylogenetic interference.",
        "Publication date": "24 September, 2024",
        "Link": "https://arxiv.org/pdf/2404.05610"
    },
    {
        "ID": 215,
        "Title": "A Coq Library of Sets for Teaching Denotational Semantics",
        "Authors": [
            "Qinxiang Cao",
            "Xiwei Wu",
            "Yalun Liang"
        ],
        "Abstract": "Sets and relations are very useful concepts for defining denotational semantics. In the Coq proof assistant, curried functions to Prop are used to represent sets and relations, e.g. A -> Prop, A -> B -> Prop, A -> B -> C -> Prop, etc. Further, the membership relation can be encoded by function applications, e.g. X a represents a in X if X: A -> Prop. This is very convenient for developing formal definitions and proofs for professional users, but it makes propositions more difficult to read for non-professional users, e.g. students of a program semantics course. We develop a small Coq library of sets and relations so that standard math notations can be used when teaching denotational semantics of simple imperative languages. This library is developed using Coq's type class system. It brings about zero proof-term overhead comparing with the existing formalization of sets.",
        "Publication date": "8 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.05459"
    },
    {
        "ID": 216,
        "Title": "WebPie: A Tiny Slice of Dependent Typing",
        "Authors": [
            "Christophe Scholliers"
        ],
        "Abstract": "Dependently typed programming languages have become increasingly relevant in recent years. They have been adopted in industrial strength programming languages and have been extremely successful as the basis for theorem provers. There are however, very few entry level introductions to the theory of language constructs for dependently typed languages, and even less sources on didactical implementations. In this paper, we present a small dependently typed programming language called WebPie. The main features of the language are inductive types, recursion and case matching. While none of these features are new, we believe this article can provide a step forward towards the understanding and systematic construction of dependently typed languages for researchers new to dependent types.",
        "Publication date": "8 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.05457"
    },
    {
        "ID": 217,
        "Title": "Fork is All You Need in Heterogeneous Systems",
        "Authors": [
            "Zixuan Wang",
            "Jishen Zhao"
        ],
        "Abstract": "We present a unified programming model for heterogeneous computing systems. Such systems integrate multiple computing accelerators and memory units to deliver higher performance than CPU-centric systems. Although heterogeneous systems have been adopted by modern workloads such as machine learning, programming remains a critical limiting factor. Conventional heterogeneous programming techniques either impose heavy modifications to the code base or require rewriting the program in a different language. Such programming complexity stems from the lack of a unified abstraction layer for computing and data exchange, which forces each programming model to define its abstractions. However, with the emerging cache-coherent interconnections such as Compute Express Link, we see an opportunity to standardize such architecture heterogeneity and provide a unified programming model. We present CodeFlow, a language runtime system for heterogeneous computing. CodeFlow abstracts architecture computation in programming language runtime and utilizes CXL as a unified data exchange protocol. Workloads written in high-level languages such as C++ and Rust can be compiled to CodeFlow, which schedules different parts of the workload to suitable accelerators without requiring the developer to implement code or call APIs for specific accelerators. CodeFlow reduces programmers' effort in utilizing heterogeneous systems and improves workload performance.",
        "Publication date": "16 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.05085"
    },
    {
        "ID": 218,
        "Title": "KATch: A Fast Symbolic Verifier for NetKAT",
        "Authors": [
            "Mark Moeller",
            "Jules Jacobs",
            "Olivier Savary Belanger",
            "David Darais",
            "Cole Schlesinger",
            "Steffen Smolka",
            "Nate Foster",
            "Alexandra Silva"
        ],
        "Abstract": "We develop new data structures and algorithms for checking verification queries in NetKAT, a domain-specific language for specifying the behavior of network data planes. Our results extend the techniques obtained in prior work on symbolic automata and provide a framework for building efficient and scalable verification tools. We present KATch, an implementation of these ideas in Scala, featuring an extended set of NetKAT operators that are useful for expressing network-wide specifications, and a verification engine that constructs a bisimulation or generates a counter-example showing that none exists. We evaluate the performance of our implementation on real-world and synthetic benchmarks, verifying properties such as reachability and slice isolation, typically returning a result in well under a second, which is orders of magnitude faster than previous approaches. Our advancements underscore NetKAT's potential as a practical, declarative language for network specification and verification.",
        "Publication date": "21 June, 2024",
        "Link": "https://arxiv.org/pdf/2404.04760"
    },
    {
        "ID": 219,
        "Title": "An Investigation into Misuse of Java Security APIs by Large Language Models",
        "Authors": [
            "Zahra Mousavi",
            "Chadni Islam",
            "Kristen Moore",
            "Alsharif Abuadbba",
            "Muhammad Ali Babar"
        ],
        "Abstract": "The increasing trend of using Large Language Models (LLMs) for code generation raises the question of their capability to generate trustworthy code. While many researchers are exploring the utility of code generation for uncovering software vulnerabilities, one crucial but often overlooked aspect is the security Application Programming Interfaces (APIs). APIs play an integral role in upholding software security, yet effectively integrating security APIs presents substantial challenges. This leads to inadvertent misuse by developers, thereby exposing software to vulnerabilities. To overcome these challenges, developers may seek assistance from LLMs. In this paper, we systematically assess ChatGPT's trustworthiness in code generation for security API use cases in Java. To conduct a thorough evaluation, we compile an extensive collection of 48 programming tasks for 5 widely used security APIs. We employ both automated and manual approaches to effectively detect security API misuse in the code generated by ChatGPT for these tasks. Our findings are concerning: around 70% of the code instances across 30 attempts per task contain security API misuse, with 20 distinct misuse types identified. Moreover, for roughly half of the tasks, this rate reaches 100%, indicating that there is a long way to go before developers can rely on ChatGPT to securely implement security API code.",
        "Publication date": "4 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.03823"
    },
    {
        "ID": 220,
        "Title": "AI-Tutoring in Software Engineering Education",
        "Authors": [
            "Eduard Frankford",
            "Clemens Sauerwein",
            "Patrick Bassner",
            "Stephan Krusche",
            "Ruth Breu"
        ],
        "Abstract": "With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences. In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.",
        "Publication date": "5 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.02548"
    },
    {
        "ID": 221,
        "Title": "CSEPrompts: A Benchmark of Introductory Computer Science Prompts",
        "Authors": [
            "Nishat Raihan",
            "Dhiman Goswami",
            "Sadiya Sayara Chowdhury Puspo",
            "Christian Newman",
            "Tharindu Ranasinghe",
            "Marcos Zampieri"
        ],
        "Abstract": "Recent advances in AI, machine learning, and NLP have led to the development of a new generation of Large Language Models (LLMs) that are trained on massive amounts of data and often have trillions of parameters. Commercial applications (e.g., ChatGPT) have made this technology available to the general public, thus making it possible to use LLMs to produce high-quality texts for academic and professional purposes. Schools and universities are aware of the increasing use of AI-generated content by students and they have been researching the impact of this new technology and its potential misuse. Educational programs in Computer Science (CS) and related fields are particularly affected because LLMs are also capable of generating programming code in various programming languages. To help understand the potential impact of publicly available LLMs in CS education, we introduce CSEPrompts, a framework with hundreds of programming exercise prompts and multiple-choice questions retrieved from introductory CS and programming courses. We also provide experimental results on CSEPrompts to evaluate the performance of several LLMs with respect to generating Python code and answering basic computer science and programming questions.",
        "Publication date": "4 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.02540"
    },
    {
        "ID": 222,
        "Title": "Stable Code Technical Report",
        "Authors": [
            "Nikhil Pinnaparaju",
            "Reshinth Adithyan",
            "Duy Phung",
            "Jonathan Tow",
            "James Baicoianu",
            "Ashish Datta",
            "Maksym Zhuravinskyi",
            "Dakota Mahan",
            "Marco Bellagente",
            "Carlos Riquelme",
            "Nathan Cooper"
        ],
        "Abstract": "We introduce Stable Code, the first in our new-generation of code language models series, which serves as a general-purpose base code language model targeting code completion, reasoning, math, and other software engineering-based tasks. Additionally, we introduce an instruction variant named Stable Code Instruct that allows conversing with the model in a natural chat interface for performing question-answering and instruction-based tasks. In this technical report, we detail the data and training procedure leading to both models. Their weights are available via Hugging Face for anyone to download and use at https://huggingface.co/stabilityai/stable-code-3b and https://huggingface.co/stabilityai/stable-code-instruct-3b. This report contains thorough evaluations of the models, including multilingual programming benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of its release, Stable Code is the state-of-the-art open model under 3B parameters and even performs comparably to larger models of sizes 7 billion and 15 billion parameters on the popular Multi-PL benchmark. Stable Code Instruct also exhibits state-of-the-art performance on the MT-Bench coding tasks and on Multi-PL completion compared to other instruction tuned models. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.",
        "Publication date": "1 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.01226"
    },
    {
        "ID": 223,
        "Title": "Enabling Memory Safety of C Programs using LLMs",
        "Authors": [
            "Nausheen Mohammed",
            "Akash Lal",
            "Aseem Rastogi",
            "Subhajit Roy",
            "Rahul Sharma"
        ],
        "Abstract": "Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique.\n  The task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use Large Language Models (LLMs) towards addressing both these concerns. We show how to harness LLM capabilities to do complex code reasoning as well as rewriting of large codebases. We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM. We implement our ideas in a tool called MSA that targets the CheckedC dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla LLM baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.",
        "Publication date": "1 April, 2024",
        "Link": "https://arxiv.org/pdf/2404.01096"
    },
    {
        "ID": 224,
        "Title": "DHNet: A Distributed Network Architecture for Smart Home",
        "Authors": [
            "Chaoqi Zhou",
            "Jingpu Duan",
            "YuPeng Xiao",
            "Qing Li",
            "Dingding Chen",
            "Ruobin Zheng",
            "Shaoteng Liu"
        ],
        "Abstract": "With the increasing popularity of smart homes, more and more devices need to connect to home networks. Traditional home networks mainly rely on centralized networking, where an excessive number of devices in the centralized topology can increase the pressure on the central router, potentially leading to decreased network performance metrics such as communication latency. To address the latency performance issues brought about by centralized networks, this paper proposes a new network system called DHNet, and designs an algorithm for clustering networking and communication based on vector routing. Communication within clusters in a simulated virtual environment achieves a latency of approximately 0.7 milliseconds. Furthermore, by directly using the first non-\"lo\" network card address of a device as the protocol's network layer address, the protocol avoids the several tens of milliseconds of access latency caused by DHCP. The integration of service discovery functionality into the network layer protocol is achieved through a combination of \"server-initiated service push\" and \"client request + server reply\" methods. Compared to traditional application-layer DNS passive service discovery, the average latency is reduced by over 50%. The PVH protocol is implemented in the user space using the Go programming language, with implementation details drawn from Google's gVisor project. The code has been ported from x86\\_64 Linux computers to devices such as OpenWrt routers and Android smartphones. The PVH protocol can communicate through \"tunnels\" to provide IP compatibility, allowing existing applications based on TCP/IP to communicate using the PVH protocol without requiring modifications to their code.",
        "Publication date": "28 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.19931"
    },
    {
        "ID": 225,
        "Title": "Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",
        "Authors": [
            "Chunqiu Steven Xia",
            "Yinlin Deng",
            "Lingming Zhang"
        ],
        "Abstract": "LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows that compared to the high performance obtained on standard benchmarks like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval. Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential overfitting of existing benchmarks. Furthermore, we showcase various insights, including the brittleness of instruction-following models when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition. EvoEval not only provides comprehensive benchmarks, but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of LLMs for code. We have open-sourced our benchmarks, tools, and complete LLM generations at https://github.com/evo-eval/evoeval",
        "Publication date": "27 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.19114"
    },
    {
        "ID": 226,
        "Title": "Resource Allocation in Large Language Model Integrated 6G Vehicular Networks",
        "Authors": [
            "Chang Liu",
            "Jun Zhao"
        ],
        "Abstract": "In the upcoming 6G era, vehicular networks are shifting from simple Vehicle-to-Vehicle (V2V) communication to the more complex Vehicle-to-Everything (V2X) connectivity. At the forefront of this shift is the incorporation of Large Language Models (LLMs) into vehicles. Known for their sophisticated natural language processing abilities, LLMs change how users interact with their vehicles. This integration facilitates voice-driven commands and interactions, departing from the conventional manual control systems. However, integrating LLMs into vehicular systems presents notable challenges. The substantial computational demands and energy requirements of LLMs pose significant challenges, especially in the constrained environment of a vehicle. Additionally, the time-sensitive nature of tasks in vehicular networks adds another layer of complexity. In this paper, we consider an edge computing system where vehicles process the initial layers of LLM computations locally, and offload the remaining LLM computation tasks to the Roadside Units (RSUs), envisioning a vehicular ecosystem where LLM computations seamlessly interact with the ultra-low latency and high-bandwidth capabilities of 6G networks. To balance the trade-off between completion time and energy consumption, we formulate a multi-objective optimization problem to minimize the total cost of the vehicles and RSUs. The problem is then decomposed into two sub-problems, which are solved by sequential quadratic programming (SQP) method and fractional programming technique. The simulation results clearly indicate that the algorithm we have proposed is highly effective in reducing both the completion time and energy consumption of the system.",
        "Publication date": "27 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.19016"
    },
    {
        "ID": 227,
        "Title": "ConstraintFlow: A DSL for Specification and Verification of Neural Network Analyses",
        "Authors": [
            "Avaljot Singh",
            "Yasmin Sarita",
            "Charith Mendis",
            "Gagandeep Singh"
        ],
        "Abstract": "The uninterpretability of DNNs hinders their deployment to safety-critical applications. Recent works have shown that Abstract-Interpretation-based formal certification techniques provide promising avenues for building trust in DNNs to some extent. The intricate mathematical background of Abstract Interpretation poses two challenges: (i) easily designing the algorithms that capture the intricate DNN behavior by balancing cost vs. precision tradeoff, and (ii) maintaining the over-approximation-based soundness of these certifiers.\n  General-purpose programming languages like C++ provide extensive functionality, however, verifying the soundness of the algorithms written in them can be impractical. The most commonly used DNN certification libraries like auto_LiRPA and ERAN prove the correctness of their analyses. However, they consist of only a few hard-coded abstract domains and abstract transformers (or transfer functions) and do not allow the user to define new analyses. Further, these libraries can handle only specific DNN architectures.\n  To address these issues, we develop a declarative DSL -- ConstraintFlow -- that can be used to specify Abstract Interpretation-based DNN certifiers. In ConstraintFlow, programmers can easily define various existing and new abstract domains and transformers, all within just a few 10s of Lines of Code as opposed to 1000s of LOCs of existing libraries. We also provide lightweight automatic verification, which can be used to ensure the over-approximation-based soundness of the certifier code written in ConstraintFlow for arbitrary (but bounded) DNN architectures. Using this automated verification procedure, for the first time, we can verify the soundness of state-of-the-art DNN certifiers for arbitrary DNN architectures, all within a few minutes. We prove the soundness of our verification procedure and the completeness of a subset of ConstraintFlow.",
        "Publication date": "27 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.18729"
    },
    {
        "ID": 228,
        "Title": "Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation",
        "Authors": [
            "Marcos Macedo",
            "Yuan Tian",
            "Filipe R. Cogo",
            "Bram Adams"
        ],
        "Abstract": "Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to underestimation of their actual performance. This is particularly evident when evaluating them with execution-based metrics such as Computational Accuracy (CA). Our results demonstrate that a strategic combination of prompt engineering and regular expression can effectively extract the source code from the model generation output. In particular, our method can help eleven selected models achieve an average Code Extraction Success Rate (CSR) of 92.73%. Our findings shed light on and motivate future research to conduct more reliable benchmarks of LLMs for code translation.",
        "Publication date": "25 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.17214"
    },
    {
        "ID": 229,
        "Title": "ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search",
        "Authors": [
            "Zehan Li",
            "Jianfei Zhang",
            "Chuantao Yin",
            "Yuanxin Ouyang",
            "Wenge Rong"
        ],
        "Abstract": "Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.",
        "Publication date": "25 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.16702"
    },
    {
        "ID": 230,
        "Title": "ChatDBG: An AI-Powered Debugging Assistant",
        "Authors": [
            "Kyla Levin",
            "Nicolas van Kempen",
            "Emery D. Berger",
            "Stephen N. Freund"
        ],
        "Abstract": "Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like `why is x null?'. To handle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded roughly 50,000 times.",
        "Publication date": "24 September, 2024",
        "Link": "https://arxiv.org/pdf/2403.16354"
    },
    {
        "ID": 231,
        "Title": "NonlinearSolve.jl: High-Performance and Robust Solvers for Systems of Nonlinear Equations in Julia",
        "Authors": [
            "Avik Pal",
            "Flemming Holtorf",
            "Axel Larsson",
            "Torkel Loman",
            "Utkarsh",
            "Frank SchÃ¤efer",
            "Qingyu Qu",
            "Alan Edelman",
            "Chris Rackauckas"
        ],
        "Abstract": "Efficiently solving nonlinear equations underpins numerous scientific and engineering disciplines, yet scaling these solutions for complex system models remains a challenge. This paper presents NonlinearSolve.jl - a suite of high-performance open-source nonlinear equation solvers implemented natively in the Julia programming language. NonlinearSolve.jl distinguishes itself by offering a unified API that accommodates a diverse range of solver specifications alongside features such as automatic algorithm selection based on runtime analysis, support for GPU-accelerated computation through static array kernels, and the utilization of sparse automatic differentiation and Jacobian-free Krylov methods for large-scale problem-solving. Through rigorous comparison with established tools such as Sundials and MINPACK, NonlinearSolve.jl demonstrates unparalleled robustness and efficiency, achieving significant advancements in solving benchmark problems and challenging real-world applications. The capabilities of NonlinearSolve.jl unlock new potentials in modeling and simulation across various domains, making it a valuable addition to the computational toolkit of researchers and practitioners alike.",
        "Publication date": "28 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.16341"
    },
    {
        "ID": 232,
        "Title": "CodeShell Technical Report",
        "Authors": [
            "Rui Xie",
            "Zhengran Zeng",
            "Zhuohao Yu",
            "Chang Gao",
            "Shikun Zhang",
            "Wei Ye"
        ],
        "Abstract": "Code large language models mark a pivotal breakthrough in artificial intelligence. They are specifically crafted to understand and generate programming languages, significantly boosting the efficiency of coding development workflows. In this technical report, we present CodeShell-Base, a seven billion-parameter foundation model with 8K context length, showcasing exceptional proficiency in code comprehension. By incorporating Grouped-Query Attention and Rotary Positional Embedding into GPT-2, CodeShell-Base integrates the structural merits of StarCoder and CodeLlama and forms its unique architectural design. We then carefully built a comprehensive data pre-processing process, including similar data deduplication, perplexity-based data filtering, and model-based data filtering. Through this process, We have curated 100 billion high-quality pre-training data from GitHub. Benefiting from the high-quality data, CodeShell-Base outperforms CodeLlama in Humaneval after training on just 500 billion tokens (5 epochs). We have conducted extensive experiments across multiple language datasets, including Python, Java, and C++, and the results indicate that our model possesses robust foundational capabilities in code comprehension and generation.",
        "Publication date": "23 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.15747"
    },
    {
        "ID": 233,
        "Title": "A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond",
        "Authors": [
            "Qiushi Sun",
            "Zhirui Chen",
            "Fangzhi Xu",
            "Kanzhi Cheng",
            "Chang Ma",
            "Zhangyue Yin",
            "Jianing Wang",
            "Chengcheng Han",
            "Renyu Zhu",
            "Shuai Yuan",
            "Qipeng Guo",
            "Xipeng Qiu",
            "Pengcheng Yin",
            "Xiaoli Li",
            "Fei Yuan",
            "Lingpeng Kong",
            "Xiang Li",
            "Zhiyong Wu"
        ],
        "Abstract": "Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we also observe a co-evolving shift. It spans from initial endeavors to tackling specific scenarios, through exploring a diverse array of tasks during its rapid expansion, to currently focusing on tackling increasingly complex and varied real-world challenges. Building on our examination of the developmental trajectories, we further investigate the emerging synergies between code intelligence and broader machine intelligence, uncovering new cross-domain opportunities and illustrating the substantial influence of code intelligence across various domains. Finally, we delve into both the opportunities and challenges associated with this field, alongside elucidating our insights on the most promising research directions. An ongoing, dynamically updated project and resources associated with this survey have been released at https://github.com/QiushiSun/NCISurvey.",
        "Publication date": "31 August, 2024",
        "Link": "https://arxiv.org/pdf/2403.14734"
    },
    {
        "ID": 234,
        "Title": "Compiler generated feedback for Large Language Models",
        "Authors": [
            "Dejan Grubisic",
            "Chris Cummins",
            "Volker Seeker",
            "Hugh Leather"
        ],
        "Abstract": "We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.",
        "Publication date": "18 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.14714"
    },
    {
        "ID": 235,
        "Title": "ChatGPT in Veterinary Medicine: A Practical Guidance of Generative Artificial Intelligence in Clinics, Education, and Research",
        "Authors": [
            "Candice P. Chu"
        ],
        "Abstract": "ChatGPT, the most accessible generative artificial intelligence (AI) tool, offers considerable potential for veterinary medicine, yet a dedicated review of its specific applications is lacking. This review concisely synthesizes the latest research and practical applications of ChatGPT within the clinical, educational, and research domains of veterinary medicine. It intends to provide specific guidance and actionable examples of how generative AI can be directly utilized by veterinary professionals without a programming background. For practitioners, ChatGPT can extract patient data, generate progress notes, and potentially assist in diagnosing complex cases. Veterinary educators can create custom GPTs for student support, while students can utilize ChatGPT for exam preparation. ChatGPT can aid in academic writing tasks in research, but veterinary publishers have set specific requirements for authors to follow. Despite its transformative potential, careful use is essential to avoid pitfalls like hallucination. This review addresses ethical considerations, provides learning resources, and offers tangible examples to guide responsible implementation. Carefully selected, up-to-date links to platforms that host large language models are provided for advanced readers with programming capability. A table of key takeaways was provided to summarize this review. By highlighting potential benefits and limitations, this review equips veterinarians, educators, and researchers to harness the power of ChatGPT effectively.",
        "Publication date": "25 February, 2024",
        "Link": "https://arxiv.org/pdf/2403.14654"
    },
    {
        "ID": 236,
        "Title": "Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics",
        "Authors": [
            "Shan Jia",
            "Reilin Lyu",
            "Kangran Zhao",
            "Yize Chen",
            "Zhiyuan Yan",
            "Yan Ju",
            "Chuanbo Hu",
            "Xin Li",
            "Baoyuan Wu",
            "Siwei Lyu"
        ],
        "Abstract": "DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.",
        "Publication date": "11 June, 2024",
        "Link": "https://arxiv.org/pdf/2403.14077"
    },
    {
        "ID": 237,
        "Title": "Regent based parallel meshfree LSKUM solver for heterogenous HPC platforms",
        "Authors": [
            "Sanath Salil",
            "Nischay Ram Mamidi",
            "Anil Nemili",
            "Elliott Slaughter"
        ],
        "Abstract": "Regent is an implicitly parallel programming language that allows the development of a single codebase for heterogeneous platforms targeting CPUs and GPUs. This paper presents the development of a parallel meshfree solver in Regent for two-dimensional inviscid compressible flows. The meshfree solver is based on the least squares kinetic upwind method. Example codes are presented to show the difference between the Regent and CUDA-C implementations of the meshfree solver on a GPU node. For CPU parallel computations, details are presented on how the data communication and synchronisation are handled by Regent and Fortran+MPI codes. The Regent solver is verified by applying it to the standard test cases for inviscid flows. Benchmark simulations are performed on coarse to very fine point distributions to assess the solver's performance. The computational efficiency of the Regent solver on an A100 GPU is compared with an equivalent meshfree solver written in CUDA-C. The codes are then profiled to investigate the differences in their performance. The performance of the Regent solver on CPU cores is compared with an equivalent explicitly parallel Fortran meshfree solver based on MPI. Scalability results are shown to offer insights into performance.",
        "Publication date": "19 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.13287"
    },
    {
        "ID": 238,
        "Title": "Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs",
        "Authors": [
            "Zhihong Sun",
            "Chen Lyu",
            "Bolun Li",
            "Yao Wan",
            "Hongyu Zhang",
            "Ge Li",
            "Zhi Jin"
        ],
        "Abstract": "Large Language Models (LLMs) have recently made significant advances in code generation through the 'Chain-of-Thought' prompting technique. This technique empowers the model to autonomously devise \"solution plans\" to tackle intricate programming challenges, thereby improving its performance in code generation. Nevertheless, smaller models have been struggling to keep up with LLMs in deducing these plans, adversely affecting their code generation capabilities. Given the considerable size and associated deployment costs, along with concerns about data security, many teams opt for deploying smaller models for code generation. Consequently, there arises a compelling need for transferring LLMs' code generation reasoning abilities to the smaller models. In this paper, we propose the CodePLAN framework, which aims to transfer LLMs' reasoning capabilities to smaller models through distillation. We adopt a multi-task learning approach, jointly undertaking code generation and solution plan generation tasks, to enhance the code generation capabilities of the smaller model. To ensure the superior quality of the solution plans, we advocate for the utilization of backward reasoning and plan sampling strategies. Our experiments show that in comparison to the conventional fine-tuning approach, our approach improves the smaller model's code generation performance (measured in pass@1 metric) by over 130% on the challenging APPS benchmark.",
        "Publication date": "19 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.13271"
    },
    {
        "ID": 239,
        "Title": "Fundamental Components of Deep Learning: A category-theoretic approach",
        "Authors": [
            "Bruno GavranoviÄ"
        ],
        "Abstract": "Deep learning, despite its remarkable achievements, is still a young field. Like the early stages of many scientific disciplines, it is marked by the discovery of new phenomena, ad-hoc design decisions, and the lack of a uniform and compositional mathematical foundation. From the intricacies of the implementation of backpropagation, through a growing zoo of neural network architectures, to the new and poorly understood phenomena such as double descent, scaling laws or in-context learning, there are few unifying principles in deep learning. This thesis develops a novel mathematical foundation for deep learning based on the language of category theory. We develop a new framework that is a) end-to-end, b) unform, and c) not merely descriptive, but prescriptive, meaning it is amenable to direct implementation in programming languages with sufficient features. We also systematise many existing approaches, placing many existing constructions and concepts from the literature under the same umbrella. In Part I we identify and model two main properties of deep learning systems parametricity and bidirectionality by we expand on the previously defined construction of actegories and Para to study the former, and define weighted optics to study the latter. Combining them yields parametric weighted optics, a categorical model of artificial neural networks, and more. Part II justifies the abstractions from Part I, applying them to model backpropagation, architectures, and supervised learning. We provide a lens-theoretic axiomatisation of differentiation, covering not just smooth spaces, but discrete settings of boolean circuits as well. We survey existing, and develop new categorical models of neural network architectures. We formalise the notion of optimisers and lastly, combine all the existing concepts together, providing a uniform and compositional framework for supervised learning.",
        "Publication date": "12 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.13001"
    },
    {
        "ID": 240,
        "Title": "C Analyzer : A Static Program Analysis Tool for C Programs",
        "Authors": [
            "Rajendra Kumar Solanki"
        ],
        "Abstract": "In our times, when the world is increasingly becoming more dependent on software programs, writing bug-free, correct programs is crucial. Program verification based on formal methods can guarantee this by detecting run-time errors in safety-critical systems to avoid possible adverse impacts on human life and save time and money.\n  This project work tries to leverage Abstract Interpretation techniques for static analysis of C programs. C Analyzer is a tool developed for static analysis of C programs. This implementation of C Analyzer provides a plug-and-play domain architecture for multiple abstract domains to be used. C Analyzer supports four abstract domains - Interval, Octagon, Polyhedra, and Bit Vector. We use these different domains for required precision in program verification. C Analyzer tool uses LLVM C/C++ compiler frontend Clang API to generate and traverse the Control Flow Graph (CFG) of a given C program. This tool generates invariants in different abstract domains for statements in basic blocks of CFG during CFG traversal. Using these invariants, some properties of a program, such as dividing by zero, modulus zero, arithmetic overflow, etc., can be analyzed. We also use a source-to-source transformation tool, CIL (Common Intermediate language), to transform some C constructs into simpler constructs, such as transforming logical operators, switch statements, and conditional operators into if-else ladders and transforming do-while and for loops into while loops.\n  Using C Analyzer, C program constructs such as declarations, assignments, binary operations (arithmetic, relational, bitwise shift, etc.), conditions (if-else), loops (while, do while, for loop), nested conditions, and nested loops can be analyzed. Currently, this tool does not support arrays, structures, unions, pointers, or function calls.",
        "Publication date": "28 January, 2024",
        "Link": "https://arxiv.org/pdf/2403.12973"
    },
    {
        "ID": 241,
        "Title": "Python Fuzzing for Trustworthy Machine Learning Frameworks",
        "Authors": [
            "Ilya Yegorov",
            "Eli Kobrin",
            "Darya Parygina",
            "Alexey Vishnyakov",
            "Andrey Fedotov"
        ],
        "Abstract": "Ensuring the security and reliability of machine learning frameworks is crucial for building trustworthy AI-based systems. Fuzzing, a popular technique in secure software development lifecycle (SSDLC), can be used to develop secure and robust software. Popular machine learning frameworks such as PyTorch and TensorFlow are complex and written in multiple programming languages including C/C++ and Python. We propose a dynamic analysis pipeline for Python projects using the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus minimization, crash triaging, and coverage collection. Crash triaging and severity estimation are important steps to ensure that the most critical vulnerabilities are addressed promptly. Furthermore, the proposed pipeline is integrated in GitLab CI. To identify the most vulnerable parts of the machine learning frameworks, we analyze their potential attack surfaces and develop fuzz targets for PyTorch, TensorFlow, and related projects such as h5py. Applying our dynamic analysis pipeline to these targets, we were able to discover 3 new bugs and propose fixes for them.",
        "Publication date": "19 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.12723"
    },
    {
        "ID": 242,
        "Title": "A Coq Mechanization of JavaScript Regular Expression Semantics",
        "Authors": [
            "NoÃ© De Santo",
            "AurÃ¨le BarriÃ¨re",
            "ClÃ©ment Pit-Claudel"
        ],
        "Abstract": "We present an executable, proven-safe, faithful, and future-proof Coq mechanization of JavaScript regular expression (regex) matching, as specified by the latest published edition of ECMA-262 section 22.2. This is, to our knowledge, the first time that an industrial-strength regex language has been faithfully mechanized in an interactive theorem prover. We highlight interesting challenges that arose in the process (including issues of encoding, corner cases, and executability), and we document the steps that we took to ensure that the result is straightforwardly auditable and that our understanding of the specification aligns with existing implementations.\n  We demonstrate the usability and versatility of the mechanization through a broad collection of analyses, case studies, and experiments: we prove that JavaScript regex matching always terminates and is safe (no assertion failures); we identify subtle corner cases that led to mistakes in previous publications; we verify an optimization extracted from a state-of-the-art regex engine; we show that some classic properties described in automata textbooks and used in derivatives-based matchers do not hold in JavaScript regexes; and we demonstrate that the cost of updating the mechanization to account for changes in the original specification is reasonably low.\n  Our mechanization can be extracted to OCaml and JavaScript and linked with Unicode libraries to produce an executable regex engine that passes the relevant parts of the official Test262 conformance test suite.",
        "Publication date": "26 July, 2024",
        "Link": "https://arxiv.org/pdf/2403.11919"
    },
    {
        "ID": 243,
        "Title": "LLM Guided Evolution -- The Automation of Models Advancing Models",
        "Authors": [
            "Clint Morris",
            "Michael Jurado",
            "Jason Zutty"
        ],
        "Abstract": "In the realm of machine learning, traditional model development and automated approaches like AutoML typically rely on layers of abstraction, such as tree-based or Cartesian genetic programming. Our study introduces \"Guided Evolution\" (GE), a novel framework that diverges from these methods by utilizing Large Language Models (LLMs) to directly modify code. GE leverages LLMs for a more intelligent, supervised evolutionary process, guiding mutations and crossovers. Our unique \"Evolution of Thought\" (EoT) technique further enhances GE by enabling LLMs to reflect on and learn from the outcomes of previous mutations. This results in a self-sustaining feedback loop that augments decision-making in model evolution. GE maintains genetic diversity, crucial for evolutionary algorithms, by leveraging LLMs' capability to generate diverse responses from expertly crafted prompts and modulate model temperature. This not only accelerates the evolution process but also injects expert like creativity and insight into the process. Our application of GE in evolving the ExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously produced variants with improved accuracy, increasing from 92.52% to 93.34%, without compromising model compactness. This underscores the potential of LLMs to accelerate the traditional model design pipeline, enabling models to autonomously evolve and enhance their own designs.",
        "Publication date": "17 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.11446"
    },
    {
        "ID": 244,
        "Title": "Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework",
        "Authors": [
            "Kaiyan Chang",
            "Kun Wang",
            "Nan Yang",
            "Ying Wang",
            "Dantong Jin",
            "Wenlong Zhu",
            "Zhirong Chen",
            "Cangyuan Li",
            "Hao Yan",
            "Yunhao Zhou",
            "Zhuoliang Zhao",
            "Yuan Cheng",
            "Yudong Pan",
            "Yiqi Liu",
            "Mengdi Wang",
            "Shengwen Liang",
            "Yinhe Han",
            "Huawei Li",
            "Xiaowei Li"
        ],
        "Abstract": "Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts. Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design. However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers. This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts. For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template. For Verilog repair, it uses predefined rules to generate the wrong verilog file and then pairs EDA Tool feedback with the right and wrong verilog file. For EDA Script generation, it uses existing LLM(GPT-3.5) to obtain the description of the Script. To evaluate the effectiveness of our data augmentation method, we finetune Llama2-13B and Llama2-7B models using the dataset generated by our augmentation framework. The results demonstrate a significant improvement in the Verilog generation tasks with LLMs. Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.",
        "Publication date": "10 July, 2024",
        "Link": "https://arxiv.org/pdf/2403.11202"
    },
    {
        "ID": 245,
        "Title": "CETASim: A numerical tool for beam collective effect study in storage rings",
        "Authors": [
            "Chao Li",
            "Yong-Chul Chae"
        ],
        "Abstract": "We developed a 6D multi-particle tracking program CETASim in C++ programming language to simulate intensity-dependent effects in electron storage rings. The program can simulate the beam collective effects due to short-range/long-range wakefields for single/coupled-bunch instability studies. It also features to simulate interactions among charged ions and the trains of electron bunches, including both fast ion and ion trapping effects. The bunch-by-bunch feedback is also included so that the user can simulate the damping of the unstable motion when its growth rate is faster than the radiation damping rate. The particle dynamics is based on the one-turn map, including the nonlinear effects of amplitude-dependent tune shift, high-order chromaticity, and second-order momentum compaction factor. A skew quadrupole can also be introduced by the users, which is very useful for the emittance sharing and the emittance exchange studies. This paper describes the code structure, the physics models, and the algorithms used in CETASim. We also present the results of its application to PETRA-IV storage ring.",
        "Publication date": "16 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.10973"
    },
    {
        "ID": 246,
        "Title": "A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks",
        "Authors": [
            "Beatrice Casey",
            "Joanna C. S. Santos",
            "George Perry"
        ],
        "Abstract": "Machine learning techniques for cybersecurity-related software engineering tasks are becoming increasingly popular. The representation of source code is a key portion of the technique that can impact the way the model is able to learn the features of the source code. With an increasing number of these techniques being developed, it is valuable to see the current state of the field to better understand what exists and what's not there yet. This paper presents a study of these existing ML-based approaches and demonstrates what type of representations were used for different cybersecurity tasks and programming languages. Additionally, we study what types of models are used with different representations. We have found that graph-based representations are the most popular category of representation, and Tokenizers and Abstract Syntax Trees (ASTs) are the two most popular representations overall. We also found that the most popular cybersecurity task is vulnerability detection, and the language that is covered by the most techniques is C. Finally, we found that sequence-based models are the most popular category of models, and Support Vector Machines (SVMs) are the most popular model overall.",
        "Publication date": "15 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.10646"
    },
    {
        "ID": 247,
        "Title": "Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties",
        "Authors": [
            "Denis Schwachhofer",
            "Peter Domanski",
            "Steffen Becker",
            "Stefan Wagner",
            "Matthias Sauer",
            "Dirk PflÃ¼ger",
            "Ilia Polian"
        ],
        "Abstract": "System-Level Test (SLT) has been a part of the test flow for integrated circuits for over a decade and still gains importance. However, no systematic approaches exist for test program generation, especially targeting non-functional properties of the Device under Test (DUT). Currently, test engineers manually compose test suites from off-the-shelf software, approximating the end-user environment of the DUT. This is a challenging and tedious task that does not guarantee sufficient control over non-functional properties. This paper proposes Large Language Models (LLMs) to generate test programs. We take a first glance at how pre-trained LLMs perform in test program generation to optimize non-functional properties of the DUT. Therefore, we write a prompt to generate C code snippets that maximize the instructions per cycle of a super-scalar, out-of-order architecture in simulation. Additionally, we apply prompt and hyperparameter optimization to achieve the best possible results without further training.",
        "Publication date": "19 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.10086"
    },
    {
        "ID": 248,
        "Title": "Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality",
        "Authors": [
            "Cathy Mengying Fang",
            "Krzysztof ZieliÅski",
            "Pattie Maes",
            "Joe Paradiso",
            "Bruce Blumberg",
            "Mikkel Baun KjÃ¦rgaard"
        ],
        "Abstract": "Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. We propose a framework that simplifies robot deployment by allowing direct communication using natural language. It uses large language models (LLM) for prompt processing, workspace understanding, and waypoint generation. It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome. We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).",
        "Publication date": "17 July, 2024",
        "Link": "https://arxiv.org/pdf/2403.09308"
    },
    {
        "ID": 249,
        "Title": "Bugs in Large Language Models Generated Code: An Empirical Study",
        "Authors": [
            "Florian Tambon",
            "Arghavan Moradi Dakhel",
            "Amin Nikanjam",
            "Foutse Khomh",
            "Michel C. Desmarais",
            "Giuliano Antoniol"
        ],
        "Abstract": "Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.",
        "Publication date": "18 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.08937"
    },
    {
        "ID": 250,
        "Title": "JAXbind: Bind any function to JAX",
        "Authors": [
            "Jakob Roth",
            "Martin Reinecke",
            "Gordian Edenhofer"
        ],
        "Abstract": "JAX is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into JAX. Reimplementing the existing code in JAX is often impractical and the existing interface in JAX for binding custom code either limits the user to a single Jacobian product or requires deep knowledge of JAX and its C++ backend for general Jacobian products. With JAXbind we drastically reduce the effort required to bind custom functions implemented in other programming languages with full support for Jacobian-vector products and vector-Jacobian products to JAX. Specifically, JAXbind provides an easy-to-use Python interface for defining custom, so-called JAX primitives. Via JAXbind, any function callable from Python can be exposed as a JAX primitive. JAXbind allows a user to interface the JAX function transformation engine with custom derivatives and batching rules, enabling all JAX transformations for the custom primitive.",
        "Publication date": "27 June, 2024",
        "Link": "https://arxiv.org/pdf/2403.08847"
    },
    {
        "ID": 251,
        "Title": "DevBench: A Comprehensive Benchmark for Software Development",
        "Authors": [
            "Bowen Li",
            "Wenhan Wu",
            "Ziwei Tang",
            "Lin Shi",
            "John Yang",
            "Jinyang Li",
            "Shunyu Yao",
            "Chen Qian",
            "Binyuan Hui",
            "Qicheng Zhang",
            "Zhiyin Yu",
            "He Du",
            "Ping Yang",
            "Dahua Lin",
            "Chao Peng",
            "Kai Chen"
        ],
        "Abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications. Our benchmark is available at https://github.com/open-compass/DevBench",
        "Publication date": "15 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.08604"
    },
    {
        "ID": 252,
        "Title": "DrPlanner: Diagnosis and Repair of Motion Planners for Automated Vehicles Using Large Language Models",
        "Authors": [
            "Yuanfei Lin",
            "Chenran Li",
            "Mingyu Ding",
            "Masayoshi Tomizuka",
            "Wei Zhan",
            "Matthias Althoff"
        ],
        "Abstract": "Motion planners are essential for the safe operation of automated vehicles across various scenarios. However, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive. To tackle the aforementioned issues, we present DrPlanner, the first framework designed to automatically diagnose and repair motion planners using large language models. Initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages. Leveraging the profound capabilities of large language models, our framework returns repaired planners with detailed diagnostic descriptions. Furthermore, our framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes. Our approach is validated using both search- and sampling-based motion planners for automated vehicles; experimental results highlight the need for demonstrations in the prompt and show the ability of our framework to effectively identify and rectify elusive issues.",
        "Publication date": "7 August, 2024",
        "Link": "https://arxiv.org/pdf/2403.07470"
    },
    {
        "ID": 253,
        "Title": "Cedar: A New Language for Expressive, Fast, Safe, and Analyzable Authorization (Extended Version)",
        "Authors": [
            "Joseph W. Cutler",
            "Craig Disselkoen",
            "Aaron Eline",
            "Shaobo He",
            "Kyle Headley",
            "Michael Hicks",
            "Kesha Hietala",
            "Eleftherios Ioannidis",
            "John Kastner",
            "Anwar Mamat",
            "Darin McAdams",
            "Matt McCutchen",
            "Neha Rungta",
            "Emina Torlak",
            "Andrew Wells"
        ],
        "Abstract": "Cedar is a new authorization policy language designed to be ergonomic, fast, safe, and analyzable. Rather than embed authorization logic in an application's code, developers can write that logic as Cedar policies and delegate access decisions to Cedar's evaluation engine. Cedar's simple and intuitive syntax supports common authorization use-cases with readable policies, naturally leveraging concepts from role-based, attribute-based, and relation-based access control models. Cedar's policy structure enables access requests to be decided quickly. Cedar's policy validator leverages optional typing to help policy writers avoid mistakes, but not get in their way. Cedar's design has been finely balanced to allow for a sound and complete logical encoding, which enables precise policy analysis, e.g., to ensure that when refactoring a set of policies, the authorized permissions do not change. We have modeled Cedar in the Lean programming language, and used Lean's proof assistant to prove important properties of Cedar's design. We have implemented Cedar in Rust, and released it open-source. Comparing Cedar to two open-source languages, OpenFGA and Rego, we find (subjectively) that Cedar has equally or more readable policies, but (objectively) performs far better.",
        "Publication date": "8 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.04651"
    },
    {
        "ID": 254,
        "Title": "Identify Critical Nodes in Complex Network with Large Language Models",
        "Authors": [
            "Jinzhu Mao",
            "Dongyun Zou",
            "Li Sheng",
            "Siyi Liu",
            "Chen Gao",
            "Yue Wang",
            "Yong Li"
        ],
        "Abstract": "Identifying critical nodes in networks is a classical decision-making task, and many methods struggle to strike a balance between adaptability and utility. Therefore, we propose an approach that empowers Evolutionary Algorithm (EA) with Large Language Models (LLMs), to generate a function called \"score\\_nodes\" which can further be used to identify crucial nodes based on their assigned scores. Our model consists of three main components: Manual Initialization, Population Management, and LLMs-based Evolution. It evolves from initial populations with a set of designed node scoring functions created manually. LLMs leverage their strong contextual understanding and rich programming skills to perform crossover and mutation operations on the individuals, generating excellent new functions. These functions are then categorized, ranked, and eliminated to ensure the stable development of the populations while preserving diversity. Extensive experiments demonstrate the excellent performance of our method, showcasing its strong generalization ability compared to other state-of-the-art algorithms. It can consistently and orderly generate diverse and efficient node scoring functions. All source codes and models that can reproduce all results in this work are publicly available at this link: \\url{https://anonymous.4open.science/r/LLM4CN-6520}",
        "Publication date": "1 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.03962"
    },
    {
        "ID": 255,
        "Title": "Automatic Bi-modal Question Title Generation for Stack Overflow with Prompt Learning",
        "Authors": [
            "Shaoyu Yang",
            "Xiang Chen",
            "Ke Liu",
            "Guang Yang",
            "Chi Yu"
        ],
        "Abstract": "When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts). To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction.",
        "Publication date": "6 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.03677"
    },
    {
        "ID": 256,
        "Title": "Dr Wenowdis: Specializing dynamic language C extensions using type information",
        "Authors": [
            "Maxwell Bernstein",
            "CF Bolz-Tereick"
        ],
        "Abstract": "C-based interpreters such as CPython make extensive use of C \"extension\" code, which is opaque to static analysis tools and faster runtimes with JIT compilers, such as PyPy. Not only are the extensions opaque, but the interface between the dynamic language types and the C types can introduce impedance. We hypothesise that frequent calls to C extension code introduce significant overhead that is often unnecessary. We validate this hypothesis by introducing a simple technique, \"typed methods\", which allow selected C extension functions to have additional metadata attached to them in a backward-compatible way. This additional metadata makes it much easier for a JIT compiler (and as we show, even an interpreter!) to significantly reduce the call and return overhead. Although we have prototyped typed methods in PyPy, we suspect that the same technique is applicable to a wider variety of language runtimes and that the information can also be consumed by static analysis tooling.",
        "Publication date": "4 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.02420"
    },
    {
        "ID": 257,
        "Title": "LiveRec: Prototyping Probes by Framing Debug Protocols",
        "Authors": [
            "Jean-Baptiste DÃ¶derlein",
            "Riemer van Rozen",
            "Tijs van der Storm"
        ],
        "Abstract": "Context:  In the first part of his 2012 presentation \"Inventing on Principle\", Bret Victor gives a demo of a live code editor for Javascript which shows the dynamic history of values of variables in real time. This form of live programming has become known as \"probes\". Probes provide the programmer with permanent and continuous insight into the dynamic evolution of function or method variables, thus improving feedback and developer experience.\n  Inquiry: Although Victor shows a working prototype of live probes in the context of Javascript, he does not discuss strategies for implementing them. Later work provides an implementation approach, but this requires a programming language to be implemented on top of the GraalVM runtime. In this paper we present **LiveRec**, a generic approach for implementing probes which can be applied in the context of many programming languages, without requiring the modification of compilers or run-time systems.\n  Approach:  **LiveRec** is based on reusing existing debug protocols to implement probes. Methods or functions are compiled after every code change and executed inside the debugger. During execution the evolution of all local variables in the current stack frame are recorded and communicated back to the editor or IDE for display to the user.\n  Knowledge:  It turns out that mainstream debug protocols are rich enough for implementing live probes. Step-wise execution, code hot swapping, and stack frame inspection provide the right granularity and sufficient information to realize live probes, without modifying compilers or language runtimes. Furthermore, it turns out that the recently proposed Debugger Adapter Protocol (DAP) provides an even more generic approach of implementing live probes, but, in some cases, at the cost of a significant performance penalty.\n  Grounding:  We have applied **LiveRec** to implement probes using stack recording natively for Java through the Java Debug Interface (JDI), and through the DAP for Java, Python, C, and Javascript, all requiring just modest amounts of configuration code. We evaluate the run-time performance of all four probes prototypes, decomposed into: compile-after-change, hot swap, single step overhead, and stack recording overhead. Our initial results show that live probes on top of native debug APIs can be performant enough for interactive use. In the case of DAP, however, it highly depends on characteristics of the programming language implementation and its associated debugging infrastructure.\n  Importance: Live programming improves the programmer experience by providing immediate feedback about a program's execution and eliminating disruptive edit-compile-restart sequences. Probes are one way to shorten the programmer feedback loop at the level of functions and methods. Although probes are not new, and have been implemented in (prototype) systems, **LiveRec**'s approach of building live probes on top of existing and generic debug protocols promises a path towards probes for a host of mainstream programming languages, with reasonable effort.",
        "Publication date": "4 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.02161"
    },
    {
        "ID": 258,
        "Title": "Making Hybrid Languages: A Recipe",
        "Authors": [
            "Leif Andersen",
            "Cameron Moy",
            "Stephen Chang",
            "Matthias Felleisen"
        ],
        "Abstract": "The dominant programming languages support only linear text to express ideas. Visual languages offer graphical representations for entire programs, when viewed with special tools. Hybrid languages, with support from existing tools, allow developers to express their ideas with a mix of textual and graphical syntax tailored to an application domain. This mix puts both kinds of syntax on equal footing and, importantly, the enriched language does not disrupt a programmer's typical workflow. This paper presents a recipe for equipping existing textual programming languages as well as accompanying IDEs with a mechanism for creating and using graphical interactive syntax. It also presents the first hybrid language and IDE created using the recipe.",
        "Publication date": "2 March, 2024",
        "Link": "https://arxiv.org/pdf/2403.01335"
    },
    {
        "ID": 259,
        "Title": "Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning",
        "Authors": [
            "Christian Raymond",
            "Qi Chen",
            "Bing Xue",
            "Mengjie Zhang"
        ],
        "Abstract": "In this paper, we develop upon the topic of loss function learning, an emergent meta-learning paradigm that aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new meta-learning framework for task and model-agnostic loss function learning via a hybrid search approach. The framework first uses genetic programming to find a set of symbolic loss functions. Second, the set of learned loss functions is subsequently parameterized and optimized via unrolled differentiation. The versatility and performance of the proposed framework are empirically validated on a diverse set of supervised learning tasks. Results show that the learned loss functions bring improved convergence, sample efficiency, and inference performance on tabulated, computer vision, and natural language processing problems, using a variety of task-specific neural network architectures.",
        "Publication date": "29 February, 2024",
        "Link": "https://arxiv.org/pdf/2403.00865"
    },
    {
        "ID": 260,
        "Title": "pAGN: the one-stop solution for AGN disc modeling",
        "Authors": [
            "Daria Gangardt",
            "Alessandro Alberto Trani",
            "ClÃ©ment Bonnerot",
            "Davide Gerosa"
        ],
        "Abstract": "Models of accretion discs surrounding active galactic nuclei (AGNs) find vast applications in high-energy astrophysics. The broad strategy is to parametrize some of the key disc properties such as gas density and temperature as a function of the radial coordinate from a given set of assumptions on the underlying physics. Two of the most popular approaches in this context were presented by Sirko & Goodman (2003) and Thompson et al. (2005). We present a critical reanalysis of these widely used models, detailing their assumptions and clarifying some steps in their derivation that were previously left unsaid. Our findings are implemented in the pAGN module for the Python programming language, which is the first public implementation of these accretion-disc models. We further apply pAGN to the evolution of stellar-mass black holes embedded in AGN discs, addressing the potential occurrence of migration traps.",
        "Publication date": "14 May, 2024",
        "Link": "https://arxiv.org/pdf/2403.00060"
    },
    {
        "ID": 261,
        "Title": "StarCoder 2 and The Stack v2: The Next Generation",
        "Authors": [
            "Anton Lozhkov",
            "Raymond Li",
            "Loubna Ben Allal",
            "Federico Cassano",
            "Joel Lamy-Poirier",
            "Nouamane Tazi",
            "Ao Tang",
            "Dmytro Pykhtar",
            "Jiawei Liu",
            "Yuxiang Wei",
            "Tianyang Liu",
            "Max Tian",
            "Denis Kocetkov",
            "Arthur Zucker",
            "Younes Belkada",
            "Zijian Wang",
            "Qian Liu",
            "Dmitry Abulkhanov",
            "Indraneil Paul",
            "Zhuang Li",
            "Wen-Ding Li",
            "Megan Risdal",
            "Jia Li",
            "Jian Zhu",
            "Terry Yue Zhuo"
        ],
        "Abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.",
        "Publication date": "29 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.19173"
    },
    {
        "ID": 262,
        "Title": "A Naive Approach for Automatic Line-level Code Completion",
        "Authors": [
            "Shamima Naznin",
            "Dr. Manishankar Mondal"
        ],
        "Abstract": "Coding is an integral aspect of programming. A programmer can automatically complete a code fragment after writing a few tokens, and the process of automatic completion is known as code completion. Several research studies on code completion have previously been conducted for method body completion and method parameter completion. However, this fundamental study explores the automatic completion of any program statement that might not even be part of a method.\n  The goal is to provide suggestions to the programmer for completing code throughout the codebase by identifying and analyzing code similarities. The proposed methodology can be regarded as a fundamental framework for automated code completion. From the investigation of hundreds of revisions of four subject systems written in C and Java, it is observed that the proposed method can automatically complete around 22% of code statements with an average accuracy of 87% that a programmer writes during development, accelerating software development time. The empirical analysis further demonstrates that the approach can be used with programming language neutrality.\n  The study concludes by illustrating that taking 10 characters as prefixes before invoking completion provides maximum precision.",
        "Publication date": "29 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.19120"
    },
    {
        "ID": 263,
        "Title": "Time-efficient filtering of polarimetric data by checking physical realizability of experimental Mueller matrices",
        "Authors": [
            "Tatiana Novikova",
            "Alexey Ovchinnikov",
            "Gleb Pogudin",
            "Jessica C. Ramella-Roman"
        ],
        "Abstract": "Imaging Mueller polarimetry has already proved its potential for metrology, remote sensing and biomedicine. The real-time applications of this modality require both video rate image acquisition and fast data post-processing algorithms. First, one must check the physical realizability of the experimental Mueller matrices in order to filter out non-physical data, i.e. to test the positive semi-definiteness of the 4x4 Hermitian coherency matrix calculated from the elements of the corresponding Mueller matrix pixel-wise. For this purpose, we compared the execution time for the calculations of i) eigenvalues, ii) Cholesky decomposition, iii) Sylvester's criterion, and iv) coefficients of the characteristic polynomial of the Hermitian coherency matrix using two different approaches, all calculated for the experimental Mueller matrix images (600 pixels x 700 pixels) of mouse uterine cervix. The calculations were performed using C++ and Julia programming languages. Our results showed the superiority of the algorithm iv), in particular, the version based on the simplification via Pauli matrices, in terms of execution time for our dataset, over other algorithms. The sequential implementation of the latter algorithm on a single core already satisfies the requirements of real-time polarimetric imaging in various domains. This can be further amplified by the proposed parallelization (for example, we achieve a 5-fold speed up on 6 cores).",
        "Publication date": "28 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.18555"
    },
    {
        "ID": 264,
        "Title": "Libfork: portable continuation-stealing with stackless coroutines",
        "Authors": [
            "Conor John Williams",
            "James Elliott"
        ],
        "Abstract": "Fully-strict fork-join parallelism is a powerful model for shared-memory programming due to its optimal time scaling and strong bounds on memory scaling. The latter is rarely achieved due to the difficulty of implementing continuation stealing in traditional High Performance Computing (HPC) languages -- where it is often impossible without modifying the compiler or resorting to non-portable techniques. We demonstrate how stackless coroutines (a new feature in C++20) can enable fully-portable continuation stealing and present libfork a lock-free fine-grained parallelism library, combining coroutines with user-space, geometric segmented-stacks. We show our approach is able to achieve optimal time/memory scaling, both theoretically and empirically, across a variety of benchmarks. Compared to openMP (libomp), libfork is on average 7.2x faster and consumes 10x less memory. Similarly, compared to Intel's TBB, libfork is on average 2.7x faster and consumes 6.2x less memory. Additionally, we introduce non-uniform memory access (NUMA) optimizations for schedulers that demonstrate performance matching busy-waiting schedulers.",
        "Publication date": "28 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.18480"
    },
    {
        "ID": 265,
        "Title": "LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language",
        "Authors": [
            "Ming Wang",
            "Yuanzhong Liu",
            "Xiaoyu Liang",
            "Songlian Li",
            "Yijie Huang",
            "Xiaoming Zhang",
            "Sijia Shen",
            "Chaofeng Guan",
            "Daling Wang",
            "Shi Feng",
            "Huaiwen Zhang",
            "Yifei Zhang",
            "Minghui Zheng",
            "Chi Zhang"
        ],
        "Abstract": "LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. In addition, it is not conducive to the iterative updating of prompts. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the performance of LLMs. Moreover, the case study shows that LangGPT leads LLMs to generate higher-quality responses. Furthermore, we analyzed the ease of use and reusability of LangGPT through a user survey in our online community.",
        "Publication date": "29 June, 2024",
        "Link": "https://arxiv.org/pdf/2402.16929"
    },
    {
        "ID": 266,
        "Title": "NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification",
        "Authors": [
            "Hanna Abi Akl"
        ],
        "Abstract": "We present a neuro-symbolic (NeSy) workflow combining a symbolic-based learning technique with a large language model (LLM) agent to generate synthetic data for code comment classification in the C programming language. We also show how generating controlled synthetic data using this workflow fixes some of the notable weaknesses of LLM-based generation and increases the performance of classical machine learning models on the code comment classification task. Our best model, a Neural Network, achieves a Macro-F1 score of 91.412% with an increase of 1.033% after data augmentation.",
        "Publication date": "24 May, 2024",
        "Link": "https://arxiv.org/pdf/2402.16910"
    },
    {
        "ID": 267,
        "Title": "AgentScope: A Flexible yet Robust Multi-Agent Platform",
        "Authors": [
            "Dawei Gao",
            "Zitao Li",
            "Xuchen Pan",
            "Weirui Kuang",
            "Zhijian Ma",
            "Bingchen Qian",
            "Fei Wei",
            "Wenhao Zhang",
            "Yuexiang Xie",
            "Daoyuan Chen",
            "Liuyi Yao",
            "Hongyi Peng",
            "Zeyu Zhang",
            "Lin Zhu",
            "Chen Cheng",
            "Hongzhu Shi",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "Abstract": "With the rapid advancement of Large Language Models (LLMs), significant progress has been made in multi-agent applications. However, the complexities in coordinating agents' cooperation and LLMs' erratic performance pose notable challenges in developing robust and efficient multi-agent applications. To tackle these challenges, we propose AgentScope, a developer-centric multi-agent platform with message exchange as its core communication mechanism. The abundant syntactic tools, built-in agents and service functions, user-friendly interfaces for application demonstration and utility monitor, zero-code programming workstation, and automatic prompt tuning mechanism significantly lower the barriers to both development and deployment. Towards robust and flexible multi-agent application, AgentScope provides both built-in and customizable fault tolerance mechanisms. At the same time, it is also armed with system-level support for managing and utilizing multi-modal data, tools, and external knowledge. Additionally, we design an actor-based distribution framework, enabling easy conversion between local and distributed deployments and automatic parallel optimization without extra effort. With these features, AgentScope empowers developers to build applications that fully realize the potential of intelligent agents. We have released AgentScope at https://github.com/modelscope/agentscope, and hope AgentScope invites wider participation and innovation in this fast-moving field.",
        "Publication date": "20 May, 2024",
        "Link": "https://arxiv.org/pdf/2402.14034"
    },
    {
        "ID": 268,
        "Title": "Trustworthy Distributed Certification of Program Execution",
        "Authors": [
            "Alex Wolf",
            "Marco Edoardo Palma",
            "Pasquale Salza",
            "Harald C. Gall"
        ],
        "Abstract": "Verifying the execution of a program is complicated and often limited by the inability to validate the code's correctness. It is a crucial aspect of scientific research, where it is needed to ensure the reproducibility and validity of experimental results. Similarly, in customer software testing, it is difficult for customers to verify that their specific program version was tested or executed at all. Existing state-of-the-art solutions, such as hardware-based approaches, constraint solvers, and verifiable computation systems, do not provide definitive proof of execution, which hinders reliable testing and analysis of program results. In this paper, we propose an innovative approach that combines a prototype programming language called Mona with a certification protocol OCCP to enable the distributed and decentralized re-execution of program segments. Our protocol allows for certification of program segments in a distributed, immutable, and trustworthy system without the need for naive re-execution, resulting in significant improvements in terms of time and computational resources used. We also explore the use of blockchain technology to manage the protocol workflow following other approaches in this space. Our approach offers a promising solution to the challenges of program execution verification and opens up opportunities for further research and development in this area. Our findings demonstrate the efficiency of our approach in reducing the number of program executions compared to existing state-of-the-art methods, thus improving the efficiency of certifying program executions.",
        "Publication date": "21 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.13792"
    },
    {
        "ID": 269,
        "Title": "A Strategic Model of Software Dependency Networks",
        "Authors": [
            "Cornelius Fritz",
            "Co-Pierre Georg",
            "Angelo Mele",
            "Michael Schweinberger"
        ],
        "Abstract": "Modern software development involves collaborative efforts and reuse of existing code, which reduces the cost of developing new software. However, reusing code from existing packages exposes coders to vulnerabilities in these dependencies. We study the formation of dependency networks among software packages and libraries, guided by a structural model of network formation with observable and unobservable heterogeneity. We estimate costs, benefits, and link externalities of the network of 696,790 directed dependencies between 35,473 repositories of the Rust programming language using a novel scalable algorithm. We find evidence of a positive externality exerted on other coders when coders create dependencies. Furthermore, we show that coders are likely to link to more popular packages of the same software type but less popular packages of other types. We adopt models for the spread of infectious diseases to measure a package's systemicness as the number of downstream packages a vulnerability would affect. Systemicness is highly skewed with the most systemic repository affecting almost 90% of all repositories only two steps away. Lastly, we show that protecting only the ten most important repositories reduces vulnerability contagion by nearly 40%.",
        "Publication date": "20 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.13375"
    },
    {
        "ID": 270,
        "Title": "RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian",
        "Authors": [
            "Adrian Cosma",
            "Bogdan Iordache",
            "Paolo Rosso"
        ],
        "Abstract": "Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language. Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs. Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models. Through our results and review of related works, we argue for the need to develop code models for languages other than English.",
        "Publication date": "20 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.13222"
    },
    {
        "ID": 271,
        "Title": "Finding Cross-rule Optimization Bugs in Datalog Engines",
        "Authors": [
            "Chi Zhang",
            "Linzhang Wang",
            "Manuel Rigger"
        ],
        "Abstract": "Datalog is a popular and widely-used declarative logic programming language. Datalog engines apply many cross-rule optimizations; bugs in them can cause incorrect results. To detect such optimization bugs, we propose an automated testing approach called Incremental Rule Evaluation (IRE), which synergistically tackles the test oracle and test case generation problem. The core idea behind the test oracle is to compare the results of an optimized program and a program without cross-rule optimization; any difference indicates a bug in the Datalog engine. Our core insight is that, for an optimized, incrementally-generated Datalog program, we can evaluate all rules individually by constructing a reference program to disable the optimizations that are performed among multiple rules. Incrementally generating test cases not only allows us to apply the test oracle for every new rule generated-we also can ensure that every newly added rule generates a non-empty result with a given probability and eschew recomputing already-known facts. We implemented IRE as a tool named Deopt, and evaluated Deopt on four mature Datalog engines, namely SoufflÃ©, CozoDB, $Î¼$Z, and DDlog, and discovered a total of 30 bugs. Of these, 13 were logic bugs, while the remaining were crash and error bugs. Deopt can detect all bugs found by queryFuzz, a state-of-the-art approach. Out of the bugs identified by Deopt, queryFuzz might be unable to detect 5. Our incremental test case generation approach is efficient; for example, for test cases containing 60 rules, our incremental approach can produce 1.17$\\times$ (for DDlog) to 31.02$\\times$ (for SoufflÃ©) as many valid test cases with non-empty results as the naive random method. We believe that the simplicity and the generality of the approach will lead to its wide adoption in practice.",
        "Publication date": "20 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.12863"
    },
    {
        "ID": 272,
        "Title": "ARKS: Active Retrieval in Knowledge Soup for Code Generation",
        "Authors": [
            "Hongjin Su",
            "Shuyang Jiang",
            "Yuhang Lai",
            "Haoyuan Wu",
            "Boao Shi",
            "Che Liu",
            "Qian Liu",
            "Tao Yu"
        ],
        "Abstract": "Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama demonstrate a substantial improvement in the average execution accuracy of ARKS on LLMs. The analysis confirms the effectiveness of our proposed knowledge soup and active retrieval strategies, offering rich insights into the construction of effective retrieval-augmented code generation (RACG) pipelines. Our model, code, and data are available at https://arks-codegen.github.io.",
        "Publication date": "19 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.12317"
    },
    {
        "ID": 273,
        "Title": "Solving Data-centric Tasks using Large Language Models",
        "Authors": [
            "Shraddha Barke",
            "Christian Poelitz",
            "Carina Suzana Negreanu",
            "Benjamin Zorn",
            "JosÃ© Cambronero",
            "Andrew D. Gordon",
            "Vu Le",
            "Elnaz Nouri",
            "Nadia Polikarpova",
            "Advait Sarkar",
            "Brian Slininger",
            "Neil Toronto",
            "Jack Williams"
        ],
        "Abstract": "Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.",
        "Publication date": "24 March, 2024",
        "Link": "https://arxiv.org/pdf/2402.11734"
    },
    {
        "ID": 274,
        "Title": "When Dataflow Analysis Meets Large Language Models",
        "Authors": [
            "Chengpeng Wang",
            "Wuqi Zhang",
            "Zian Su",
            "Xiangzhe Xu",
            "Xiaoheng Xie",
            "Xiangyu Zhang"
        ],
        "Abstract": "Dataflow analysis is a powerful code analysis technique that reasons dependencies between program values, offering support for code optimization, program comprehension, and bug detection. Existing approaches require the successful compilation of the subject program and customizations for downstream applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis framework that analyzes arbitrary code snippets without requiring a compilation infrastructure and automatically synthesizes downstream applications. Inspired by summary-based dataflow analysis, LLMDFA decomposes the problem into three sub-problems, which are effectively resolved by several essential strategies, including few-shot chain-of-thought prompting and tool synthesis. Our evaluation has shown that the design can mitigate the hallucination and improve the reasoning ability, obtaining high precision and recall in detecting dataflow-related bugs upon benchmark programs, outperforming state-of-the-art (classic) tools, including a very recent industrial analyzer.",
        "Publication date": "16 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.10754"
    },
    {
        "ID": 275,
        "Title": "CodeMind: A Framework to Challenge Large Language Models for Code Reasoning",
        "Authors": [
            "Changshu Liu",
            "Shizhuo Dylan Zhang",
            "Ali Reza Ibrahimzada",
            "Reyhaneh Jabbarvand"
        ],
        "Abstract": "Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior.\n  Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly follow control flow constructs and, in general, explain how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize. However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging): ranking LLMs based on test passing can be different compared to code reasoning.",
        "Publication date": "3 April, 2024",
        "Link": "https://arxiv.org/pdf/2402.09664"
    },
    {
        "ID": 276,
        "Title": "Enhancing Source Code Representations for Deep Learning with Static Analysis",
        "Authors": [
            "Xueting Guan",
            "Christoph Treude"
        ],
        "Abstract": "Deep learning techniques applied to program analysis tasks such as code classification, summarization, and bug detection have seen widespread interest. Traditional approaches, however, treat programming source code as natural language text, which may neglect significant structural or semantic details. Additionally, most current methods of representing source code focus solely on the code, without considering beneficial additional context. This paper explores the integration of static analysis and additional context such as bug reports and design patterns into source code representations for deep learning models. We use the Abstract Syntax Tree-based Neural Network (ASTNN) method and augment it with additional context information obtained from bug reports and design patterns, creating an enriched source code representation that significantly enhances the performance of common software engineering tasks such as code classification and code clone detection. Utilizing existing open-source code data, our approach improves the representation and processing of source code, thereby improving task performance.",
        "Publication date": "14 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.09557"
    },
    {
        "ID": 277,
        "Title": "MPIrigen: MPI Code Generation through Domain-Specific Language Models",
        "Authors": [
            "Nadav Schneider",
            "Niranjan Hasabnis",
            "Vy A. Vo",
            "Tal Kadosh",
            "Neva Krien",
            "Mihai CapotÄ",
            "Guy Tamir",
            "Ted Willke",
            "Nesreen Ahmed",
            "Yuval Pinter",
            "Timothy Mattson",
            "Gal Oren"
        ],
        "Abstract": "The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose an innovative preprocessing for completion only after observing the whole code, thus enabling better completion with a wider context. Comparative analysis against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions up to 0.8 accuracy in location and function predictions, and with more than 0.9 accuracy for argument predictions. The success of this tailored solution underscores the importance of domain-specific fine-tuning in optimizing language models for parallel computing code generation, paving the way for a new generation of automatic parallelization tools. The sources of this work are available at our GitHub MPIrigen repository: https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen",
        "Publication date": "23 April, 2024",
        "Link": "https://arxiv.org/pdf/2402.09126"
    },
    {
        "ID": 278,
        "Title": "The Vienna Architecture Description Language",
        "Authors": [
            "Simon Himmelbauer",
            "Christoph Hochrainer",
            "Benedikt Huber",
            "Niklas Mischkulnig",
            "Philipp Paulweber",
            "Tobias Schwarzinger",
            "Andreas Krall"
        ],
        "Abstract": "The Vienna Architecture Description Language (VADL) is a powerful processor description language (PDL) that enables the concise formal specification of processor architectures. By utilizing a single VADL processor specification, the VADL system exhibits the capability to automatically generate a range of artifacts necessary for rapid design space exploration. These include assemblers, compilers, linkers, functional instruction set simulators, cycle-accurate instruction set simulators, synthesizable specifications in a hardware description language, as well as test cases and documentation. One distinctive feature of VADL lies in its separation of the instruction set architecture (ISA) specification and the microarchitecture (MiA) specification. This segregation allows users the flexibility to combine various ISAs with different MiAs, providing a versatile approach to processor design. In contrast to existing PDLs, VADL's MiA specification operates at a higher level of abstraction, enhancing the clarity and simplicity of the design process. Notably, with a single ISA specification, VADL streamlines compiler generation and maintenance by eliminating the need for intricate compiler-specific knowledge. This article introduces VADL, describes the generator techniques in detail and demonstrates the power of the language and the performance of the generators in an empirical evaluation. The evaluation shows the expressiveness and conciseness of VADL and the efficiency of the generated artifacts.",
        "Publication date": "14 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.09087"
    },
    {
        "ID": 279,
        "Title": "On-the-Fly Syntax Highlighting: Generalisation and Speed-ups",
        "Authors": [
            "Marco Edoardo Palma",
            "Alex Wolf",
            "Pasquale Salza",
            "Harald C. Gall"
        ],
        "Abstract": "On-the-fly syntax highlighting is the task of rapidly associating visual secondary notation values with each character of a language derivation. Research in this domain is driven by the prevalence of online software development tools, which frequently display source code on screen and heavily rely on syntax highlighting mechanisms. In this context, three contrasting demands confront resolvers in this space: speed, accuracy, and development costs. Speed constraints are essential to ensure tool usability, manifesting as responsiveness for end users accessing online source code and minimising system overhead. Simultaneously, achieving precise highlighting is critical for enhancing code comprehensibility. Nevertheless, obtaining accurate results necessitates the capacity to perform grammatical analysis on the code under consideration, even in cases of varying grammatical correctness. Furthermore, addressing the development costs of such resolvers is imperative, given the multitude of programming language versions. The current state-of-the-art approach in this field leverages the original lexer and parser of programming languages to create syntax highlighting oracles, subsequently used for training base Recurrent Neural Network models. As the question of the generalisation of such a solution persists, this paper addresses this aspect by extending the original work to three additional mainstream programming languages and conducting a comprehensive review of the outcomes. Moreover, the original limitations in evaluation performance and training costs are mitigated through the introduction of a novel Convolutional based Neural Network model. This study examines the performance gains of running models on GPUs, finding that the new CNN implementation is much faster than previous methods while maintaining high accuracy.",
        "Publication date": "13 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.08754"
    },
    {
        "ID": 280,
        "Title": "VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search",
        "Authors": [
            "David Brandfonbrener",
            "Simon Henniger",
            "Sibi Raja",
            "Tarun Prasad",
            "Chloe Loughridge",
            "Federico Cassano",
            "Sabrina Ruixin Hu",
            "Jianang Yang",
            "William E. Byrd",
            "Robert Zinkov",
            "Nada Amin"
        ],
        "Abstract": "Large Language Models (LLMs) can generate useful code, but often the code they generate cannot be trusted to be sound. In this paper, we present VerMCTS, an approach to begin to resolve this issue by generating verified programs in Dafny and Coq. VerMCTS uses a logical verifier in concert with an LLM to guide a modified Monte Carlo Tree Search (MCTS). This approach leverages the verifier to gain intermediate feedback inside the search algorithm by checking partial programs at each step to estimate an upper bound on the value function. To measure the performance of VerMCTS, we develop a new suite of multi-step verified programming problems in Dafny and Coq. In terms of pass@T, a new metric which computes the pass rate given a budget of T tokens sampled from the LLM, VerMCTS leads to more than a 30% absolute increase in average pass@5000 across the suite over repeated sampling from the base language model. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search .",
        "Publication date": "24 May, 2024",
        "Link": "https://arxiv.org/pdf/2402.08147"
    },
    {
        "ID": 281,
        "Title": "TIC: Translate-Infer-Compile for accurate \"text to plan\" using LLMs and Logical Representations",
        "Authors": [
            "Sudhir Agarwal",
            "Anu Sreepathy"
        ],
        "Abstract": "We study the problem of generating plans for given natural language planning task requests. On one hand, LLMs excel at natural language processing but do not perform well on planning. On the other hand, classical planning tools excel at planning tasks but require input in a structured language such as the Planning Domain Definition Language (PDDL). We leverage the strengths of both the techniques by using an LLM for generating the PDDL representation (task PDDL) of planning task requests followed by using a classical planner for computing a plan. Unlike previous approaches that use LLMs for generating task PDDLs directly, our approach comprises of (a) translate: using an LLM only for generating a logically interpretable intermediate representation of natural language task description, (b) infer: deriving additional logically dependent information from the intermediate representation using a logic reasoner (currently, Answer Set Programming solver), and (c) compile: generating the target task PDDL from the base and inferred information. We observe that using an LLM to only output the intermediate representation significantly reduces LLM errors. Consequently, TIC approach achieves, for at least one LLM, high accuracy on task PDDL generation for all seven domains of our evaluation dataset.",
        "Publication date": "28 June, 2024",
        "Link": "https://arxiv.org/pdf/2402.06608"
    },
    {
        "ID": 282,
        "Title": "Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study",
        "Authors": [
            "Yufei Li",
            "Simin Chen",
            "Yanghong Guo",
            "Wei Yang",
            "Yue Dong",
            "Cong Liu"
        ],
        "Abstract": "Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e.g., calibration error vs misclassification detection) and trade-off between efficacy and efficiency, highlighting necessary methodological selection tailored to specific contexts.",
        "Publication date": "11 January, 2024",
        "Link": "https://arxiv.org/pdf/2402.05939"
    },
    {
        "ID": 283,
        "Title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
        "Authors": [
            "Jordan Juravsky",
            "Bradley Brown",
            "Ryan Ehrlich",
            "Daniel Y. Fu",
            "Christopher RÃ©",
            "Azalia Mirhoseini"
        ],
        "Abstract": "Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end CodeLlama-13b throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix length. Hydragen also enables the use of very long shared contexts: with a large batch size, increasing the prefix length from 1K to 16K tokens decreases Hydragen throughput by less than 15%, while the throughput of baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based prompt sharing patterns, allowing us to further reduce inference time on competitive programming problems by 55%.",
        "Publication date": "13 May, 2024",
        "Link": "https://arxiv.org/pdf/2402.05099"
    },
    {
        "ID": 284,
        "Title": "Enhancing User Interaction in ChatGPT: Characterizing and Consolidating Multiple Prompts for Issue Resolution",
        "Authors": [
            "Saikat Mondal",
            "Suborno Deb Bappon",
            "Chanchal K. Roy"
        ],
        "Abstract": "Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the model's ability to extract contextually accurate responses. Thus, optimal prompt construction is essential for maximizing the utility and performance of ChatGPT. However, sub-optimal prompt design may necessitate iterative refinement, as imprecise or ambiguous instructions can lead to undesired responses from ChatGPT. Existing studies explore several prompt patterns and strategies to improve the relevance of responses generated by ChatGPT. However, the exploration of constraints that necessitate the submission of multiple prompts is still an unmet attempt. In this study, our contributions are twofold. First, we attempt to uncover gaps in prompt design that demand multiple iterations. In particular, we manually analyze 686 prompts that were submitted to resolve issues related to Java and Python programming languages and identify eleven prompt design gaps (e.g., missing specifications). Such gap exploration can enhance the efficacy of single prompts in ChatGPT. Second, we attempt to reproduce the ChatGPT response by consolidating multiple prompts into a single one. We can completely consolidate prompts with four gaps (e.g., missing context) and partially consolidate prompts with three gaps (e.g., additional functionality). Such an effort provides concrete evidence to users to design more optimal prompts mitigating these gaps. Our study findings and evidence can - (a) save users time, (b) reduce costs, and (c) increase user satisfaction.",
        "Publication date": "6 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.04568"
    },
    {
        "ID": 285,
        "Title": "SeMalloc: Semantics-Informed Memory Allocator",
        "Authors": [
            "Ruizhe Wang",
            "Meng Xu",
            "N. Asokan"
        ],
        "Abstract": "Use-after-free (UAF) is a critical and prevalent problem in memory unsafe languages. While many solutions have been proposed, balancing security, run-time cost, and memory overhead (an impossible trinity) is hard.\n  In this paper, we show one way to balance the trinity by passing more semantics about the heap object to the allocator for it to make informed allocation decisions. More specifically, we propose a new notion of thread-, context-, and flow-sensitive \"type\", SemaType, to capture the semantics and prototype a SemaType-based allocator that aims for the best trade-off amongst the impossible trinity. In SeMalloc, only heap objects allocated from the same call site and via the same function call stack can possibly share a virtual memory address, which effectively stops type-confusion attacks and makes UAF vulnerabilities harder to exploit.\n  Through extensive empirical evaluation, we show that SeMalloc is realistic: (a) SeMalloc is effective in thwarting all real-world vulnerabilities we tested; (b) benchmark programs run even slightly faster with SeMalloc than the default heap allocator, at a memory overhead averaged from 41% to 84%; and (c) SeMalloc balances security and overhead strictly better than other closely related works.",
        "Publication date": "22 May, 2024",
        "Link": "https://arxiv.org/pdf/2402.03373"
    },
    {
        "ID": 286,
        "Title": "Charting The Evolution of Solidity Error Handling",
        "Authors": [
            "Charalambos Mitropoulos",
            "Maria Kechagia",
            "Chrysostomos Maschas",
            "Sotiris Ioannidis",
            "Federica Sarro",
            "Dimitris Mitropoulos"
        ],
        "Abstract": "The usage of error handling in Solidity smart contracts is vital because smart contracts perform transactions that should be verified. Transactions that are not carefully handled, may lead to program crashes and vulnerabilities, implying financial loss and legal consequences. While Solidity designers attempt to constantly update the language with new features, including error-handling (EH) features, it is necessary for developers to promptly absorb how to use them. We conduct a large-scale empirical study on 283K unique open-source smart contracts to identify patterns regarding the usage of Solidity EH features over time. Overall, the usage of most EH features is limited. However, we observe an upward trend (> 60%) in the usage of a Solidity-tailored EH feature, i.e., require. This indicates that designers of modern programming languages may consider making error handling more tailored to the purposes of each language. Our analysis on 102 versions of the Solidity documentation indicates the volatile nature of Solidity, as the language changes frequently, i.e., there are changes on EH features once or twice a year. Such frequent releases may confuse smart contract developers, discouraging them to carefully read the Solidity documentation, and correctly adopt EH features. Furthermore, our findings reveal that nearly 70% of the examined smart contracts are exposed to potential failures due to missing error handing, e.g., unchecked external calls. Therefore, the use of EH features should be further supported via a more informative documentation containing (1) representative and meaningful examples and (2) details about the impact of potential EH misuses.",
        "Publication date": "5 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.03186"
    },
    {
        "ID": 287,
        "Title": "Object Graph Programming",
        "Authors": [
            "Aditya Thimmaiah",
            "Leonidas Lampropoulos",
            "Christopher J. Rossbach",
            "Milos Gligoric"
        ],
        "Abstract": "We introduce Object Graph Programming (OGO), which enables reading and modifying an object graph (i.e., the entire state of the object heap) via declarative queries. OGO models the objects and their relations in the heap as an object graph thereby treating the heap as a graph database: each node in the graph is an object (e.g., an instance of a class or an instance of a metadata class) and each edge is a relation between objects (e.g., a field of one object references another object). We leverage Cypher, the most popular query language for graph databases, as OGO's query language. Unlike LINQ, which uses collections (e.g., List) as a source of data, OGO views the entire object graph as a single \"collection\". OGO is ideal for querying collections (just like LINQ), introspecting the runtime system state (e.g., finding all instances of a given class or accessing fields via reflection), and writing assertions that have access to the entire program state. We prototyped OGO for Java in two ways: (a) by translating an object graph into a Neo4j database on which we run Cypher queries, and (b) by implementing our own in-memory graph query engine that directly queries the object heap. We used OGO to rewrite hundreds of statements in large open-source projects into OGO queries. We report our experience and performance of our prototypes.",
        "Publication date": "4 February, 2024",
        "Link": "https://arxiv.org/pdf/2402.02642"
    },
    {
        "ID": 288,
        "Title": "Bloom-epistemic and sentiment analysis hierarchical classification in course discussion forums",
        "Authors": [
            "H. Toba",
            "Y. T. Hernita",
            "M. Ayub",
            "M. C. Wijanto"
        ],
        "Abstract": "Online discussion forums are widely used for active textual interaction between lecturers and students, and to see how the students have progressed in a learning process. The objective of this study is to compare appropriate machine-learning models to assess sentiments and BloomÅ epistemic taxonomy based on textual comments in educational discussion forums. Our proposed method is called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis (BE-Sent). The research methodology consists of three main steps. The first step is the data collection from the internal discussion forum and YouTube comments of a Web Programming channel. The next step is text preprocessing to annotate the text and clear unimportant words. Furthermore, with the text dataset that has been successfully cleaned, sentiment analysis and epistemic categorization will be done in each sentence of the text. Sentiment analysis is divided into three categories: positive, negative, and neutral. BloomÅ epistemic is divided into six categories: remembering, understanding, applying, analyzing, evaluating, and creating. This research has succeeded in producing a course learning subsystem that assesses opinions based on text reviews of discussion forums according to the category of sentiment and epistemic analysis.",
        "Publication date": "26 January, 2024",
        "Link": "https://arxiv.org/pdf/2402.01716"
    },
    {
        "ID": 289,
        "Title": "SymbolicAI: A framework for logic-based approaches combining generative models and solvers",
        "Authors": [
            "Marius-Constantin Dinu",
            "Claudiu Leoveanu-Condrei",
            "Markus Holzleitner",
            "Werner Zellinger",
            "Sepp Hochreiter"
        ],
        "Abstract": "We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for multi-modal data that connects multi-step generative processes and aligns their outputs with user objectives in complex workflows. As a result, we can transition between the capabilities of various foundation models with in-context learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. Through these operations based on in-context learning our framework enables the creation and evaluation of explainable computational graphs. Finally, we introduce a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short. The framework codebase and benchmark are linked below.",
        "Publication date": "21 August, 2024",
        "Link": "https://arxiv.org/pdf/2402.00854"
    },
    {
        "ID": 290,
        "Title": "Towards AI-Assisted Synthesis of Verified Dafny Methods",
        "Authors": [
            "Md Rakib Hossain Misu",
            "Cristina V. Lopes",
            "Iris Ma",
            "James Noble"
        ],
        "Abstract": "Large language models show great promise in many domains, including programming. A promise is easy to make but hard to keep, and language models often fail to keep their promises, generating erroneous code. A promising avenue to keep models honest is to incorporate formal verification: generating programs' specifications as well as code so that the code can be proved correct with respect to the specifications. Unfortunately, existing large language models show a severe lack of proficiency in verified programming.\n  In this paper, we demonstrate how to improve two pretrained models' proficiency in the Dafny verification-aware language. Using 178 problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to synthesize Dafny methods. We use three different types of prompts: a direct Contextless prompt; a Signature prompt that includes a method signature and test cases, and a Chain of Thought (CoT) prompt that decomposes the problem into steps and includes retrieval augmentation generated example problems and solutions. Our results show that GPT-4 performs better than PaLM-2 on these tasks and that both models perform best with the retrieval augmentation generated CoT prompt. GPT-4 was able to generate verified, human-evaluated, Dafny methods for 58% of the problems, however, GPT-4 managed only 19% of the problems with the Contextless prompt, and even fewer (10%) for the Signature prompt. We are thus able to contribute 153 verified Dafny solutions to MBPP problems, 50 that we wrote manually, and 103 synthesized by GPT-4.\n  Our results demonstrate that the benefits of formal program verification are now within reach of code generating large language models...",
        "Publication date": "10 June, 2024",
        "Link": "https://arxiv.org/pdf/2402.00247"
    },
    {
        "ID": 291,
        "Title": "CONCORD: Towards a DSL for Configurable Graph Code Representation",
        "Authors": [
            "Mootez Saad",
            "Tushar Sharma"
        ],
        "Abstract": "Deep learning is widely used to uncover hidden patterns in large code corpora. To achieve this, constructing a format that captures the relevant characteristics and features of source code is essential. Graph-based representations have gained attention for their ability to model structural and semantic information. However, existing tools lack flexibility in constructing graphs across different programming languages, limiting their use. Additionally, the output of these tools often lacks interoperability and results in excessively large graphs, making graph-based neural networks training slower and less scalable.\n  We introduce CONCORD, a domain-specific language to build customizable graph representations. It implements reduction heuristics to reduce graphs' size complexity. We demonstrate its effectiveness in code smell detection as an illustrative use case and show that: first, CONCORD can produce code representations automatically per the specified configuration, and second, our heuristics can achieve comparable performance with significantly reduced size. CONCORD will help researchers a) create and experiment with customizable graph-based code representations for different software engineering tasks involving DL, b) reduce the engineering work to generate graph representations, c) address the issue of scalability in GNN models, and d) enhance the reproducibility of experiments in research through a standardized approach to code representation and analysis.",
        "Publication date": "31 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.17967"
    },
    {
        "ID": 292,
        "Title": "IRCoCo: Immediate Rewards-Guided Deep Reinforcement Learning for Code Completion",
        "Authors": [
            "Bolun Li",
            "Zhihong Sun",
            "Tao Huang",
            "Hongyu Zhang",
            "Yao Wan",
            "Ge Li",
            "Zhi Jin",
            "Chen Lyu"
        ],
        "Abstract": "Code completion aims to enhance programming productivity by predicting potential code based on the current programming context. Recently, pretrained language models (LMs) have become prominent in this field. Various approaches have been proposed to fine-tune LMs using supervised fine-tuning (SFT) techniques for code completion. However, the inherent exposure bias of these models can cause errors to accumulate early in the sequence completion, leading to even more errors in subsequent completions. To address this problem, deep reinforcement learning (DRL) is an alternative technique for fine-tuning LMs for code completion, which can improve the generalization capabilities and overall performance. Nevertheless, integrating DRL-based strategies into code completion faces two major challenges: 1) The dynamic nature of the code context requires the completion model to quickly adapt to changes, which poses difficulties for conventional DRL strategies that focus on delayed rewarding of the final code state. 2) It is difficult to evaluate the correctness of partial code, thus the reward redistribution-based strategies cannot be adapted to code completion. To tackle these challenges, we propose IRCoCo, a code completion-specific DRL-based fine-tuning framework. This framework is designed to provide immediate rewards as feedback for detecting dynamic context changes arising from continuous edits during code completion. With the aid of immediate feedback, the fine-tuned LM can gain a more precise understanding of the current context, thereby enabling effective adjustment of the LM and optimizing code completion in a more refined manner. Experimental results demonstrate that fine-tuning pretrained LMs with IRCoCo leads to significant improvements in the code completion task, outperforming both SFT-based and other DRL-based baselines.",
        "Publication date": "21 February, 2024",
        "Link": "https://arxiv.org/pdf/2401.16637"
    },
    {
        "ID": 293,
        "Title": "SECOMP: Formally Secure Compilation of Compartmentalized C Programs",
        "Authors": [
            "JÃ©rÃ©my Thibault",
            "Roberto Blanco",
            "Dongjae Lee",
            "Sven Argo",
            "Arthur Azevedo de Amorim",
            "AÃ¯na Linn Georges",
            "Catalin Hritcu",
            "Andrew Tolmach"
        ],
        "Abstract": "Undefined behavior in C often causes devastating security vulnerabilities. One practical mitigation is compartmentalization, which allows developers to structure large programs into mutually distrustful compartments with clearly specified privileges and interactions. In this paper we introduce SECOMP, a compiler for compartmentalized C code that comes with machine-checked proofs guaranteeing that the scope of undefined behavior is restricted to the compartments that encounter it and become dynamically compromised. These guarantees are formalized as the preservation of safety properties against adversarial contexts, a secure compilation criterion similar to full abstraction, and this is the first time such a strong criterion is proven for a mainstream programming language. To achieve this we extend the languages of the CompCert verified C compiler with isolated compartments that can only interact via procedure calls and returns, as specified by cross-compartment interfaces. We adapt the passes and optimizations of CompCert as well as their correctness proofs to this compartment-aware setting. We then use compiler correctness as an ingredient in a larger secure compilation proof that involves several proof engineering novelties, needed to scale formally secure compilation up to a C compiler.",
        "Publication date": "1 July, 2024",
        "Link": "https://arxiv.org/pdf/2401.16277"
    },
    {
        "ID": 294,
        "Title": "Knowledge-Aware Code Generation with Large Language Models",
        "Authors": [
            "Tao Huang",
            "Zhihong Sun",
            "Zhi Jin",
            "Ge Li",
            "Chen Lyu"
        ],
        "Abstract": "Large Language Models (LLMs) perform well on basic programming problems. However, they encounter challenges when dealing with complex tasks involving the use of diverse algorithmic and data structure skills, particularly programming competition-level problems. Notably, ChatGPT exhibits proficient performance on problems it has encountered during its pre-training phase, but this performance deteriorates when faced with novel problems. Consequently, enhancing the ability of LLMs to address unfamiliar problems has emerged as a pivotal research focus. The problem-solving process of LLMs mirrors human programmers' approach to a certain extent. When confronted with new programming tasks, human programmers engage in task planning and code writing with the previously acquired knowledge about algorithms and data structures. Despite having learned such knowledge, LLMs struggle to effectively apply it when faced with specific new problems. To address this issue, we constructed a novel dataset, CodeF, which contains a portion of programming problems that ChatGPT has not previously encountered. Furthermore, we developed a Knowledge Library tailored for Python programming contest problems and introduced the concept of Knowledge-Aware Code Generation (KareCoder). KareCoder bolsters the models' understanding and problem-solving capabilities by integrating prompt and knowledge from the library into the LLMs' code generation reasoning process, especially on Pass@1 metrics. Upon testing on the CodeF and APPS datasets, KareCoder demonstrated outstanding performance in handling novel problems previously unencountered by LLMs. In contrast with the code directly generated by ChatGPT, KareCoder achieved a relative improvement of 23.3% on the Pass@1 metric on the CodeF post2021-9 dataset. Additionally, it performs well compared to other methods when dealing with problems that LLMs have previously encountered.",
        "Publication date": "1 February, 2024",
        "Link": "https://arxiv.org/pdf/2401.15940"
    },
    {
        "ID": 295,
        "Title": "APIGen: Generative API Method Recommendation",
        "Authors": [
            "Yujia Chen",
            "Cuiyun Gao",
            "Muyijie Zhu",
            "Qing Liao",
            "Yong Wang",
            "Guoai Xu"
        ],
        "Abstract": "Automatic API method recommendation is an essential task of code intelligence, which aims to suggest suitable APIs for programming queries. Existing approaches can be categorized into two primary groups: retrieval-based and learning-based approaches. Although these approaches have achieved remarkable success, they still come with notable limitations. The retrieval-based approaches rely on the text representation capabilities of embedding models, while the learning-based approaches require extensive task-specific labeled data for training. To mitigate the limitations, we propose APIGen, a generative API recommendation approach through enhanced in-context learning (ICL). APIGen involves two main components: (1) Diverse Examples Selection. APIGen searches for similar posts to the programming queries from the lexical, syntactical, and semantic perspectives, providing more informative examples for ICL. (2) Guided API Recommendation. APIGen enables large language models (LLMs) to perform reasoning before generating API recommendations, where the reasoning involves fine-grained matching between the task intent behind the queries and the factual knowledge of the APIs. With the reasoning process, APIGen makes recommended APIs better meet the programming requirement of queries and also enhances the interpretability of results. We compare APIGen with four existing approaches on two publicly available benchmarks. Experiments show that APIGen outperforms the best baseline CLEAR by 105.8% in method-level API recommendation and 54.3% in class-level API recommendation in terms of SuccessRate@1. Besides, APIGen achieves an average 49.87% increase compared to the zero-shot performance of popular LLMs such as GPT-4 in method-level API recommendation regarding the SuccessRate@3 metric.",
        "Publication date": "28 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.15843"
    },
    {
        "ID": 296,
        "Title": "PPM: Automated Generation of Diverse Programming Problems for Benchmarking Code Generation Models",
        "Authors": [
            "Simin Chen",
            "Xiaoning Feng",
            "Xiaohong Han",
            "Cong Liu",
            "Wei Yang"
        ],
        "Abstract": "In recent times, a plethora of Large Code Generation Models (LCGMs) have been proposed, showcasing significant potential in assisting developers with complex programming tasks. Benchmarking LCGMs necessitates the creation of a set of diverse programming problems, and each problem comprises the prompt (including the task description), canonical solution, and test inputs. The existing methods for constructing such a problem set can be categorized into two main types: manual methods and perturbation-based methods. However, manual methods demand high effort and lack scalability, while also risking data integrity due to LCGMs' potentially contaminated data collection, and perturbation-based approaches mainly generate semantically homogeneous problems with the same canonical solutions and introduce typos that can be easily auto-corrected by IDE, making them ineffective and unrealistic. In this work, we propose the idea of programming problem merging (PPM) and provide two implementation of this idea, we utilize our tool on two widely-used datasets and compare it against nine baseline methods using eight code generation models. The results demonstrate the effectiveness of our tool in generating more challenging, diverse, and natural programming problems, comparing to the baselines.",
        "Publication date": "27 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.15545"
    },
    {
        "ID": 297,
        "Title": "Bridging Education and Development: IDEs as Interactive Learning Platforms",
        "Authors": [
            "Anastasiia Birillo",
            "Maria Tigina",
            "Zarina Kurbatova",
            "Anna Potriasaeva",
            "Ilya Vlasov",
            "Valerii Ovchinnikov",
            "Igor Gerasimov"
        ],
        "Abstract": "In this work, we introduce a novel approach to programming education - in-IDE courses implemented for IntelliJ-based IDEs via the JetBrains Academy Plugin. The primary objective of this approach is to address the challenge of familiarizing students with industrial technologies by moving all theory and practical materials to a professional IDE. This approach allows students to immediately use modern industrial tools as they are fully integrated into the learning process. We have already applied this approach in over 40 courses, and it successfully educates students across diverse topics such as Plugin Development, Algorithms, Data Analysis, and Language mastery in various programming languages, including Kotlin, Java, C++, and Python. Along with the paper, we are providing the community not only with a new way of learning and a set of ready-made courses but also a collection of helpful resources to assist educators in getting started with the plugin. Finally, we describe in detail an IDE plugin development course that demonstrates how the in-IDE approach covers complex topics easily.",
        "Publication date": "25 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.14284"
    },
    {
        "ID": 298,
        "Title": "libcdict: fast dictionaries in C",
        "Authors": [
            "Robert G. Izzard",
            "David D. Hendriks",
            "Daniel P. Nemergut"
        ],
        "Abstract": "A common requirement in science is to store and share large sets of simulation data in an efficient, nested, flexible and human-readable way. Such datasets contain number counts and distributions, i.e. histograms and maps, of arbitrary dimension and variable type, e.g. floating-point number, integer or character string. Modern high-level programming languages like Perl and Python have associated arrays, knowns as dictionaries or hashes, respectively, to fulfil this storage need. Low-level languages used more commonly for fast computational simulations, such as C and Fortran, lack this functionality. We present libcdict, a C dictionary library, to solve this problem. Libcdict provides C and Fortran application programming interfaces (APIs) to native dictionaries, called cdicts, and functions for cdicts to load and save these as JSON and hence for easy interpretation in other software and languages like Perl, Python and R.",
        "Publication date": "25 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.14272"
    },
    {
        "ID": 299,
        "Title": "Application of performance portability solutions for GPUs and many-core CPUs to track reconstruction kernels",
        "Authors": [
            "Ka Hei Martin Kwok",
            "Matti Kortelainen",
            "Giuseppe Cerati",
            "Alexei Strelchenko",
            "Oliver Gutsche",
            "Allison Reinsvold Hall",
            "Steve Lantz",
            "Michael Reid",
            "Daniel Riley",
            "Sophie Berkman",
            "Seyong Lee",
            "Hammad Ather",
            "Boyana Norris",
            "Cong Wang"
        ],
        "Abstract": "Next generation High-Energy Physics (HEP) experiments are presented with significant computational challenges, both in terms of data volume and processing power. Using compute accelerators, such as GPUs, is one of the promising ways to provide the necessary computational power to meet the challenge. The current programming models for compute accelerators often involve using architecture-specific programming languages promoted by the hardware vendors and hence limit the set of platforms that the code can run on. Developing software with platform restrictions is especially unfeasible for HEP communities as it takes significant effort to convert typical HEP algorithms into ones that are efficient for compute accelerators. Multiple performance portability solutions have recently emerged and provide an alternative path for using compute accelerators, which allow the code to be executed on hardware from different vendors. We apply several portability solutions, such as Kokkos, SYCL, C++17 std::execution::par and Alpaka, on two mini-apps extracted from the mkFit project: p2z and p2r. These apps include basic kernels for a Kalman filter track fit, such as propagation and update of track parameters, for detectors at a fixed z or fixed r position, respectively. The two mini-apps explore different memory layout formats.\n  We report on the development experience with different portability solutions, as well as their performance on GPUs and many-core CPUs, measured as the throughput of the kernels from different GPU and CPU vendors such as NVIDIA, AMD and Intel.",
        "Publication date": "25 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.14221"
    },
    {
        "ID": 300,
        "Title": "Developing a High-Performance Process Mining Library with Java and Python Bindings in Rust",
        "Authors": [
            "Aaron KÃ¼sters",
            "Wil M. P. van der Aalst"
        ],
        "Abstract": "The most commonly used open-source process mining software tools today are ProM and PM4Py, written in Java and Python, respectively. Such high-level, often interpreted, programming languages trade off performance with memory safety and ease-of-use. In contrast, traditional compiled languages, like C or C++, can achieve top performance but often suffer from instability related to unsafe memory management. Lately, Rust emerged as a highly performant, compiled programming language with inherent memory safety. In this paper, we describe our approach to developing a shared process mining library in Rust with bindings to both Java and Python, allowing full integration into the existing ecosystems, like ProM and PM4Py. By facilitating interoperability, our methodology enables researchers or industry to develop novel algorithms in Rust once and make them accessible to the entire community while also achieving superior performance.",
        "Publication date": "25 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.14149"
    },
    {
        "ID": 301,
        "Title": "Domain-Independent Dynamic Programming",
        "Authors": [
            "Ryo Kuroiwa",
            "J. Christopher Beck"
        ],
        "Abstract": "For combinatorial optimization problems, model-based paradigms such as mixed-integer programming (MIP) and constraint programming (CP) aim to decouple modeling and solving a problem: the `holy grail' of declarative problem solving. We propose domain-independent dynamic programming (DIDP), a new model-based paradigm based on dynamic programming (DP). While DP is not new, it has typically been implemented as a problem-specific method. We introduce Dynamic Programming Description Language (DyPDL), a formalism to define DP models based on a state transition system, inspired by AI planning. We show that heuristic search algorithms can be used to solve DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP solvers with commercial MIP and CP solvers (solving MIP and CP models, respectively) on common benchmark instances of eleven combinatorial optimization problem classes. We show that DIDP outperforms MIP in nine problem classes, CP also in nine problem classes, and both MIP and CP in seven.",
        "Publication date": "31 May, 2024",
        "Link": "https://arxiv.org/pdf/2401.13883"
    },
    {
        "ID": 302,
        "Title": "Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion",
        "Authors": [
            "Dylan Zhang",
            "Curt Tigges",
            "Zory Zhang",
            "Stella Biderman",
            "Maxim Raginsky",
            "Talia Ringer"
        ],
        "Abstract": "This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \\textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \\textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture.\n  With our framework as a powerful conceptual tool, we identify different issues under various set-ups. The models trained to emulate recursive computations cannot fully capture the recursion yet instead fit short-cut algorithms and thus cannot solve certain edge cases that are under-represented in the training distribution. In addition, it is difficult for state-of-the-art large language models (LLMs) to mine recursive rules from in-context demonstrations. Meanwhile, these LLMs fail in interesting ways when emulating reduction (step-wise computation) of the recursive function.",
        "Publication date": "23 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.12947"
    },
    {
        "ID": 303,
        "Title": "In-Context Learning for Extreme Multi-Label Classification",
        "Authors": [
            "Karel D'Oosterlinck",
            "Omar Khattab",
            "FranÃ§ois Remy",
            "Thomas Demeester",
            "Chris Develder",
            "Christopher Potts"
        ],
        "Abstract": "Multi-label classification problems with thousands of classes are hard to solve with in-context learning alone, as language models (LMs) might lack prior knowledge about the precise classes or how to assign them, and it is generally infeasible to demonstrate every class in a prompt. We propose a general program, $\\texttt{Infer--Retrieve--Rank}$, that defines multi-step interactions between LMs and retrievers to efficiently tackle such problems. We implement this program using the $\\texttt{DSPy}$ programming model, which specifies in-context systems in a declarative manner, and use $\\texttt{DSPy}$ optimizers to tune it towards specific datasets by bootstrapping only tens of few-shot examples. Our primary extreme classification program, optimized separately for each task, attains state-of-the-art results across three benchmarks (HOUSE, TECH, TECHWOLF). We apply the same program to a benchmark with vastly different characteristics and attain competitive performance as well (BioDEX). Unlike prior work, our proposed solution requires no finetuning, is easily applicable to new tasks, alleviates prompt engineering, and requires only tens of labeled examples. Our code is public at https://github.com/KarelDO/xmc.dspy.",
        "Publication date": "22 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.12178"
    },
    {
        "ID": 304,
        "Title": "FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis",
        "Authors": [
            "Chao Zhang",
            "Yuren Mao",
            "Yijiang Fan",
            "Yu Mi",
            "Yunjun Gao",
            "Lu Chen",
            "Dongfang Lou",
            "Jinshu Lin"
        ],
        "Abstract": "Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because, financial professionals may not well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, parameter-efficient fine-tuning and output calibration. Extensive experimental results on BULL demonstrate that FinSQL achieves the state-of-the-art Text-to-SQL performance at a small cost; furthermore, FinSQL can bring up to 36.64% performance improvement in scenarios requiring few-shot cross-database model transfer.",
        "Publication date": "19 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.10506"
    },
    {
        "ID": 305,
        "Title": "Semantic Analysis of Macro Usage for Portability",
        "Authors": [
            "Brent Pappas",
            "Paul Gazzillo"
        ],
        "Abstract": "C is an unsafe language. Researchers have been developing tools to port C to safer languages such as Rust, Checked C, or Go. Existing tools, however, resort to preprocessing the source file first, then porting the resulting code, leaving barely recognizable code that loses macro abstractions. To preserve macro usage, porting tools need analyses that understand macro behavior to port to equivalent constructs. But macro semantics differ from typical functions, precluding simple syntactic transformations to port them. We introduce the first comprehensive framework for analyzing the portability of macro usage. We decompose macro behavior into 26 fine-grained properties and implement a program analysis tool, called Maki, that identifies them in real-world code with 94% accuracy. We apply Maki to 21 programs containing a total of 86,199 macro definitions. We found that real-world macros are much more portable than previously known. More than a third (37%) are easy-to-port, and Maki provides hints for porting more complicated macros. We find, on average, 2x more easy-to-port macros and up to 7x more in the best case compared to prior work. Guided by Maki's output, we found and hand-ported macros in four real-world programs. We submitted patches to Linux maintainers that transform eleven macros, nine of which have been accepted.",
        "Publication date": "18 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.10422"
    },
    {
        "ID": 306,
        "Title": "The Mikado Filesystem: An experimental RPC filesystem running over gRPC",
        "Authors": [
            "John D. Dougrez-Lewis"
        ],
        "Abstract": "Computer applications seeking to persist files remotely across the Internet are faced with a bewildering choice of mechanisms which tend to boil down to monolithic proprietary closed-source Vendor solutions. We introduce The Mikado Filesystem (mikfs), which provides an open simple lightweight interoperable portable extensible remote filesystem that is open source. mikfs consists of client applications accessing remote servers via RPC running over TCP/IP connections. mikfs is defined as a concrete set of API method calls over gRPC expressed in Google's Protocol Buffers' IDL. gRPC supports a wide variety of programming languages & platforms. For a given language + platform, the gRPC toolset can generate client- & server-side stubs from the IDL callable from client & server code in the selected languages, e.g., a client written in C# or java running on a Windows PC can access a server written in C++ running on Linux. mikfs consists of a virtual hierarchical tree of files & directories. This logical filesystem is not constrained to the limits and file naming conventions of the host's own physical native filesystem. API methods are provided for authentication; for atomic file-level operations on files & directories; for clients to register to receive notifications of file & directory changes on a server. The public API allows developers to write their own new servers and clients; allowing migration of hosted files between different implementations; extension with new methods & features; is Open Source code available for inspection and adaptation. gRPC provides secure authenticated connection & communication over HTTP/2; End-to-End Privacy & Security against eavesdropping of data in transit; support for multiple alternate user login mechanisms. mikfs is provided as source code, 'The Bootstrap Distribution', consisting of an ecosystem of clients, servers, tools and utilities.",
        "Publication date": "17 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.09186"
    },
    {
        "ID": 307,
        "Title": "Code Simulation Challenges for Large Language Models",
        "Authors": [
            "Emanuele La Malfa",
            "Christoph Weinhuber",
            "Orazio Torre",
            "Fangru Lin",
            "Samuele Marro",
            "Anthony Cohn",
            "Nigel Shadbolt",
            "Michael Wooldridge"
        ],
        "Abstract": "Many reasoning, planning, and problem-solving tasks share an intrinsic algorithmic nature: correctly simulating each step is a sufficient condition to solve them correctly. This work studies to what extent Large Language Models (LLMs) can simulate coding and algorithmic tasks to provide insights into general capabilities in such algorithmic reasoning tasks. We introduce benchmarks for straight-line programs, code that contains critical paths, and approximate and redundant instructions. We further assess the simulation capabilities of LLMs with sorting algorithms and nested loops and show that a routine's computational complexity directly affects an LLM's ability to simulate its execution. While the most powerful LLMs exhibit relatively strong simulation capabilities, the process is fragile, seems to rely heavily on pattern recognition, and is affected by memorisation. We propose a novel off-the-shelf prompting method, Chain of Simulation (CoSm), which instructs LLMs to simulate code execution line by line/follow the computation pattern of compilers. CoSm efficiently helps LLMs reduce memorisation and shallow pattern recognition while improving simulation performance. We consider the success of CoSm in code simulation to be inspirational for other general routine simulation reasoning tasks.",
        "Publication date": "12 June, 2024",
        "Link": "https://arxiv.org/pdf/2401.09074"
    },
    {
        "ID": 308,
        "Title": "HasTEE+ : Confidential Cloud Computing and Analytics with Haskell",
        "Authors": [
            "Abhiroop Sarkar",
            "Alejandro Russo"
        ],
        "Abstract": "Confidential computing is a security paradigm that enables the protection of confidential code and data in a co-tenanted cloud deployment using specialized hardware isolation units called Trusted Execution Environments (TEEs). By integrating TEEs with a Remote Attestation protocol, confidential computing allows a third party to establish the integrity of an \\textit{enclave} hosted within an untrusted cloud. However, TEE solutions, such as Intel SGX and ARM TrustZone, offer low-level C/C++-based toolchains that are susceptible to inherent memory safety vulnerabilities and lack language constructs to monitor explicit and implicit information-flow leaks. Moreover, the toolchains involve complex multi-project hierarchies and the deployment of hand-written attestation protocols for verifying \\textit{enclave} integrity.\n  We address the above with HasTEE+, a domain-specific language (DSL) embedded in Haskell that enables programming TEEs in a high-level language with strong type-safety. HasTEE+ assists in multi-tier cloud application development by (1) introducing a \\textit{tierless} programming model for expressing distributed client-server interactions as a single program, (2) integrating a general remote-attestation architecture that removes the necessity to write application-specific cross-cutting attestation code, and (3) employing a dynamic information flow control mechanism to prevent explicit as well as implicit data leaks. We demonstrate the practicality of HasTEE+ through a case study on confidential data analytics, presenting a data-sharing pattern applicable to mutually distrustful participants and providing overall performance metrics.",
        "Publication date": "16 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.08901"
    },
    {
        "ID": 309,
        "Title": "Towards a Transpiler for C/C++ to Safer Rust",
        "Authors": [
            "Dhiren Tripuramallu",
            "Swapnil Singh",
            "Shrirang Deshmukh",
            "Srinivas Pinisetty",
            "Shinde Arjun Shivaji",
            "Raja Balusamy",
            "Ajaganna Bandeppa"
        ],
        "Abstract": "Rust is a multi-paradigm programming language developed by Mozilla that focuses on performance and safety. Rust code is arguably known best for its speed and memory safety, a property essential while developing embedded systems. Thus, it becomes one of the alternatives when developing operating systems for embedded devices. How to convert an existing C++ code base to Rust is also gaining greater attention. In this work, we focus on the process of transpiling C++ code to a Rust codebase in a robust and safe manner. The manual transpilation process is carried out to understand the different constructs of the Rust language and how they correspond to C++ constructs. Based on the learning from the manual transpilation, a transpilation table is created to aid in future transpilation efforts and to develop an automated transpiler. We also studied the existing automated transpilers and identified the problems and inefficiencies they involved. The results of the transpilation process were closely monitored and evaluated, showing improved memory safety without compromising performance and reliability of the resulting codebase. The study concludes with a comprehensive analysis of the findings, an evaluation of the implications for future research, and recommendations for the same in this area.",
        "Publication date": "16 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.08264"
    },
    {
        "ID": 310,
        "Title": "BUGSPHP: A dataset for Automated Program Repair in PHP",
        "Authors": [
            "K. D. Pramod",
            "W. T. N. De Silva",
            "W. U. K. Thabrew",
            "Ridwan Shariffdeen",
            "Sandareka Wickramanayake"
        ],
        "Abstract": "Automated Program Repair (APR) improves developer productivity by saving debugging and bug-fixing time. While APR has been extensively explored for C/C++ and Java programs, there is little research on bugs in PHP programs due to the lack of a benchmark PHP bug dataset. This is surprising given that PHP has been one of the most widely used server-side languages for over two decades, being used in a variety of contexts such as e-commerce, social networking, and content management. This paper presents a benchmark dataset of PHP bugs on real-world applications called BUGSPHP, which can enable research on analysis, testing, and repair for PHP programs. The dataset consists of training and test datasets, separately curated from GitHub and processed locally. The training dataset includes more than 600,000 bug-fixing commits. The test dataset contains 513 manually validated bug-fixing commits equipped with developer-provided test cases to assess patch correctness.",
        "Publication date": "21 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.07356"
    },
    {
        "ID": 311,
        "Title": "Unoccupied space and short-range order characterization in polymers under heat treatment",
        "Authors": [
            "Hossein Goodarzi-Hosseinabadi"
        ],
        "Abstract": "Large scale molecular dynamics simulations on polyvinyl alcohol were used to investigate the distribution of unoccupied space under different heat treatments. Representative volume elements consisting of 3600 chains of 300 monomers were equilibrated at melt state and cooled by different cooling rates. The positions of center of mass of the monomers were extracted and a series of spatial analysis were conducted to estimate the distribution of free volume or unoccupied space by Voronoi tessellation algorithm. An open-source software was employed which incorporates both compilers of Matlab and C programing languages to reduce the computational costs associated with Voronoi calculations and statistical analysis. The results confirmed that low free volume content is achievable through annealing while high free volume content is achievable through quenching samples at high cooling rates. An appreciable degree of short-range order in the packing of chains is revealed in annealed samples.",
        "Publication date": "18 December, 2023",
        "Link": "https://arxiv.org/pdf/2401.06776"
    },
    {
        "ID": 312,
        "Title": "Evolutionary Generative Fuzzing for Differential Testing of the Kotlin Compiler",
        "Authors": [
            "Calin Georgescu",
            "Mitchell Olsthoorn",
            "Pouria Derakhshanfar",
            "Marat Akhin",
            "Annibale Panichella"
        ],
        "Abstract": "Compiler correctness is a cornerstone of reliable software development. However, systematic testing of compilers is infeasible, given the vast space of possible programs and the complexity of modern programming languages. In this context, differential testing offers a practical methodology as it addresses the oracle problem by comparing the output of alternative compilers given the same set of programs as input. In this paper, we investigate the effectiveness of differential testing in finding bugs within the Kotlin compilers developed at JetBrains. We propose a black-box generative approach that creates input programs for the K1 and K2 compilers. First, we build workable models of Kotlin semantic (semantic interface) and syntactic (enriched context-free grammar) language features, which are subsequently exploited to generate random code snippets. Second, we extend random sampling by introducing two genetic algorithms (GAs) that aim to generate more diverse input programs. Our case study shows that the proposed approach effectively detects bugs in K1 and K2; these bugs have been confirmed and (some) fixed by JetBrains developers. While we do not observe a significant difference w.r.t. the number of defects uncovered by the different search algorithms, random search and GAs are complementary as they find different categories of bugs. Finally, we provide insights into the relationships between the size, complexity, and fault detection capability of the generated input programs.",
        "Publication date": "12 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.06653"
    },
    {
        "ID": 313,
        "Title": "Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs",
        "Authors": [
            "Ziyu Li",
            "Donghwan Shin"
        ],
        "Abstract": "Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.\n  In this paper, we propose a novel method to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. We apply different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. We then use these pairs to test the ability of LLMs to correctly detect the inconsistencies.\n  We propose a new LLM testing method, called Mutation-based Consistency Testing (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of six programming languages (Python, C++, Java, Go, JavaScript, and Rust). We compare the performance of the LLMs across different types of code mutations and programming languages and analyze the results. We find that the LLMs show significant variation in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language.",
        "Publication date": "11 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.05940"
    },
    {
        "ID": 314,
        "Title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
        "Authors": [
            "Runchu Tian",
            "Yining Ye",
            "Yujia Qin",
            "Xin Cong",
            "Yankai Lin",
            "Yinxu Pan",
            "Yesai Wu",
            "Haotian Hui",
            "Weichuan Liu",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "Abstract": "Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.",
        "Publication date": "6 June, 2024",
        "Link": "https://arxiv.org/pdf/2401.04621"
    },
    {
        "ID": 315,
        "Title": "AKN_Regie: bridging digital and performing arts",
        "Authors": [
            "Georges GagnerÃ©",
            "Anastasiia Ternova"
        ],
        "Abstract": "AvatarStaging framework consists in directing avatars on a mixed theatrical stage, enabling a co-presence between the materiality of the physical actor and the virtuality of avatars controlled in real time by motion capture or specific animation players. It led to the implementation of the AKN_Regie authoring tool, programmed with the Blueprint visual language as a plugin for the Unreal Engine (UE) video game engine. The paper describes AKN_Regie main functionalities as a tool for non-programmer theatrical people. It gives insights of its implementation in the Blueprint visual language specific to UE. It details how the tool evolved along with its use in around ten theater productions. A circulation process between a nonprogramming point of view on AKN_Regie called Plugin Perspective and a programming acculturation to its development called Blueprint Perspective is discussed. Finally, a C++ Perspective is suggested to enhance the cultural appropriation of technological issues, bridging the gap between performing arts deeply involved in human materiality and avatars inviting to discover new worlds.",
        "Publication date": "8 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.03761"
    },
    {
        "ID": 316,
        "Title": "Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education",
        "Authors": [
            "Wei Hung Pan",
            "Ming Jie Chok",
            "Jonathan Leong Shan Wong",
            "Yung Xin Shin",
            "Yeong Shian Poon",
            "Zhou Yang",
            "Chun Yong Chong",
            "David Lo",
            "Mei Kuan Lim"
        ],
        "Abstract": "Educators are increasingly concerned about the usage of Large Language Models (LLMs) such as ChatGPT in programming education, particularly regarding the potential exploitation of imperfections in Artificial Intelligence Generated Content (AIGC) Detectors for academic misconduct. In this paper, we present an empirical study where the LLM is examined for its attempts to bypass detection by AIGC Detectors. This is achieved by generating code in response to a given question using different variants. We collected a dataset comprising 5,069 samples, with each sample consisting of a textual description of a coding problem and its corresponding human-written Python solution codes. These samples were obtained from various sources, including 80 from Quescol, 3,264 from Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs. Subsequently, we assessed the performance of five AIGC detectors. Our results demonstrate that existing AIGC Detectors perform poorly in distinguishing between human-written code and AI-generated code.",
        "Publication date": "8 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.03676"
    },
    {
        "ID": 317,
        "Title": "An Exploratory Study on Automatic Identification of Assumptions in the Development of Deep Learning Frameworks",
        "Authors": [
            "Chen Yang",
            "Peng Liang",
            "Zinan Ma"
        ],
        "Abstract": "Stakeholders constantly make assumptions in the development of deep learning (DL) frameworks. These assumptions are related to various types of software artifacts (e.g., requirements, design decisions, and technical debt) and can turn out to be invalid, leading to system failures. Existing approaches and tools for assumption management usually depend on manual identification of assumptions. However, assumptions are scattered in various sources (e.g., code comments, commits, pull requests, and issues) of DL framework development, and manually identifying assumptions has high costs. This study intends to evaluate different classification models for the purpose of identification with respect to assumptions from the point of view of developers and users in the context of DL framework projects (i.e., issues, pull requests, and commits) on GitHub. First, we constructed a new and largest dataset (i.e., the AssuEval dataset) of assumptions collected from the TensorFlow and Keras repositories on GitHub. Then we explored the performance of seven non-transformers based models (e.g., Support Vector Machine, Classification and Regression Trees), the ALBERT model, and three decoder-only models (i.e., ChatGPT, Claude, and Gemini) for identifying assumptions on the AssuEval dataset. The study results show that ALBERT achieves the best performance (f1-score: 0.9584) for identifying assumptions on the AssuEval dataset, which is much better than the other models (the 2nd best f1-score is 0.8858, achieved by the Claude 3.5 Sonnet model). Though ChatGPT, Claude, and Gemini are popular models, we do not recommend using them to identify assumptions in DL framework development because of their low performance. Fine-tuning ChatGPT, Claude, Gemini, or other language models (e.g., Llama3, Falcon, and BLOOM) specifically for assumptions might improve their performance for assumption identification.",
        "Publication date": "6 October, 2024",
        "Link": "https://arxiv.org/pdf/2401.03653"
    },
    {
        "ID": 318,
        "Title": "CG-Kit: Code Generation Toolkit for Performant and Maintainable Variants of Source Code Applied to Flash-X Hydrodynamics Simulations",
        "Authors": [
            "Johann Rudi",
            "Youngjun Lee",
            "Aidan H. Chadha",
            "Mohamed Wahib",
            "Klaus Weide",
            "Jared P. O'Neal",
            "Anshu Dubey"
        ],
        "Abstract": "CG-Kit is a new code generation toolkit that we propose as a solution for portability and maintainability for scientific computing applications. The development of CG-Kit is rooted in the urgent need created by the shifting landscape of high-performance computing platforms and the algorithmic complexities of a particular large-scale multiphysics application: Flash-X. This combination leads to unique challenges including handling an existing large code base in Fortran and/or C/C++, subdivision of code into a great variety of units supporting a wide range of physics and numerical methods, different parallelization techniques for distributed- and shared-memory systems and accelerator devices, and heterogeneity of computing platforms requiring coexisting variants of parallel algorithms. The challenges demand that developers determine custom abstractions and granularity for code generation. CG-Kit tackles this with standalone tools that can be combined into highly specific and, we argue, highly effective portability and maintainability tool chains. Here we present the design of our new tools: parametrized source trees, control flow graphs, and recipes. The tools are implemented in Python. Although the tools are agnostic to the programming language of the source code, we focus on C/C++ and Fortran. Code generation experiments demonstrate the generation of variants of parallel algorithms: first, multithreaded variants of the basic AXPY operation (scalar-vector addition and vector-vector multiplication) to introduce the application of CG-Kit tool chains; and second, variants of parallel algorithms within a hydrodynamics solver, called Spark, from Flash-X that operates on block-structured adaptive meshes. In summary, code generated by CG-Kit achieves a reduction by over 60% of the original C/C++/Fortran source code.",
        "Publication date": "6 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.03378"
    },
    {
        "ID": 319,
        "Title": "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding",
        "Authors": [
            "Linyuan Gong",
            "Mostafa Elhoushi",
            "Alvin Cheung"
        ],
        "Abstract": "Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our code and model are publicly available at https://github.com/gonglinyuan/ast_t5.",
        "Publication date": "22 June, 2024",
        "Link": "https://arxiv.org/pdf/2401.03003"
    },
    {
        "ID": 320,
        "Title": "Practical Reasoning in DatalogMTL",
        "Authors": [
            "Dingmin Wang",
            "PrzemysÅaw A. WaÅÄga",
            "Pan Hu",
            "Bernardo Cuenca Grau"
        ],
        "Abstract": "DatalogMTL is an extension of Datalog with metric temporal operators that has found an increasing number of applications in recent years. Reasoning in DatalogMTL is, however, of high computational complexity, which makes reasoning in modern data-intensive applications challenging. In this paper we present a practical reasoning algorithm for the full DatalogMTL language, which we have implemented in a system called MeTeoR. Our approach effectively combines an optimised (but generally non-terminating) materialisation (a.k.a. forward chaining) procedure, which provides scalable behaviour, with an automata-based component that guarantees termination and completeness. To ensure favourable scalability of the materialisation component, we propose a novel seminaÃ¯ve materialisation procedure for DatalogMTL enjoying the non-repetition property, which ensures that each specific rule application will be considered at most once throughout the entire execution of the algorithm. Moreover, our materialisation procedure is enhanced with additional optimisations which further reduce the number of redundant computations performed during materialisation by disregarding rules as soon as it is certain that they cannot derive new facts in subsequent materialisation steps. Our extensive evaluation supports the practicality of our approach.",
        "Publication date": "5 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.02869"
    },
    {
        "ID": 321,
        "Title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
        "Authors": [
            "Chengyue Wu",
            "Yukang Gan",
            "Yixiao Ge",
            "Zeyu Lu",
            "Jiahao Wang",
            "Ye Feng",
            "Ying Shan",
            "Ping Luo"
        ],
        "Abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.",
        "Publication date": "30 May, 2024",
        "Link": "https://arxiv.org/pdf/2401.02415"
    },
    {
        "ID": 322,
        "Title": "Empirical Analysis of Vulnerabilities Life Cycle in Golang Ecosystem",
        "Authors": [
            "Jinchang Hu",
            "Lyuye Zhang",
            "Chengwei Liu",
            "Sen Yang",
            "Song Huang",
            "Yang Liu"
        ],
        "Abstract": "Open-source software (OSS) greatly facilitates program development for developers. However, the high number of vulnerabilities in open-source software is a major concern, including in Golang, a relatively new programming language. In contrast to other commonly used OSS package managers, Golang presents a distinctive feature whereby commits are prevalently used as dependency versions prior to their integration into official releases. This attribute can prove advantageous to users, as patch commits can be implemented in a timely manner before the releases. However, Golang employs a decentralized mechanism for managing dependencies, whereby dependencies are upheld and distributed in separate repositories. This approach can result in delays in the dissemination of patches and unresolved vulnerabilities.\n  To tackle the aforementioned concern, a comprehensive investigation was undertaken to examine the life cycle of vulnerability in Golang, commencing from its introduction and culminating with its rectification. To this end, a framework was established by gathering data from diverse sources and systematically amalgamating them with an algorithm to compute the lags in vulnerability patching. It turned out that 66.10% of modules in the Golang ecosystem were affected by vulnerabilities. Within the vulnerability life cycle, we found two kinds of lag impeding the propagation of vulnerability fixing. By analyzing reasons behind non-lagged and lagged vulnerabilities, timely releasing and indexing patch versions could significantly enhance ecosystem security.",
        "Publication date": "17 January, 2024",
        "Link": "https://arxiv.org/pdf/2401.00515"
    },
    {
        "ID": 323,
        "Title": "Latent Idiom Recognition for a Minimalist Functional Array Language using Equality Saturation",
        "Authors": [
            "Jonathan Van der Cruysse",
            "Christophe Dubach"
        ],
        "Abstract": "Accelerating programs is typically done by recognizing code idioms matching high-performance libraries or hardware interfaces. However, recognizing such idioms automatically is challenging. The idiom recognition machinery is difficult to write and requires expert knowledge. In addition, slight variations in the input program might hide the idiom and defeat the recognizer.\n  This paper advocates for the use of a minimalist functional array language supporting a small, but expressive, set of operators. The minimalist design leads to a tiny sets of rewrite rules, which encode the language semantics. Crucially, the same minimalist language is also used to encode idioms. This removes the need for hand-crafted analysis passes, or for having to learn a complex domain-specific language to define the idioms.\n  Coupled with equality saturation, this approach is able to match the core functions from the BLAS and PyTorch libraries on a set of computational kernels. Compared to reference C kernel implementations, the approach produces a geometric mean speedup of 1.46x for C programs using BLAS, when generating such programs from the high-level minimalist language.",
        "Publication date": "29 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.17682"
    },
    {
        "ID": 324,
        "Title": "Probabilistic programming interfaces for random graphs: Markov categories, graphons, and nominal sets",
        "Authors": [
            "Nathanael L. Ackerman",
            "Cameron E. Freer",
            "Younesse Kaddar",
            "Jacek Karwowski",
            "Sean K. Moss",
            "Daniel M. Roy",
            "Sam Staton",
            "Hongseok Yang"
        ],
        "Abstract": "We study semantic models of probabilistic programming languages over graphs, and establish a connection to graphons from graph theory and combinatorics. We show that every well-behaved equational theory for our graph probabilistic programming language corresponds to a graphon, and conversely, every graphon arises in this way.\n  We provide three constructions for showing that every graphon arises from an equational theory. The first is an abstract construction, using Markov categories and monoidal indeterminates. The second and third are more concrete. The second is in terms of traditional measure theoretic probability, which covers 'black-and-white' graphons. The third is in terms of probability monads on the nominal sets of Gabbay and Pitts. Specifically, we use a variation of nominal sets induced by the theory of graphs, which covers ErdÅs-RÃ©nyi graphons. In this way, we build new models of graph probabilistic programming from graphons.",
        "Publication date": "28 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.17127"
    },
    {
        "ID": 325,
        "Title": "Participatory prompting: a user-centric research method for eliciting AI assistance opportunities in knowledge workflows",
        "Authors": [
            "Advait Sarkar",
            "Ian Drosos",
            "Rob Deline",
            "Andrew D. Gordon",
            "Carina Negreanu",
            "Sean Rintel",
            "Jack Williams",
            "Benjamin Zorn"
        ],
        "Abstract": "Generative AI, such as image generation models and large language models, stands to provide tremendous value to end-user programmers in creative and knowledge workflows. Current research methods struggle to engage end-users in a realistic conversation that balances the actually existing capabilities of generative AI with the open-ended nature of user workflows and the many opportunities for the application of this technology. In this work-in-progress paper, we introduce participatory prompting, a method for eliciting opportunities for generative AI in end-user workflows. The participatory prompting method combines a contextual inquiry and a researcher-mediated interaction with a generative model, which helps study participants interact with a generative model without having to develop prompting strategies of their own. We discuss the ongoing development of a study whose aim will be to identify end-user programming opportunities for generative AI in data analysis workflows.",
        "Publication date": "27 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.16633"
    },
    {
        "ID": 326,
        "Title": "Unifying Static and Dynamic Intermediate Languages for Accelerator Generators",
        "Authors": [
            "Caleb Kim",
            "Pai Li",
            "Anshuman Mohan",
            "Andrew Butt",
            "Adrian Sampson",
            "Rachit Nigam"
        ],
        "Abstract": "Compilers for accelerator design languages (ADLs) translate high-level languages into application-specific hardware. ADL compilers rely on a hardware control interface to compose hardware units. There are two choices: static control, which relies on cycle-level timing; or dynamic control, which uses explicit signalling to avoid depending on timing details. Static control is efficient but brittle; dynamic control incurs hardware costs to support compositional reasoning. Piezo is an ADL compiler that unifies static and dynamic control in a single intermediate language (IL). Its key insight is that the IL's static fragment is a refinement of its dynamic fragment: static code admits a subset of the run-time behaviors of the dynamic equivalent. Piezo can optimize code by combining facts from static and dynamic submodules, and it opportunistically converts code from dynamic to static control styles. We implement Piezo as an extension to an existing dynamic ADL compiler, Calyx. We use Piezo to implement an MLIR frontend, a systolic array generator, and a packet-scheduling hardware generator to demonstrate its optimizations and the static-dynamic interactions it enables.",
        "Publication date": "26 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.16300"
    },
    {
        "ID": 327,
        "Title": "A Prompt Learning Framework for Source Code Summarization",
        "Authors": [
            "Weisong Sun",
            "Chunrong Fang",
            "Yudu You",
            "Yuchen Chen",
            "Yi Liu",
            "Chong Wang",
            "Jian Zhang",
            "Quanjun Zhang",
            "Hanwei Qian",
            "Wei Zhao",
            "Yang Liu",
            "Zhenyu Chen"
        ],
        "Abstract": "(Source) code summarization is the task of automatically generating natural language summaries for given code snippets. Such summaries play a key role in helping developers understand and maintain source code. Recently, with the successful application of large language models (LLMs) in numerous fields, software engineering researchers have also attempted to adapt LLMs to solve code summarization tasks. The main adaptation schemes include instruction prompting and task-oriented fine-tuning. However, instruction prompting involves designing crafted prompts for zero-shot learning or selecting appropriate samples for few-shot learning and requires users to have professional domain knowledge, while task-oriented fine-tuning requires high training costs. In this paper, we propose a novel prompt learning framework for code summarization called PromptCS. PromptCS trains a prompt agent that can generate continuous prompts to unleash the potential for LLMs in code summarization. Compared to the human-written discrete prompt, the continuous prompts are produced under the guidance of LLMs and are therefore easier to understand by LLMs. PromptCS freezes the parameters of LLMs when training the prompt agent, which can greatly reduce the requirements for training resources. We evaluate PromptCS on the CodeSearchNet dataset involving multiple programming languages. The results show that PromptCS significantly outperforms instruction prompting schemes on all four widely used metrics. In some base LLMs, e.g., CodeGen-Multi-2B and StarCoderBase-1B and -3B, PromptCS even outperforms the task-oriented fine-tuning scheme. More importantly, the training efficiency of PromptCS is faster than the task-oriented fine-tuning scheme, with a more pronounced advantage on larger LLMs. The results of the human evaluation demonstrate that PromptCS can generate more good summaries compared to baselines.",
        "Publication date": "26 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.16066"
    },
    {
        "ID": 328,
        "Title": "Report of the DOE/NSF Workshop on Correctness in Scientific Computing, June 2023, Orlando, FL",
        "Authors": [
            "Maya Gokhale",
            "Ganesh Gopalakrishnan",
            "Jackson Mayo",
            "Santosh Nagarakatte",
            "Cindy Rubio-GonzÃ¡lez",
            "Stephen F. Siegel"
        ],
        "Abstract": "This report is a digest of the DOE/NSF Workshop on Correctness in Scientific Computing (CSC'23) held on June 17, 2023, as part of the Federated Computing Research Conference (FCRC) 2023. CSC was conceived by DOE and NSF to address the growing concerns about correctness among those who employ computational methods to perform large-scale scientific simulations. These concerns have escalated, given the complexity, scale, and heterogeneity of today's HPC software and hardware. If correctness is not proactively addressed, there is the risk of producing flawed science on top of unacceptable productivity losses faced by computational scientists and engineers. HPC systems are beginning to include data-driven methods, including machine learning and surrogate models, and their impact on overall HPC system correctness was also felt urgent to discuss.\n  Stakeholders of correctness in this space were identified to belong to several sub-disciplines of computer science; from computer architecture researchers who design special-purpose hardware that offers high energy efficiencies; numerical algorithm designers who develop efficient computational schemes based on reduced precision as well as reduced data movement; all the way to researchers in programming language and formal methods who seek methodologies for correct compilation and verification. To include attendees with such a diverse set of backgrounds, CSC was held during the Federated Computing Research Conference (FCRC) 2023.",
        "Publication date": "27 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.15640"
    },
    {
        "ID": 329,
        "Title": "cuPDLP-C: A Strengthened Implementation of cuPDLP for Linear Programming by C language",
        "Authors": [
            "Haihao Lu",
            "Jinwen Yang",
            "Haodong Hu",
            "Qi Huangfu",
            "Jinsong Liu",
            "Tianhao Liu",
            "Yinyu Ye",
            "Chuwen Zhang",
            "Dongdong Ge"
        ],
        "Abstract": "A recent GPU implementation of the Restarted Primal-Dual Hybrid Gradient Method for Linear Programming was proposed in Lu and Yang (2023). Its computational results demonstrate the significant computational advantages of the GPU-based first-order algorithm on certain large-scale problems. The average performance also achieves a level close to commercial solvers for the first time in history. However, due to limitations in experimental hardware and the disadvantage of implementing the algorithm in Julia compared to C language, neither the commercial solver nor cuPDLP reached their maximum efficiency. Therefore, in this report, we have re-implemented and optimized cuPDLP in C language. Utilizing state-of-the-art CPU and GPU hardware, we extensively compare cuPDLP with the best commercial solvers. The experiments further highlight its substantial computational advantages and potential for solving large-scale linear programming problems. We also discuss the profound impact this breakthrough may have on mathematical programming research and the entire operations research community.",
        "Publication date": "7 January, 2024",
        "Link": "https://arxiv.org/pdf/2312.14832"
    },
    {
        "ID": 330,
        "Title": "What Do You Mean by Memory? When Engineers Are Lost in the Maze of Complexity",
        "Authors": [
            "Gunnar Kudrjavets",
            "Aditya Kumar",
            "Jeff Thomas",
            "Ayushi Rastogi"
        ],
        "Abstract": "An accepted practice to decrease applications' memory usage is to reduce the amount and frequency of memory allocations. Factors such as (a) the prevalence of out-of-memory (OOM) killers, (b) memory allocations in modern programming languages done implicitly, (c) overcommitting being a default strategy in the Linux kernel, and (d) the rise in complexity and terminology related to memory management makes the existing guidance inefficient. The industry needs detailed guidelines for optimizing memory usage targeting specific operating systems (OS) and programming language types.",
        "Publication date": "20 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.13462"
    },
    {
        "ID": 331,
        "Title": "DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines",
        "Authors": [
            "Arnav Singhvi",
            "Manish Shetty",
            "Shangyin Tan",
            "Christopher Potts",
            "Koushik Sen",
            "Matei Zaharia",
            "Omar Khattab"
        ],
        "Abstract": "Chaining language model (LM) calls as composable modules is fueling a new way of programming, but ensuring LMs adhere to important constraints requires heuristic \"prompt engineering\". We introduce LM Assertions, a programming construct for expressing computational constraints that LMs should satisfy. We integrate our constructs into the recent DSPy programming model for LMs, and present new strategies that allow DSPy to compile programs with LM Assertions into more reliable and accurate systems. We also propose strategies to use assertions at inference time for automatic self-refinement with LMs. We report on four diverse case studies for text generation and find that LM Assertions improve not only compliance with imposed rules but also downstream task performance, passing constraints up to 164% more often and generating up to 37% more higher-quality responses. Our reference implementation of LM Assertions is integrated into DSPy at https://github.com/stanfordnlp/dspy",
        "Publication date": "2 February, 2024",
        "Link": "https://arxiv.org/pdf/2312.13382"
    },
    {
        "ID": 332,
        "Title": "MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks",
        "Authors": [
            "Tal Kadosh",
            "Niranjan Hasabnis",
            "Vy A. Vo",
            "Nadav Schneider",
            "Neva Krien",
            "Mihai Capota",
            "Abdul Wasay",
            "Nesreen Ahmed",
            "Ted Willke",
            "Guy Tamir",
            "Yuval Pinter",
            "Timothy Mattson",
            "Gal Oren"
        ],
        "Abstract": "With easier access to powerful compute resources, there is a growing trend in AI for software development to develop large language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size and demand expensive compute resources for training. This is partly because LLMs for HPC tasks are obtained by finetuning existing LLMs that support several natural and/or programming languages. We found this design choice confusing - why do we need LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question choices made by existing LLMs by developing smaller language models (LMs) for specific domains - we call them domain-specific LMs. Specifically, we start with HPC as a domain and build an HPC-specific LM, named MonoCoder, which is orders of magnitude smaller than existing LMs but delivers better performance on non-HPC and HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific dataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated the performance of MonoCoder against state-of-the-art multi-lingual LLMs. Results demonstrate that MonoCoder, although much smaller than existing LMs, outperforms other LLMs on normalized-perplexity tests (in relation to model size) while also delivering competing CodeBLEU scores for high-performance and parallel code generations. In other words, results suggest that MonoCoder understands HPC code better than state-of-the-art LLMs.",
        "Publication date": "19 September, 2024",
        "Link": "https://arxiv.org/pdf/2312.13322"
    },
    {
        "ID": 333,
        "Title": "A functional scripting interface to an object oriented C++ library",
        "Authors": [
            "Markus Kloimwieder",
            "Christoph Gadermaier"
        ],
        "Abstract": "The object oriented programming paradigm is widely used in science and engineering. Many open and commercial libraries are written in C++ and increasingly provide bindings to Python, which is much easier to learn, but still partly encourages the use of object oriented programming. However, scientific ideas are much more directly and meaningfully expressed in the purely functional programming paradigm. Here, we take a best practice example, CERNs Python binding for its ROOT library, designed to handle the enormous amounts of data generated by the worlds largest particle accelerator, and translate a simple segment of its tutorial into Clojure, a functional language from the Lisp family. The code examples demonstrate how a purely functional language straightforwardly expresses scientific ideas. Subsequently, we develop a compiled Lisp-C++ interoperation layer to access the ROOT library exclusively via functional code. To preserve the expressivity of the Lisp code, the type hints necessary for C++ code generation are stored in a separate file. The interop system presented here is a generic framework that, when provided with a suitable file of type hints, facilitates access to methods of arbitrary C++ libraries and platforms like real-time microcontrollers.",
        "Publication date": "17 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.13295"
    },
    {
        "ID": 334,
        "Title": "LPR: Large Language Models-Aided Program Reduction",
        "Authors": [
            "Mengxiao Zhang",
            "Yongqiang Tian",
            "Zhenyang Xu",
            "Yiwen Dong",
            "Shin Hwei Tan",
            "Chengnian Sun"
        ],
        "Abstract": "Program reduction is a prevalent technique to facilitate compilers' debugging by automatically minimizing bug-triggering programs. Existing program reduction techniques are either generic across languages (e.g., Perses and Vulcan) or specifically customized for one certain language by employing language-specific features, like C-Reduce. However, striking the balance between generality across multiple programming languages and specificity to individual languages in program reduction is yet to be explored. This paper proposes LPR, the first technique utilizing LLMs to perform language-specific program reduction for multiple languages. The core insight is to utilize both the language-generic syntax level program reduction (e.g., Perses) and the language-specific semantic level program transformations learned by LLMs. Alternately, language-generic program reducers efficiently reduce programs into 1-tree-minimality, which is small enough to be manageable for LLMs; LLMs effectively transform programs via the learned semantics to expose new reduction opportunities for the language-generic program reducers to further reduce the programs. Our extensive evaluation on 50 benchmarks across three languages (C, Rust, and JavaScript) has highlighted LPR's practicality and superiority over Vulcan, the state-of-the-art language-generic program reducer. For effectiveness, LPR surpasses Vulcan by producing 24.93%, 4.47%, and 11.71% smaller programs on benchmarks in C, Rust and JavaScript. Moreover, LPR and Vulcan have demonstrated their potential to complement each other. By using Vulcan on LPR's output for C programs, we achieve program sizes comparable to those reduced by C-Reduce. For efficiency, LPR takes 10.77%, 34.88%, 36.96% less time than Vulcan to finish all benchmarks in C, Rust and JavaScript, separately.",
        "Publication date": "11 May, 2024",
        "Link": "https://arxiv.org/pdf/2312.13064"
    },
    {
        "ID": 335,
        "Title": "Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions",
        "Authors": [
            "Federico Cassano",
            "Luisa Li",
            "Akul Sethi",
            "Noah Shinn",
            "Abby Brennan-Jones",
            "Jacob Ginesin",
            "Edward Berman",
            "George Chakhnashvili",
            "Anton Lozhkov",
            "Carolyn Jane Anderson",
            "Arjun Guha"
        ],
        "Abstract": "A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is provided a block of code and an instruction to modify the code. The editing instruction may ask for a feature to be added or removed, describe a bug and ask for a fix, or ask for a different kind of solution. We introduce a carefully crafted benchmark of code editing tasks and use it to evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is better than the best open model at code editing tasks. We also introduce a new, carefully curated, permissively licensed training dataset of code editing tasks coupled with natural language instructions. Using this training dataset, we show that we can fine-tune open Code LLMs to significantly improve their code editing capabilities, closing the gap between open and closed models. All code, data, and models are available at https://github.com/nuprl/CanItEdit.",
        "Publication date": "23 September, 2024",
        "Link": "https://arxiv.org/pdf/2312.12450"
    },
    {
        "ID": 336,
        "Title": "Xpert: Empowering Incident Management with Query Recommendations via Large Language Models",
        "Authors": [
            "Yuxuan Jiang",
            "Chaoyun Zhang",
            "Shilin He",
            "Zhihao Yang",
            "Minghua Ma",
            "Si Qin",
            "Yu Kang",
            "Yingnong Dang",
            "Saravan Rajmohan",
            "Qingwei Lin",
            "Dongmei Zhang"
        ],
        "Abstract": "Large-scale cloud systems play a pivotal role in modern IT infrastructure. However, incidents occurring within these systems can lead to service disruptions and adversely affect user experience. To swiftly resolve such incidents, on-call engineers depend on crafting domain-specific language (DSL) queries to analyze telemetry data. However, writing these queries can be challenging and time-consuming. This paper presents a thorough empirical study on the utilization of queries of KQL, a DSL employed for incident management in a large-scale cloud management system at Microsoft. The findings obtained underscore the importance and viability of KQL queries recommendation to enhance incident management.\n  Building upon these valuable insights, we introduce Xpert, an end-to-end machine learning framework that automates KQL recommendation process. By leveraging historical incident data and large language models, Xpert generates customized KQL queries tailored to new incidents. Furthermore, Xpert incorporates a novel performance metric called Xcore, enabling a thorough evaluation of query quality from three comprehensive perspectives. We conduct extensive evaluations of Xpert, demonstrating its effectiveness in offline settings. Notably, we deploy Xpert in the real production environment of a large-scale incident management system in Microsoft, validating its efficiency in supporting incident management. To the best of our knowledge, this paper represents the first empirical study of its kind, and Xpert stands as a pioneering DSL query recommendation framework designed for incident management.",
        "Publication date": "19 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.11988"
    },
    {
        "ID": 337,
        "Title": "APIDocBooster: An Extract-Then-Abstract Framework Leveraging Large Language Models for Augmenting API Documentation",
        "Authors": [
            "Chengran Yang",
            "Jiakun Liu",
            "Bowen Xu",
            "Christoph Treude",
            "Yunbo Lyu",
            "Junda He",
            "Ming Li",
            "David Lo"
        ],
        "Abstract": "API documentation is often the most trusted resource for programming. Many approaches have been proposed to augment API documentation by summarizing complementary information from external resources such as Stack Overflow. Existing extractive-based summarization approaches excel in producing faithful summaries that accurately represent the source content without input length restrictions. Nevertheless, they suffer from inherent readability limitations. On the other hand, our empirical study on the abstractive-based summarization method, i.e., GPT-4, reveals that GPT-4 can generate coherent and concise summaries but presents limitations in terms of informativeness and faithfulness.\n  We introduce APIDocBooster, an extract-then-abstract framework that seamlessly fuses the advantages of both extractive (i.e., enabling faithful summaries without length limitation) and abstractive summarization (i.e., producing coherent and concise summaries). APIDocBooster consists of two stages: (1) \\textbf{C}ontext-aware \\textbf{S}entence \\textbf{S}ection \\textbf{C}lassification (CSSC) and (2) \\textbf{UP}date \\textbf{SUM}marization (UPSUM). CSSC classifies API-relevant information collected from multiple sources into API documentation sections. UPSUM first generates extractive summaries distinct from the original API documentation and then generates abstractive summaries guided by extractive summaries through in-context learning.\n  To enable automatic evaluation of APIDocBooster, we construct the first dataset for API document augmentation. Our automatic evaluation results reveal that each stage in APIDocBooster outperforms its baselines by a large margin. Our human evaluation also demonstrates the superiority of APIDocBooster over GPT-4 and shows that it improves informativeness, relevance, and faithfulness by 13.89\\%, 15.15\\%, and 30.56\\%, respectively.",
        "Publication date": "10 January, 2024",
        "Link": "https://arxiv.org/pdf/2312.10934"
    },
    {
        "ID": 338,
        "Title": "Primitive-based 3D Human-Object Interaction Modelling and Programming",
        "Authors": [
            "Siqi Liu",
            "Yong-Lu Li",
            "Zhou Fang",
            "Xinpeng Liu",
            "Yang You",
            "Cewu Lu"
        ],
        "Abstract": "Embedding Human and Articulated Object Interaction (HAOI) in 3D is an important direction for a deeper human activity understanding. Different from previous works that use parametric and CAD models to represent humans and objects, in this work, we propose a novel 3D geometric primitive-based language to encode both humans and objects. Given our new paradigm, humans and objects are all compositions of primitives instead of heterogeneous entities. Thus, mutual information learning may be achieved between the limited 3D data of humans and different object categories. Moreover, considering the simplicity of the expression and the richness of the information it contains, we choose the superquadric as the primitive representation. To explore an effective embedding of HAOI for the machine, we build a new benchmark on 3D HAOI consisting of primitives together with their images and propose a task requiring machines to recover 3D HAOI using primitives from images. Moreover, we propose a baseline of single-view 3D reconstruction on HAOI. We believe this primitive-based 3D HAOI representation would pave the way for 3D HAOI studies. Our code and data are available at https://mvig-rhos.com/p3haoi.",
        "Publication date": "17 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.10714"
    },
    {
        "ID": 339,
        "Title": "WaveCert: Translation Validation for Asynchronous Dataflow Programs via Dynamic Fractional Permissions",
        "Authors": [
            "Zhengyao Lin",
            "Joshua Gancher",
            "Bryan Parno"
        ],
        "Abstract": "Coarse-grained reconfigurable arrays (CGRAs) have gained attention in recent years due to their promising power efficiency compared to traditional von Neumann architectures. To program these architectures using ordinary languages such as C, a dataflow compiler must transform the original sequential, imperative program into an equivalent dataflow graph, composed of dataflow operators running in parallel. This transformation is challenging since the asynchronous nature of dataflow graphs allows out-of-order execution of operators, leading to behaviors not present in the original imperative programs.\n  We address this challenge by developing a translation validation technique for dataflow compilers to ensure that the dataflow program has the same behavior as the original imperative program on all possible inputs and schedules of execution. We apply this method to a state-of-the-art dataflow compiler targeting the RipTide CGRA architecture. Our tool uncovers 8 compiler bugs where the compiler outputs incorrect dataflow graphs, including a data race that is otherwise hard to discover via testing. After repairing these bugs, our tool verifies the correct compilation of all programs in the RipTide benchmark suite.",
        "Publication date": "12 April, 2024",
        "Link": "https://arxiv.org/pdf/2312.09326"
    },
    {
        "ID": 340,
        "Title": "SGLang: Efficient Execution of Structured Language Model Programs",
        "Authors": [
            "Lianmin Zheng",
            "Liangsheng Yin",
            "Zhiqiang Xie",
            "Chuyue Sun",
            "Jeff Huang",
            "Cody Hao Yu",
            "Shiyi Cao",
            "Christos Kozyrakis",
            "Ion Stoica",
            "Joseph E. Gonzalez",
            "Clark Barrett",
            "Ying Sheng"
        ],
        "Abstract": "Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang",
        "Publication date": "5 June, 2024",
        "Link": "https://arxiv.org/pdf/2312.07104"
    },
    {
        "ID": 341,
        "Title": "\"I Want It That Way\": Enabling Interactive Decision Support Using Large Language Models and Constraint Programming",
        "Authors": [
            "Connor Lawless",
            "Jakob Schoeffer",
            "Lindy Le",
            "Kael Rowan",
            "Shilad Sen",
            "Cristina St. Hill",
            "Jina Suh",
            "Bahareh Sarrafzadeh"
        ],
        "Abstract": "A critical factor in the success of decision support systems is the accurate modeling of user preferences. Psychology research has demonstrated that users often develop their preferences during the elicitation process, highlighting the pivotal role of system-user interaction in developing personalized systems. This paper introduces a novel approach, combining Large Language Models (LLMs) with Constraint Programming to facilitate interactive decision support. We study this hybrid framework through the lens of meeting scheduling, a time-consuming daily activity faced by a multitude of information workers. We conduct three studies to evaluate the novel framework, including a diary study (n=64) to characterize contextual scheduling preferences, a quantitative evaluation of the system's performance, and a user study (n=10) with a prototype system. Our work highlights the potential for a hybrid LLM and optimization approach for iterative preference elicitation and design considerations for building systems that support human-system collaborative decision-making processes.",
        "Publication date": "1 October, 2024",
        "Link": "https://arxiv.org/pdf/2312.06908"
    },
    {
        "ID": 342,
        "Title": "Numeric Truncation Security Predicate",
        "Authors": [
            "Timofey Mezhuev",
            "Ilay Kobrin",
            "Alexey Vishnyakov",
            "Daniil Kuts"
        ],
        "Abstract": "Numeric truncation is a widely spread error in software written in languages with static data typing, such as C/C++ or Java. It occurs when the significant bits of the value with a bigger type size are truncated during value conversion to the smaller type. Utilizing one of the most powerful methods for path exploration and automated bug detection called dynamic symbolic execution (DSE), we propose the symbolic security predicate for numeric truncation error detection, developed on top of DSE tool Sydr. Firstly, we execute the program on the data, which does not lead to any errors. During program execution we update symbolic shadow stack and shadow registers to track symbolic sizes of the symbolic variables to avoid false positives. Then, if we meet the instruction, which truncates the symbolic variable, we build the security predicate, try to solve it with the SMT-solver and in case of success save new input file to reproduce the error. We tested our approach on Juliet Dynamic test suite for CWE-197 and achieved 100% accuracy. We approved the workability of our approach by detecting 12 new errors of numeric truncation in 5 different real-world open source projects within OSS-Sydr-Fuzz project. All of the errors were reported, most of the reports were equipped with appropriate fixes, successfully confirmed and applied by project maintainers.",
        "Publication date": "2 May, 2024",
        "Link": "https://arxiv.org/pdf/2312.06425"
    },
    {
        "ID": 343,
        "Title": "Leveraging Reinforcement Learning and Large Language Models for Code Optimization",
        "Authors": [
            "Shukai Duan",
            "Nikos Kanakaris",
            "Xiongye Xiao",
            "Heng Ping",
            "Chenyu Zhou",
            "Nesreen K. Ahmed",
            "Guixiang Ma",
            "Mihai Capota",
            "Theodore L. Willke",
            "Shahin Nazarian",
            "Paul Bogdan"
        ],
        "Abstract": "Code optimization is a daunting task that requires a significant level of expertise from experienced programmers. This level of expertise is not sufficient when compared to the rapid development of new hardware architectures. Towards advancing the whole code optimization process, recent approaches rely on machine learning and artificial intelligence techniques. This paper introduces a new framework to decrease the complexity of code optimization. The proposed framework builds on large language models (LLMs) and reinforcement learning (RL) and enables LLMs to receive feedback from their environment (i.e., unit tests) during the fine-tuning process. We compare our framework with existing state-of-the-art models and show that it is more efficient with respect to speed and computational usage, as a result of the decrement in training steps and its applicability to models with fewer parameters. Additionally, our framework reduces the possibility of logical and syntactical errors. Toward evaluating our approach, we run several experiments on the PIE dataset using a CodeT5 language model and RRHF, a new reinforcement learning algorithm. We adopt a variety of evaluation metrics with regards to optimization quality, and speedup. The evaluation results demonstrate that the proposed framework has similar results in comparison with existing models using shorter training times and smaller pre-trained models. In particular, we accomplish an increase of 5.6% and 2.2 over the baseline models concerning the %OP T and SP metrics.",
        "Publication date": "9 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.05657"
    },
    {
        "ID": 344,
        "Title": "Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing",
        "Authors": [
            "Robson Santos",
            "Italo Santos",
            "Cleyton Magalhaes",
            "Ronnie de Souza Santos"
        ],
        "Abstract": "A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates coherent content, including grammatically precise sentences, human-like paragraphs, and syntactically accurate code snippets. LLMs can play a pivotal role in software development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts, specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.",
        "Publication date": "8 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.04860"
    },
    {
        "ID": 345,
        "Title": "DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions",
        "Authors": [
            "Fangzhou Wu",
            "Xiaogeng Liu",
            "Chaowei Xiao"
        ],
        "Abstract": "With the advancement of Large Language Models (LLMs), significant progress has been made in code generation, enabling LLMs to transform natural language into programming code. These Code LLMs have been widely accepted by massive users and organizations. However, a dangerous nature is hidden in the code, which is the existence of fatal vulnerabilities. While some LLM providers have attempted to address these issues by aligning with human guidance, these efforts fall short of making Code LLMs practical and robust. Without a deep understanding of the performance of the LLMs under the practical worst cases, it would be concerning to apply them to various real-world applications. In this paper, we answer the critical issue: Are existing Code LLMs immune to generating vulnerable code? If not, what is the possible maximum severity of this issue in practical deployment scenarios? In this paper, we introduce DeceptPrompt, a novel algorithm that can generate adversarial natural language instructions that drive the Code LLMs to generate functionality correct code with vulnerabilities. DeceptPrompt is achieved through a systematic evolution-based algorithm with a fine grain loss design. The unique advantage of DeceptPrompt enables us to find natural prefix/suffix with totally benign and non-directional semantic meaning, meanwhile, having great power in inducing the Code LLMs to generate vulnerable code. This feature can enable us to conduct the almost-worstcase red-teaming on these LLMs in a real scenario, where users are using natural language. Our extensive experiments and analyses on DeceptPrompt not only validate the effectiveness of our approach but also shed light on the huge weakness of LLMs in the code generation task. When applying the optimized prefix/suffix, the attack success rate (ASR) will improve by average 50% compared with no prefix/suffix applying.",
        "Publication date": "12 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.04730"
    },
    {
        "ID": 346,
        "Title": "The BigCode Project Governance Card",
        "Authors": [
            "BigCode collaboration",
            "Sean Hughes",
            "Harm de Vries",
            "Jennifer Robinson",
            "Carlos MuÃ±oz Ferrandis",
            "Loubna Ben Allal",
            "Leandro von Werra",
            "Jennifer Ding",
            "Sebastien Paquet",
            "Yacine Jernite"
        ],
        "Abstract": "This document serves as an overview of the different mechanisms and areas of governance in the BigCode project. It aims to support transparency by providing relevant information about choices that were made during the project to the broader public, and to serve as an example of intentional governance of an open research project that future endeavors can leverage to shape their own approach. The first section, Project Structure, covers the project organization, its stated goals and values, its internal decision processes, and its funding and resources. The second section, Data and Model Governance, covers decisions relating to the questions of data subject consent, privacy, and model release.",
        "Publication date": "6 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.03872"
    },
    {
        "ID": 347,
        "Title": "Real Customization or Just Marketing: Are Customized Versions of Chat GPT Useful?",
        "Authors": [
            "Eduardo C. Garrido-MerchÃ¡n",
            "Jose L. Arroyo-BarrigÃ¼ete",
            "Francisco BorrÃ¡s-Pala",
            "Leandro Escobar-Torres",
            "Carlos MartÃ­nez de Ibarreta",
            "Jose MarÃ­a Ortiz-Lozano",
            "Antonio Rua-Vieites"
        ],
        "Abstract": "Large Language Models (LLMs), as the case of OpenAI ChatGPT-4 Turbo, are revolutionizing several industries, including higher education. In this context, LLMs can be personalized through a fine-tuning process to meet the student demands on every particular subject, like statistics. Recently, OpenAI has launched the possibility to fine-tune their model with a natural language web interface, enabling the possibility to create customized GPT version deliberately conditioned to meet the demands of a specific task. The objective of this research is to assess the potential of the customized GPTs that have recently been launched by OpenAI. After developing a Business Statistics Virtual Professor (BSVP), tailored for students at the Universidad Pontificia Comillas, its behavior was evaluated and compared with that of ChatGPT-4 Turbo. The results lead to several conclusions. Firstly, a substantial modification in the style of communication was observed. Following the instructions it was trained with, BSVP provided responses in a more relatable and friendly tone, even incorporating a few minor jokes. Secondly, and this is a matter of relevance, when explicitly asked for something like, \"I would like to practice a programming exercise similar to those in R practice 4,\" BSVP was capable of providing a far superior response: having access to contextual documentation, it could fulfill the request, something beyond ChatGPT-4 Turbo's capabilities. On the downside, the response times were generally higher. Lastly, regarding overall performance, quality, depth, and alignment with the specific content of the course, no statistically significant differences were observed in the responses between BSVP and ChatGPT-4 Turbo. It appears that customized assistants trained with prompts present advantages as virtual aids for students, yet they do not constitute a substantial improvement over ChatGPT-4 Turbo.",
        "Publication date": "27 November, 2023",
        "Link": "https://arxiv.org/pdf/2312.03728"
    },
    {
        "ID": 348,
        "Title": "Investigating Technology Usage Span by Analyzing Users' Q&A Traces in Stack Overflow",
        "Authors": [
            "Saikat Mondal",
            "Debajyoti Mondal",
            "Chanchal K. Roy"
        ],
        "Abstract": "Choosing an appropriate software development technology (e.g., programming language) is challenging due to the proliferation of diverse options. The selection of inappropriate technologies for development may have a far-reaching effect on software developers' career growth. Switching to a different technology after working with one may lead to a complex learning curve and, thus, be more challenging. Therefore, it is crucial for software developers to find technologies that have a high usage span. Intuitively, the usage span of a technology can be determined by the time span developers have used that technology. Existing literature focuses on the technology landscape to explore the complex and implicit dependencies among technologies but lacks formal studies to draw insights about their usage span. This paper investigates the technology usage span by analyzing the question and answering (Q&A) traces of Stack Overflow (SO), the largest technical Q&A website available to date. In particular, we analyze 6.7 million Q&A traces posted by about 97K active SO users and see what technologies have appeared in their questions or answers over 15 years. According to our analysis, C# and Java programming languages have a high usage span, followed by JavaScript. Besides, developers used the .NET framework, iOS & Windows Operating Systems (OS), and SQL query language for a long time (on average). Our study also exposes the emerging (i.e., newly growing) technologies. For example, usages of technologies such as SwiftUI, .NET-6.0, Visual Studio 2022, and Blazor WebAssembly framework are increasing. The findings from our study can assist novice developers, startup software industries, and software users in determining appropriate technologies. This also establishes an initial benchmark for future investigation on the use span of software technologies.",
        "Publication date": "5 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.03182"
    },
    {
        "ID": 349,
        "Title": "A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education",
        "Authors": [
            "Jacob Doughty",
            "Zipiao Wan",
            "Anishka Bompelli",
            "Jubahed Qayum",
            "Taozhi Wang",
            "Juran Zhang",
            "Yujia Zheng",
            "Aidan Doyle",
            "Pragnya Sridhar",
            "Arav Agarwal",
            "Christopher Bogart",
            "Eric Keylor",
            "Can Kultur",
            "Jaromir Savelka",
            "Majd Sakr"
        ],
        "Abstract": "There is a constant need for educators to develop and maintain effective up-to-date assessments. While there is a growing body of research in computing education on utilizing large language models (LLMs) in generation and engagement with coding exercises, the use of LLMs for generating programming MCQs has not been extensively explored. We analyzed the capability of GPT-4 to produce multiple-choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education. Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs from high-level course context and module-level LOs. We evaluated 651 LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python courses. We found that GPT-4 was capable of producing MCQs with clear language, a single correct choice, and high-quality distractors. We also observed that the generated MCQs appeared to be well-aligned with the LOs. Our findings can be leveraged by educators wishing to take advantage of the state-of-the-art generative models to support MCQ authoring efforts.",
        "Publication date": "5 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.03173"
    },
    {
        "ID": 350,
        "Title": "Checkpoint-based rollback recovery in session programming",
        "Authors": [
            "Claudio Antares Mezzina",
            "Francesco Tiezzi",
            "Nobuko Yoshida"
        ],
        "Abstract": "To react to unforeseen circumstances or amend abnormal situations in communication-centric systems, programmers are in charge of \"undoing\" the interactions which led to an undesired state. To assist this task, session-based languages can be endowed with reversibility mechanisms. In this paper we propose a language enriched with programming facilities to commit session interactions, to roll back the computation to a previous commit point, and to abort the session. Rollbacks in our language always bring the system to previous visited states and a rollback cannot bring the system back to a point prior to the last commit. Programmers are relieved from the burden of ensuring that a rollback never restores a checkpoint imposed by a session participant different from the rollback requester. Such undesired situations are prevented at design-time (statically) by relying on a decidable compliance check at the type level, implemented in MAUDE. We show that the language satisfies error-freedom and progress of a session.",
        "Publication date": "19 August, 2024",
        "Link": "https://arxiv.org/pdf/2312.02851"
    },
    {
        "ID": 351,
        "Title": "Competition-Level Problems are Effective LLM Evaluators",
        "Authors": [
            "Yiming Huang",
            "Zhenghao Lin",
            "Xiao Liu",
            "Yeyun Gong",
            "Shuai Lu",
            "Fangyu Lei",
            "Yaobo Liang",
            "Yelong Shen",
            "Chen Lin",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "Abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered. Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges. Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future.",
        "Publication date": "4 June, 2024",
        "Link": "https://arxiv.org/pdf/2312.02143"
    },
    {
        "ID": 352,
        "Title": "On the Effectiveness of Large Language Models in Domain-Specific Code Generation",
        "Authors": [
            "Yalan Lin",
            "Meng Chen",
            "Yuhan Hu",
            "Hongyu Zhang",
            "Chengcheng Wan",
            "Zhao Wei",
            "Yong Xu",
            "Juhong Wang",
            "Xiaodong Gu"
        ],
        "Abstract": "Large language models (LLMs) such as ChatGPT have shown remarkable capabilities in code generation. Despite significant achievements, they rely on enormous training data to acquire a broad spectrum of open-domain knowledge. Besides, their evaluation revolves around open-domain benchmarks like HumanEval, which primarily consist of programming contests. Therefore, it is hard to fully characterize the intricacies and challenges associated with particular domains (e.g., web, game, and math). In this paper, we conduct an in-depth study of the LLMs in domain-specific code generation. Our results demonstrate that LLMs exhibit sub-optimal performance in generating domain-specific code, due to their limited proficiency in utilizing domain-specific libraries. We further observe that incorporating API knowledge as prompts can empower LLMs to generate more professional code. Based on these findings, we further investigate how to effectively incorporate API knowledge into the code generation process. We experiment with three strategies for incorporating domain knowledge, namely, external knowledge inquirer, chain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these strategies as a new code generation approach called DomCoder. Experimental results show that all strategies of DomCoder lead to improvement in the effectiveness of domain-specific code generation under certain settings.",
        "Publication date": "19 September, 2024",
        "Link": "https://arxiv.org/pdf/2312.01639"
    },
    {
        "ID": 353,
        "Title": "Advanced Large Language Model (LLM)-Driven Verilog Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis",
        "Authors": [
            "Kiran Thorat",
            "Jiahui Zhao",
            "Yaotian Liu",
            "Hongwu Peng",
            "Xi Xie",
            "Bin Lei",
            "Jeff Zhang",
            "Caiwen Ding"
        ],
        "Abstract": "The increasing use of Advanced Language Models (ALMs) in diverse sectors, particularly due to their impressive capability to generate top-tier content following linguistic instructions, forms the core of this investigation. This study probes into ALMs' deployment in electronic hardware design, with a specific emphasis on the synthesis and enhancement of Verilog programming. We introduce an innovative framework, crafted to assess and amplify ALMs' productivity in this niche. The methodology commences with the initial crafting of Verilog programming via ALMs, succeeded by a distinct dual-stage refinement protocol. The premier stage prioritizes augmenting the code's operational and linguistic precision, while the latter stage is dedicated to aligning the code with Power-Performance-Area (PPA) benchmarks, a pivotal component in proficient hardware design. This bifurcated strategy, merging error remediation with PPA enhancement, has yielded substantial upgrades in the caliber of ALM-created Verilog programming. Our framework achieves an 81.37% rate in linguistic accuracy and 62.0% in operational efficacy in programming synthesis, surpassing current leading-edge techniques, such as 73% in linguistic accuracy and 46% in operational efficacy. These findings illuminate ALMs' aptitude in tackling complex technical domains and signal a positive shift in the mechanization of hardware design operations.",
        "Publication date": "9 January, 2024",
        "Link": "https://arxiv.org/pdf/2312.01022"
    },
    {
        "ID": 354,
        "Title": "Abstract Syntax Tree for Programming Language Understanding and Representation: How Far Are We?",
        "Authors": [
            "Weisong Sun",
            "Chunrong Fang",
            "Yun Miao",
            "Yudu You",
            "Mengzhe Yuan",
            "Yuchen Chen",
            "Quanjun Zhang",
            "An Guo",
            "Xiang Chen",
            "Yang Liu",
            "Zhenyu Chen"
        ],
        "Abstract": "Programming language understanding and representation (a.k.a code representation learning) has always been a hot and challenging task in software engineering. It aims to apply deep learning techniques to produce numerical representations of the source code features while preserving its semantics. These representations can be used for facilitating subsequent code-related tasks. The abstract syntax tree (AST), a fundamental code feature, illustrates the syntactic information of the source code and has been widely used in code representation learning. However, there is still a lack of systematic and quantitative evaluation of how well AST-based code representation facilitates subsequent code-related tasks. In this paper, we first conduct a comprehensive empirical study to explore the effectiveness of the AST-based code representation in facilitating follow-up code-related tasks. To do so, we compare the performance of models trained with code token sequence (Token for short) based code representation and AST-based code representation on three popular types of code-related tasks. Surprisingly, the overall quantitative statistical results demonstrate that models trained with AST-based code representation consistently perform worse across all three tasks compared to models trained with Token-based code representation. Our further quantitative analysis reveals that models trained with AST-based code representation outperform models trained with Token-based code representation in certain subsets of samples across all three tasks. We also conduct comprehensive experiments to evaluate and reveal the impact of the choice of AST parsing/preprocessing/encoding methods on AST-based code representation and subsequent code-related tasks. Our study provides future researchers with detailed guidance on how to select solutions at each stage to fully exploit AST.",
        "Publication date": "1 December, 2023",
        "Link": "https://arxiv.org/pdf/2312.00413"
    },
    {
        "ID": 355,
        "Title": "Linear Matching of JavaScript Regular Expressions",
        "Authors": [
            "AurÃ¨le BarriÃ¨re",
            "ClÃ©ment Pit-Claudel"
        ],
        "Abstract": "Modern regex languages have strayed far from well-understood traditional regular expressions: they include features that fundamentally transform the matching problem. In exchange for these features, modern regex engines at times suffer from exponential complexity blowups, a frequent source of denial-of-service vulnerabilities in JavaScript applications. Worse, regex semantics differ across languages, and the impact of these divergences on algorithmic design and worst-case matching complexity has seldom been investigated.\n  This paper provides a novel perspective on JavaScript's regex semantics by identifying a larger-than-previously-understood subset of the language that can be matched with linear time guarantees. In the process, we discover several cases where state-of-the-art algorithms were either wrong (semantically incorrect), inefficient (suffering from superlinear complexity) or excessively restrictive (assuming certain features could not be matched linearly). We introduce novel algorithms to restore correctness and linear complexity. We further advance the state-of-the-art in linear regex matching by presenting the first nonbacktracking algorithms for matching lookarounds in linear time: one supporting captureless lookbehinds in any regex language, and another leveraging a JavaScript property to support unrestricted lookaheads and lookbehinds. Finally, we describe new time and space complexity tradeoffs for regex engines. All of our algorithms are practical: we validated them in a prototype implementation, and some have also been merged in the V8 JavaScript implementation used in Chrome and Node.js.",
        "Publication date": "23 July, 2024",
        "Link": "https://arxiv.org/pdf/2311.17620"
    },
    {
        "ID": 356,
        "Title": "Ascle: A Python Natural Language Processing Toolkit for Medical Text Generation",
        "Authors": [
            "Rui Yang",
            "Qingcheng Zeng",
            "Keen You",
            "Yujie Qiao",
            "Lucas Huang",
            "Chia-Chun Hsieh",
            "Benjamin Rosand",
            "Jeremy Goldwasser",
            "Amisha D Dave",
            "Tiarnan D. L. Keenan",
            "Emily Y Chew",
            "Dragomir Radev",
            "Zhiyong Lu",
            "Hua Xu",
            "Qingyu Chen",
            "Irene Li"
        ],
        "Abstract": "This study introduces Ascle, a pioneering natural language processing (NLP) toolkit designed for medical text generation. Ascle is tailored for biomedical researchers and healthcare professionals with an easy-to-use, all-in-one solution that requires minimal programming expertise. For the first time, Ascle evaluates and provides interfaces for the latest pre-trained language models, encompassing four advanced and challenging generative functions: question-answering, text summarization, text simplification, and machine translation. In addition, Ascle integrates 12 essential NLP functions, along with query and search capabilities for clinical databases. The toolkit, its models, and associated data are publicly available via https://github.com/Yale-LILY/MedGen.",
        "Publication date": "9 December, 2023",
        "Link": "https://arxiv.org/pdf/2311.16588"
    },
    {
        "ID": 357,
        "Title": "Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding",
        "Authors": [
            "Zhihao Yuan",
            "Jinke Ren",
            "Chun-Mei Feng",
            "Hengshuang Zhao",
            "Shuguang Cui",
            "Zhen Li"
        ],
        "Abstract": "3D Visual Grounding (3DVG) aims at localizing 3D object based on textual descriptions. Conventional supervised methods for 3DVG often necessitate extensive annotations and a predefined vocabulary, which can be restrictive. To address this issue, we propose a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs). Our approach begins with a unique dialog-based method, engaging with LLMs to establish a foundational understanding of zero-shot 3DVG. Building on this, we design a visual program that consists of three types of modules, i.e., view-independent, view-dependent, and functional modules. These modules, specifically tailored for 3D scenarios, work collaboratively to perform complex reasoning and inference. Furthermore, we develop an innovative language-object correlation module to extend the scope of existing 3D object detectors into open-vocabulary scenarios. Extensive experiments demonstrate that our zero-shot approach can outperform some supervised baselines, marking a significant stride towards effective 3DVG.",
        "Publication date": "23 March, 2024",
        "Link": "https://arxiv.org/pdf/2311.15383"
    },
    {
        "ID": 358,
        "Title": "Untargeted Code Authorship Evasion with Seq2Seq Transformation",
        "Authors": [
            "Soohyeon Choi",
            "Rhongho Jang",
            "DaeHun Nyang",
            "David Mohaisen"
        ],
        "Abstract": "Code authorship attribution is the problem of identifying authors of programming language codes through the stylistic features in their codes, a topic that recently witnessed significant interest with outstanding performance. In this work, we present SCAE, a code authorship obfuscation technique that leverages a Seq2Seq code transformer called StructCoder. SCAE customizes StructCoder, a system designed initially for function-level code translation from one language to another (e.g., Java to C#), using transfer learning. SCAE improved the efficiency at a slight accuracy degradation compared to existing work. We also reduced the processing time by about 68% while maintaining an 85% transformation success rate and up to 95.77% evasion success rate in the untargeted setting.",
        "Publication date": "26 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.15366"
    },
    {
        "ID": 359,
        "Title": "Code Generation Based Grading: Evaluating an Auto-grading Mechanism for \"Explain-in-Plain-English\" Questions",
        "Authors": [
            "David H. Smith IV",
            "Craig Zilles"
        ],
        "Abstract": "Comprehending and elucidating the purpose of code is often cited as being a key learning objective within introductory programming courses. To address this objective ``Explain-in-Plain-English'' questions, in which students are shown a segment of code and asked to provide an abstract description of the code's purpose, have been adopted. However, given EiPE questions require a natural language response, they often require manual grading which is time-consuming for course staff and delays feedback for students. With the advent of large language models (LLMs) capable of generating code, responses to EiPE questions can be used to generate code segments, the correctness of which can then be easily verified using test cases. We refer to this approach as \"Code Generation Based Grading\" (CGBG) and in this paper we explore its agreement with human graders using EiPE responses from past exams in an introductory programming course taught in Python. Overall, we find that CGBG achieves moderate agreement with human graders with the primary area of disagreement being its leniency with respect to low-level and line-by-line descriptions of code.",
        "Publication date": "24 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.14903"
    },
    {
        "ID": 360,
        "Title": "Anyone Can Code: Algorithmic Thinking",
        "Authors": [
            "Ali Arya"
        ],
        "Abstract": "As the second book in the Anyone Can Code series, Algorithmic Thinking focuses on the logic behind computer programming and software design. With a data-centred approach, it starts with simple algorithms that work on simple data items and advances to more complex ones covering data structures and classes. Examples are given in C/C++ and Python and use both plain text and graphics applications to illustrate the concepts in different languages and forms. With the advances in artificial intelligence and automated code generators, it is essential to learn about the logic of what a code needs to do, not just how to write the code. Anyone Can Code: Algorithmic Thinking is suitable for anyone who aims to improve their programming skills and go beyond the simple craft of programming, stepping into the world of algorithm design.",
        "Publication date": "23 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.14186"
    },
    {
        "ID": 361,
        "Title": "CNL2ASP: converting controlled natural language sentences into ASP",
        "Authors": [
            "Simone Caruso",
            "Carmine Dodaro",
            "Marco Maratea",
            "Marco Mochi",
            "Francesco Riccio"
        ],
        "Abstract": "Answer Set Programming (ASP) is a popular declarative programming language for solving hard combinatorial problems. Although ASP has gained widespread acceptance in academic and industrial contexts, there are certain user groups who may find it more advantageous to employ a higher-level language that closely resembles natural language when specifying ASP programs. In this paper, we propose a novel tool, called CNL2ASP, for translating English sentences expressed in a controlled natural language (CNL) form into ASP. In particular, we first provide a definition of the type of sentences allowed by our CNL and their translation as ASP rules, and then exemplify the usage of the CNL for the specification of both synthetic and real-world combinatorial problems. Finally, we report the results of an experimental analysis conducted on the real-world problems to compare the performance of automatically generated encodings with the ones written by ASP practitioners, showing that our tool can obtain satisfactory performance on these benchmarks. Under consideration in Theory and Practice of Logic Programming (TPLP).",
        "Publication date": "17 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.10505"
    },
    {
        "ID": 362,
        "Title": "Characterizing Tradeoffs in Language Model Decoding with Informational Interpretations",
        "Authors": [
            "Chung-Ching Chang",
            "William W. Cohen",
            "Yun-Hsuan Sung"
        ],
        "Abstract": "We propose a theoretical framework for formulating language model decoder algorithms with dynamic programming and information theory. With dynamic programming, we lift the design of decoder algorithms from the logit space to the action-state value function space, and show that the decoding algorithms are consequences of optimizing the action-state value functions. Each component in the action-state value function space has an information theoretical interpretation. With the lifting and interpretation, it becomes evident what the decoder algorithm is optimized for, and hence facilitating the arbitration of the tradeoffs in sensibleness, diversity, and attribution.",
        "Publication date": "16 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.10083"
    },
    {
        "ID": 363,
        "Title": "SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models",
        "Authors": [
            "Shicheng Liu",
            "Jialiang Xu",
            "Wesley Tjangnaka",
            "Sina J. Semnani",
            "Chen Jie Yu",
            "Monica S. Lam"
        ],
        "Abstract": "While most conversational agents are grounded on either free-text or structured knowledge, many knowledge corpora consist of hybrid sources. This paper presents the first conversational agent that supports the full generality of hybrid data access for large knowledge corpora, through a language we developed called SUQL (Structured and Unstructured Query Language). Specifically, SUQL extends SQL with free-text primitives (summary and answer), so information retrieval can be composed with structured data accesses arbitrarily in a formal, succinct, precise, and interpretable notation. With SUQL, we propose the first semantic parser, an LLM with in-context learning, that can handle hybrid data sources.\n  Our in-context learning-based approach, when applied to the HybridQA dataset, comes within 8.9% exact match and 7.1% F1 of the SOTA, which was trained on 62K data samples. More significantly, unlike previous approaches, our technique is applicable to large databases and free-text corpora. We introduce a dataset consisting of crowdsourced questions and conversations on Yelp, a large, real restaurant knowledge base with structured and unstructured data. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 90.3% of the time, compared to 63.4% for a baseline based on linearization.",
        "Publication date": "13 March, 2024",
        "Link": "https://arxiv.org/pdf/2311.09818"
    },
    {
        "ID": 364,
        "Title": "Verification of a Rust Implementation of Knuth's Dancing Links using ACL2",
        "Authors": [
            "David S. Hardin"
        ],
        "Abstract": "Dancing Links connotes an optimization to a circular doubly-linked list data structure implementation which provides for fast list element removal and restoration.  The Dancing Links optimization is used primarily in fast algorithms to find exact covers, and has been popularized by Knuth  in Volume 4B of his seminal series The Art of Computer Programming. We describe an implementation of the Dancing Links optimization in the Rust programming language, as well as its formal verification using the ACL2 theorem prover.  Rust has garnered significant endorsement in the past few years as a modern, memory-safe successor to C/C++ at companies such as Amazon, Google, and Microsoft, and is being integrated into both the Linux and Windows operating system kernels. Our interest in Rust stems from its potential as a hardware/software co-assurance language, with application to critical systems.  We have crafted a Rust subset, inspired by Russinoff's Restricted Algorithmic C (RAC), which we have imaginatively named Restricted Algorithmic Rust, or RAR.  In previous work, we described our initial implementation of a RAR toolchain, wherein we simply transpile the RAR source into RAC.  By so doing,  we leverage a number of existing hardware/software co-assurance tools with a minimum investment of time and effort.  In this paper, we describe the RAR Rust subset, describe our improved prototype RAR toolchain, and detail the design and verification of a circular doubly-linked list data structure employing the Dancing Links optimization in RAR, with full proofs of functional correctness accomplished using the ACL2 theorem prover.",
        "Publication date": "15 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.08862"
    },
    {
        "ID": 365,
        "Title": "Exploring Multi-Programming-Language Commits and Their Impacts on Software Quality: An Empirical Study on Apache Projects",
        "Authors": [
            "Zengyang Li",
            "Xiaoxiao Qi",
            "Qinyi Yu",
            "Peng Liang",
            "Ran Mo",
            "Chen Yang"
        ],
        "Abstract": "Context: Modern software systems (e.g., Apache Spark) are usually written in multiple programming languages (PLs). There is little understanding on the phenomenon of multi-programming-language commits (MPLCs), which involve modified source files written in multiple PLs. Objective: This work aims to explore MPLCs and their impacts on development difficulty and software quality. Methods: We performed an empirical study on eighteen non-trivial Apache projects with 197,566 commits. Results: (1) the most commonly used PL combination consists of all the four PLs, i.e., C/C++, Java, JavaScript, and Python; (2) 9% of the commits from all the projects are MPLCs, and the proportion of MPLCs in 83% of the projects goes to a relatively stable level; (3) more than 90% of the MPLCs from all the projects involve source files in two PLs; (4) the change complexity of MPLCs is significantly higher than that of non-MPLCs; (5) issues fixed in MPLCs take significantly longer to be resolved than issues fixed in non-MPLCs in 89% of the projects; (6) MPLCs do not show significant effects on issue reopen; (7) source files undergoing MPLCs tend to be more bug-prone; and (8) MPLCs introduce more bugs than non-MPLCs. Conclusions: MPLCs are related to increased development difficulty and decreased software quality.",
        "Publication date": "12 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.08424"
    },
    {
        "ID": 366,
        "Title": "GT4Py: High Performance Stencils for Weather and Climate Applications using Python",
        "Authors": [
            "Enrique G. Paredes",
            "Linus Groner",
            "Stefano Ubbiali",
            "Hannes Vogt",
            "Alberto Madonna",
            "Kean Mariotti",
            "Felipe Cruz",
            "Lucas Benedicic",
            "Mauro Bianco",
            "Joost VandeVondele",
            "Thomas C. Schulthess"
        ],
        "Abstract": "All major weather and climate applications are currently developed using languages such as Fortran or C++. This is typical in the domain of high performance computing (HPC), where efficient execution is an important concern. Unfortunately, this approach leads to implementations that intermix optimizations for specific hardware architectures with the high-level numerical methods that are typical for the domain. This leads to code that is verbose, difficult to extend and maintain, and difficult to port to different hardware architectures. Here, we propose a different strategy based on GT4Py (GridTools for Python). GT4Py is a Python framework to write weather and climate applications that includes a high-level embedded domain specific language (DSL) to write stencil computations. The toolchain integrated in GT4Py enables automatic code-generation,to obtain the performance of state-of-the-art C++ and CUDA implementations. The separation of concerns between the mathematical definitions and the actual implementations allows for performance portability of the computations on a wide range of computing architectures, while being embedded in Python allows easy access to the tools of the Python ecosystem to enhance the productivity of the scientists and facilitate integration in complex workflows. Here, the initial release of GT4Py is described, providing an overview of the current state of the framework and performance results showing how GT4Py can outperform pure Python implementations by orders of magnitude.",
        "Publication date": "14 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.08322"
    },
    {
        "ID": 367,
        "Title": "TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree Transformation",
        "Authors": [
            "Zixiang Xian",
            "Rubing Huang",
            "Dave Towey",
            "Chunrong Fang",
            "Zhenyu Chen"
        ],
        "Abstract": "Artificial intelligence (AI) has revolutionized software engineering (SE) by enhancing software development efficiency. The advent of pre-trained models (PTMs) leveraging transfer learning has significantly advanced AI for SE. However, existing PTMs that operate on individual code tokens suffer from several limitations: They are costly to train and fine-tune; and they rely heavily on labeled data for fine-tuning on task-specific datasets. In this paper, we present TransformCode, a novel framework that learns code embeddings in a contrastive learning manner. Our framework is encoder-agnostic and language-agnostic, which means that it can leverage any encoder model and handle any programming language. We also propose a novel data-augmentation technique called abstract syntax tree (AST) transformation, which applies syntactic and semantic transformations to the original code snippets, to generate more diverse and robust samples for contrastive learning. Our framework has several advantages over existing methods: (1) It is flexible and adaptable, because it can easily be extended to other downstream tasks that require code representation (such as code-clone detection and classification); (2) it is efficient and scalable, because it does not require a large model or a large amount of training data, and it can support any programming language; (3) it is not limited to unsupervised learning, but can also be applied to some supervised learning tasks by incorporating task-specific labels or objectives; and (4) it can also adjust the number of encoder parameters based on computing resources. We evaluate our framework on several code-related tasks, and demonstrate its effectiveness and superiority over the state-of-the-art methods such as SourcererCC, Code2vec, and InferCode.",
        "Publication date": "23 April, 2024",
        "Link": "https://arxiv.org/pdf/2311.08157"
    },
    {
        "ID": 368,
        "Title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code",
        "Authors": [
            "Ziyin Zhang",
            "Chaoyu Chen",
            "Bingchang Liu",
            "Cong Liao",
            "Zi Gong",
            "Hang Yu",
            "Jianguo Li",
            "Rui Wang"
        ],
        "Abstract": "In this work we systematically review the recent advancements in software engineering with language models, covering 70+ models, 40+ evaluation tasks, 180+ datasets, and 900 related works. Unlike previous works, we integrate software engineering (SE) with natural language processing (NLP) by discussing the perspectives of both sides: SE applies language models for development automation, while NLP adopts SE tasks for language model evaluation. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also go beyond programming and review LLMs' application in other software engineering activities including requirement engineering, testing, deployment, and operations in an endeavor to provide a global view of NLP in SE, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.",
        "Publication date": "26 June, 2024",
        "Link": "https://arxiv.org/pdf/2311.07989"
    },
    {
        "ID": 369,
        "Title": "Creation of a CS1 Course with Modern C++ Principles",
        "Authors": [
            "Ryan E. Dougherty"
        ],
        "Abstract": "Best practices in programming need to be emphasized in a CS1 course as bad student habits persist if not reinforced well. The C++ programming language, although a relatively old language, has been regularly updated with new versions since 2011, on the pace of once every three years. Each new version contains important features that make the C++ language more complex for backwards compatibility, but often introduce new features to make common use cases simpler to implement. This poster contains experiences in designing a CS1 course that uses the C++ programming language that incorporates ``modern'' versions of the language from the start, as well as recent conferences about the language. Our goals were to prevent many common bad habits among C++ programmers.",
        "Publication date": "13 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.07807"
    },
    {
        "ID": 370,
        "Title": "Sidekick compilation with xDSL",
        "Authors": [
            "Mathieu Fehr",
            "Michel Weber",
            "Christian Ulmann",
            "Alexandre Lopoukhine",
            "Martin LÃ¼cke",
            "ThÃ©o Degioanni",
            "Michel Steuwer",
            "Tobias Grosser"
        ],
        "Abstract": "Traditionally, compiler researchers either conduct experiments within an existing production compiler or develop their own prototype compiler; both options come with trade-offs. On one hand, prototyping in a production compiler can be cumbersome, as they are often optimized for program compilation speed at the expense of software simplicity and development speed. On the other hand, the transition from a prototype compiler to production requires significant engineering work. To bridge this gap, we introduce the concept of sidekick compiler frameworks, an approach that uses multiple frameworks that interoperate with each other by leveraging textual interchange formats and declarative descriptions of abstractions. Each such compiler framework is specialized for specific use cases, such as performance or prototyping. Abstractions are by design shared across frameworks, simplifying the transition from prototyping to production. We demonstrate this idea with xDSL, a sidekick for MLIR focused on prototyping and teaching. xDSL interoperates with MLIR through a shared textual IR and the exchange of IRs through an IR Definition Language. The benefits of sidekick compiler frameworks are evaluated by showing on three use cases how xDSL impacts their development: teaching, DSL compilation, and rewrite system prototyping. We also investigate the trade-offs that xDSL offers, and demonstrate how we simplify the transition between frameworks using the IRDL dialect. With sidekick compilation, we envision a future in which engineers minimize the cost of development by choosing a framework built for their immediate needs, and later transitioning to production with minimal overhead.",
        "Publication date": "16 June, 2024",
        "Link": "https://arxiv.org/pdf/2311.07422"
    },
    {
        "ID": 371,
        "Title": "Wasm SpecTec: Engineering a Formal Language Standard",
        "Authors": [
            "Joachim Breitner",
            "Philippa Gardner",
            "Jaehyun Lee",
            "Sam Lindley",
            "Matija Pretnar",
            "Xiaojia Rao",
            "Andreas Rossberg",
            "Sukyoung Ryu",
            "Wonho Shin",
            "Conrad Watt",
            "Dongjun Youn"
        ],
        "Abstract": "WebAssembly (Wasm) is a low-level bytecode language and virtual machine, intended as a compilation target for a wide range of programming languages, which is seeing increasing adoption across diverse ecosystems. As a young technology, Wasm continues to evolve -- it reached version 2.0 last year and another major update is expected soon.\n  For a new feature to be standardised in Wasm, four key artefacts must be presented: a formal (mathematical) specification of the feature, an accompanying prose pseudocode description, an implementation in the official reference interpreter, and a suite of unit tests. This rigorous process helps to avoid errors in the design and implementation of new Wasm features, and Wasm's distinctive formal specification in particular has facilitated machine-checked proofs of various correctness properties for the language. However, manually crafting all of these artefacts requires expert knowledge combined with repetitive and tedious labor, which is a burden on the language's standardization process and authoring of the specification.\n  This paper presents Wasm SpecTec, a technology to express the formal specification of Wasm through a domain-specific language. This DSL allows all of Wasm's currently handwritten specification artefacts to be error-checked and generated automatically from a single source of truth, and is designed to be easy to write, read, compare, and review. We believe that Wasm SpecTec's automation and meta-level error checking will significantly ease the current burden of the language's specification authors. We demonstrate the current capabilities of Wasm SpecTec by showcasing its proficiency in generating various artefacts, and describe our work towards replacing the manually written official Wasm specification document with specifications generated by Wasm SpecTec.",
        "Publication date": "13 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.07223"
    },
    {
        "ID": 372,
        "Title": "A Compiler from Array Programs to Vectorized Homomorphic Encryption",
        "Authors": [
            "Rolph Recto",
            "Andrew C. Myers"
        ],
        "Abstract": "Homomorphic encryption (HE) is a practical approach to secure computation over encrypted data. However, writing programs with efficient HE implementations remains the purview of experts. A difficult barrier for programmability is that efficiency requires operations to be vectorized in inobvious ways, forcing efficient HE programs to manipulate ciphertexts with complex data layouts and to interleave computations with data movement primitives.\n  We present Viaduct-HE, a compiler generates efficient vectorized HE programs. Viaduct-HE can generate both the operations and complex data layouts required for efficient HE programs. The source language of Viaduct-HE is array-oriented, enabling the compiler to have a simple representation of possible vectorization schedules. With such a representation, the compiler searches the space of possible vectorization schedules and finds those with efficient data layouts. After finding a vectorization schedule, Viaduct-HE further optimizes HE programs through term rewriting. The compiler has extension points to customize the exploration of vectorization schedules, to customize the cost model for HE programs, and to add back ends for new HE libraries.\n  Our evaluation of the prototype Viaduct-HE compiler shows that it produces efficient vectorized HE programs with sophisticated data layouts and optimizations comparable to those designed by experts.",
        "Publication date": "10 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.06142"
    },
    {
        "ID": 373,
        "Title": "Statistical Learning of Conjunction Data Messages Through a Bayesian Non-Homogeneous Poisson Process",
        "Authors": [
            "Marta GuimarÃ£es",
            "ClÃ¡udia Soares",
            "Chiara Manfletti"
        ],
        "Abstract": "Current approaches for collision avoidance and space traffic management face many challenges, mainly due to the continuous increase in the number of objects in orbit and the lack of scalable and automated solutions. To avoid catastrophic incidents, satellite owners/operators must be aware of their assets' collision risk to decide whether a collision avoidance manoeuvre needs to be performed. This process is typically executed through the use of warnings issued in the form of CDMs which contain information about the event, such as the expected TCA and the probability of collision. Our previous work presented a statistical learning model that allowed us to answer two important questions: (1) Will any new conjunctions be issued in the next specified time interval? (2) When and with what uncertainty will the next CDM arrive? However, the model was based on an empirical Bayes homogeneous Poisson process, which assumes that the arrival rates of CDMs are constant over time. In fact, the rate at which the CDMs are issued depends on the behaviour of the objects as well as on the screening process performed by third parties. Thus, in this work, we extend the previous study and propose a Bayesian non-homogeneous Poisson process implemented with high precision using a Probabilistic Programming Language to fully describe the underlying phenomena. We compare the proposed solution with a baseline model to demonstrate the added value of our approach. The results show that this problem can be successfully modelled by our Bayesian non-homogeneous Poisson Process with greater accuracy, contributing to the development of automated collision avoidance systems and helping operators react timely but sparingly with satellite manoeuvres.",
        "Publication date": "15 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.05426"
    },
    {
        "ID": 374,
        "Title": "An approach to performance portability through generic programming",
        "Authors": [
            "Andreas Hadjigeorgiou",
            "Christodoulos Stylianou",
            "Michele Weiland",
            "Dirk Jacob Verschuur",
            "Jacob Finkenrath"
        ],
        "Abstract": "The expanding hardware diversity in high performance computing adds enormous complexity to scientific software development. Developers who aim to write maintainable software have two options: 1) To use a so-called data locality abstraction that handles portability internally, thereby, performance-productivity becomes a trade off. Such abstractions usually come in the form of libraries, domain-specific languages, and run-time systems. 2) To use generic programming where performance, productivity and portability are subject to software design. In the direction of the second, this work describes a design approach that allows the integration of low-level and verbose programming tools into high-level generic algorithms based on template meta-programming in C++. This enables the development of performance-portable applications targeting host-device computer architectures, such as CPUs and GPUs. With a suitable design in place, the extensibility of generic algorithms to new hardware becomes a well defined procedure that can be developed in isolation from other parts of the code. That allows scientific software to be maintainable and efficient in a period of diversifying hardware in HPC. As proof of concept, a finite-difference modelling algorithm for the acoustic wave equation is developed and benchmarked using roofline model analysis on Intel Xeon Gold 6248 CPU, Nvidia Tesla V100 GPU, and AMD MI100 GPU.",
        "Publication date": "8 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.05038"
    },
    {
        "ID": 375,
        "Title": "GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs",
        "Authors": [
            "Zhenfang Chen",
            "Rui Sun",
            "Wenjun Liu",
            "Yining Hong",
            "Chuang Gan"
        ],
        "Abstract": "Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate language into module descriptions, thus achieving strong visual reasoning results while maintaining the model's transparency and efficiency. However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. We propose generative neuro-symbolic visual reasoning by growing and reusing modules. Specifically, our model consists of three unique stages, module initialization, module generation, and module execution. First, given a vision-language task, we adopt LLMs to examine whether we could reuse and grow over established modules to handle this new task. If not, we initialize a new module needed by the task and specify the inputs and outputs of this new module. After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements. In order to get a better sense of the new module's ability, we treat few-shot training examples as test cases to see if our new module could pass these cases. If yes, the new module is added to the module library for future reuse. Finally, we evaluate the performance of our model on the testing set by executing the parsed programs with the newly made visual modules to get the results. We find the proposed model possesses several advantages. First, it performs competitively on standard tasks like visual question answering and referring expression comprehension; Second, the modules learned from one task can be seamlessly transferred to new tasks; Last but not least, it is able to adapt to new visual reasoning tasks by observing a few training examples and reusing modules.",
        "Publication date": "8 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.04901"
    },
    {
        "ID": 376,
        "Title": "Higher-Order Bayesian Networks, Exactly (Extended version)",
        "Authors": [
            "Claudia Faggian",
            "Daniele Pautasso",
            "Gabriele Vanoni"
        ],
        "Abstract": "Bayesian networks (BNs) are graphical \\emph{first-order} probabilistic models that allow for a compact representation of large probability distributions, and for efficient inference, both exact and approximate. We introduce a \\emph{higher-order} programming language -- in the idealized form of a $Î»$-calculus -- which we prove \\emph{sound and complete} w.r.t. BNs: each BN can be encoded as a term, and conversely each (possibly higher-order and recursive) program of ground type \\emph{compiles} into a BN. The language allows for the specification of recursive probability models and hierarchical structures. Moreover, we provide a \\emph{compositional} and \\emph{cost-aware} semantics which is based on factors, the standard mathematical tool used in Bayesian inference. Our results rely on advanced techniques rooted into linear logic, intersection types, rewriting theory, and Girard's geometry of interaction, which are here combined in a novel way.",
        "Publication date": "11 December, 2023",
        "Link": "https://arxiv.org/pdf/2311.04651"
    },
    {
        "ID": 377,
        "Title": "Bilingual Corpus Mining and Multistage Fine-Tuning for Improving Machine Translation of Lecture Transcripts",
        "Authors": [
            "Haiyue Song",
            "Raj Dabre",
            "Chenhui Chu",
            "Atsushi Fujita",
            "Sadao Kurohashi"
        ],
        "Abstract": "Lecture transcript translation helps learners understand online courses, however, building a high-quality lecture machine translation system lacks publicly available parallel corpora. To address this, we examine a framework for parallel corpus mining, which provides a quick and effective way to mine a parallel corpus from publicly available lectures on Coursera. To create the parallel corpora, we propose a dynamic programming based sentence alignment algorithm which leverages the cosine similarity of machine-translated sentences. The sentence alignment F1 score reaches 96%, which is higher than using the BERTScore, LASER, or sentBERT methods. For both English--Japanese and English--Chinese lecture translations, we extracted parallel corpora of approximately 50,000 lines and created development and test sets through manual filtering for benchmarking translation performance. Through machine translation experiments, we show that the mined corpora enhance the quality of lecture transcript translation when used in conjunction with out-of-domain parallel corpora via multistage fine-tuning. Furthermore, this study also suggests guidelines for gathering and cleaning corpora, mining parallel sentences, cleaning noise in the mined data, and creating high-quality evaluation splits. For the sake of reproducibility, we have released the corpora as well as the code to create them. The dataset is available at https://github.com/shyyhs/CourseraParallelCorpusMining.",
        "Publication date": "6 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.03696"
    },
    {
        "ID": 378,
        "Title": "Enhancing Performance Monitoring in C/C++ Programs with EDPM: A Domain-Specific Language for Performance Monitoring",
        "Authors": [
            "David Weisskopf Holmqvist",
            "Suejb Memeti"
        ],
        "Abstract": "The utilization of performance monitoring probes is a valuable tool for programmers to gather performance data. However, the manual insertion of these probes can result in an increase in code size, code obfuscation, and an added burden of learning different APIs associated with performance monitoring tools. To mitigate these issues, EDPM, an embedded domain-specific language, was developed to provide a higher level of abstraction for annotating regions of code that require instrumentation in C and C++ programs. This paper presents the design and implementation of EDPM and compares it to the well-known tool PAPI, in terms of required lines of code, flexibility in configuring regions, and performance overhead. The results of this study demonstrate that EDPM is a low-resolution profiling tool that offers a reduction in required lines of code and enables programmers to express various configurations of regions. Furthermore, the design of EDPM is such that its pragmas are ignored by the standard compiler, allowing for seamless integration into existing software processes without disrupting build systems or increasing the size of the executable. Additionally, the design of the EDPM pre-compiler allows for the extension of available performance counters while maintaining a high level of abstraction for programmers. Therefore, EDPM offers a promising solution to simplify and optimize performance monitoring in C and C++ programs.",
        "Publication date": "6 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.03535"
    },
    {
        "ID": 379,
        "Title": "Can ChatGPT support software verification?",
        "Authors": [
            "Christian JanÃen",
            "Cedric Richter",
            "Heike Wehrheim"
        ],
        "Abstract": "Large language models have become increasingly effective in software engineering tasks such as code generation, debugging and repair. Language models like ChatGPT can not only generate code, but also explain its inner workings and in particular its correctness. This raises the question whether we can utilize ChatGPT to support formal software verification.\n  In this paper, we take some first steps towards answering this question. More specifically, we investigate whether ChatGPT can generate loop invariants. Loop invariant generation is a core task in software verification, and the generation of valid and useful invariants would likely help formal verifiers. To provide some first evidence on this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants. We check validity and usefulness of the generated invariants by passing them to two verifiers, Frama-C and CPAchecker. Our evaluation shows that ChatGPT is able to produce valid and useful invariants allowing Frama-C to verify tasks that it could not solve before. Based on our initial insights, we propose ways of combining ChatGPT (or large language models in general) and software verifiers, and discuss current limitations and open issues.",
        "Publication date": "4 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.02433"
    },
    {
        "ID": 380,
        "Title": "Relax: Composable Abstractions for End-to-End Dynamic Machine Learning",
        "Authors": [
            "Ruihang Lai",
            "Junru Shao",
            "Siyuan Feng",
            "Steven S. Lyubomirsky",
            "Bohan Hou",
            "Wuwei Lin",
            "Zihao Ye",
            "Hongyi Jin",
            "Yuchen Jin",
            "Jiawei Liu",
            "Lesheng Jin",
            "Yaxing Cai",
            "Ziheng Jiang",
            "Yong Wu",
            "Sunghyun Park",
            "Prakalp Srivastava",
            "Jared G. Roesch",
            "Todd C. Mowry",
            "Tianqi Chen"
        ],
        "Abstract": "Dynamic shape computations have become critical in modern machine learning workloads, especially in emerging large language models. The success of these models has driven demand for deploying them to a diverse set of backend environments. In this paper, we present Relax, a compiler abstraction for optimizing end-to-end dynamic machine learning workloads. Relax introduces first-class symbolic shape annotations to track dynamic shape computations globally across the program. It also introduces a cross-level abstraction that encapsulates computational graphs, loop-level tensor programs, and library calls in a single representation to enable cross-level optimizations. We build an end-to-end compilation framework using the proposed approach to optimize dynamic shape models. Experimental results on large language models show that Relax delivers performance competitive with state-of-the-art hand-optimized systems across platforms and enables deployment of emerging dynamic models to a broader set of environments, including mobile phones, embedded devices, and web browsers.",
        "Publication date": "1 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.02103"
    },
    {
        "ID": 381,
        "Title": "Implementation and Synthesis of Math Library Functions",
        "Authors": [
            "Ian Briggs",
            "Yash Lad",
            "Pavel Panchekha"
        ],
        "Abstract": "Achieving speed and accuracy for math library functions like exp, sin, and log is difficult. This is because low-level implementation languages like C do not help math library developers catch mathematical errors, build implementations incrementally, or separate high-level and low-level decision making. This ultimately puts development of such functions out of reach for all but the most experienced experts. To address this, we introduce MegaLibm, a domain-specific language for implementing, testing, and tuning math library implementations. MegaLibm is safe, modular, and tunable. Implementations in MegaLibm can automatically detect mathematical mistakes like sign flips via semantic wellformedness checks, and components like range reductions can be implemented in a modular, composable way, simplifying implementations. Once the high-level algorithm is done, tuning parameters like working precisions and evaluation schemes can be adjusted through orthogonal tuning parameters to achieve the desired speed and accuracy. MegaLibm also enables math library developers to work interactively, compiling, testing, and tuning their implementations and invoking tools like Sollya and type-directed synthesis to complete components and synthesize entire implementations. MegaLibm can express 8 state-of-the-art math library implementations with comparable speed and accuracy to the original C code, and can synthesize 5 variations and 3 from-scratch implementations with minimal guidance.",
        "Publication date": "2 November, 2023",
        "Link": "https://arxiv.org/pdf/2311.01515"
    },
    {
        "ID": 382,
        "Title": "SALLM: Security Assessment of Generated Code",
        "Authors": [
            "Mohammed Latif Siddiq",
            "Joanna C. S. Santos",
            "Sajith Devareddy",
            "Anna Muller"
        ],
        "Abstract": "With the growing popularity of Large Language Models (LLMs) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate LLMs do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. Second, existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations. Therefore, in this paper, we described SALLM, a framework to benchmark LLMs' abilities to generate secure code systematically. This framework has three major components: a novel dataset of security-centric Python prompts, configurable assessment techniques to evaluate the generated code, and novel metrics to evaluate the models' performance from the perspective of secure code generation.",
        "Publication date": "4 September, 2024",
        "Link": "https://arxiv.org/pdf/2311.00889"
    },
    {
        "ID": 383,
        "Title": "Data Augmentation for Code Translation with Comparable Corpora and Multiple References",
        "Authors": [
            "Yiqing Xie",
            "Atharva Naik",
            "Daniel Fried",
            "Carolyn Rose"
        ],
        "Abstract": "One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of translations by execution. The code is available at https://github.com/Veronicium/CMTrans.",
        "Publication date": "4 October, 2024",
        "Link": "https://arxiv.org/pdf/2311.00317"
    },
    {
        "ID": 384,
        "Title": "Students' Perspective on AI Code Completion: Benefits and Challenges",
        "Authors": [
            "Wannita Takerngsaksiri",
            "Cleshan Warusavitarne",
            "Christian Yaacoub",
            "Matthew Hee Keng Hou",
            "Chakkrit Tantithamthavorn"
        ],
        "Abstract": "AI Code Completion (e.g., GitHub's Copilot) has revolutionized how computer science students interact with programming languages. However, AI code completion has been studied from the developers' perspectives, not the students' perspectives who represent the future generation of our digital world. In this paper, we investigated the benefits, challenges, and expectations of AI code completion from students' perspectives. To facilitate the study, we first developed an open-source Visual Studio Code Extension tool AutoAurora, powered by a state-of-the-art large language model StarCoder, as an AI code completion research instrument. Next, we conduct an interview study with ten student participants and apply grounded theory to help analyze insightful findings regarding the benefits, challenges, and expectations of students on AI code completion. Our findings show that AI code completion enhanced students' productivity and efficiency by providing correct syntax suggestions, offering alternative solutions, and functioning as a coding tutor. However, the over-reliance on AI code completion may lead to a surface-level understanding of programming concepts, diminishing problem-solving skills and restricting creativity. In the future, AI code completion should be explainable and provide best coding practices to enhance the education process.",
        "Publication date": "31 May, 2024",
        "Link": "https://arxiv.org/pdf/2311.00177"
    },
    {
        "ID": 385,
        "Title": "Serverless Scheduling Policies based on Cost Analysis",
        "Authors": [
            "Giuseppe De Palma",
            "Saverio Giallorenzo",
            "Cosimo Laneve",
            "Jacopo Mauro",
            "Matteo Trentin",
            "Gianluigi Zavattaro"
        ],
        "Abstract": "Current proprietary and open-source serverless platforms follow opinionated, hardcoded scheduling policies to deploy the functions to be executed over the available workers. Such policies may decrease the performance and the security of the application due to locality issues (e.g., functions executed by workers far from the databases to be accessed). These limitations are partially overcome by the adoption of APP, a new platform-agnostic declarative language that allows serverless platforms to support multiple scheduling logics. Defining the \"right\" scheduling policy in APP is far from being a trivial task since it often requires rounds of refinement involving knowledge of the underlying infrastructure, guesswork, and empirical testing. In this paper, we start investigating how information derived from static analysis could be incorporated into APP scheduling function policies to help users select the best-performing workers at function allocation. We substantiate our proposal by presenting a pipeline able to extract cost equations from functions' code, synthesising cost expressions through the usage of off-the-shelf solvers, and extending APP allocation policies to consider this information.",
        "Publication date": "31 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.20391"
    },
    {
        "ID": 386,
        "Title": "CodeFusion: A Pre-trained Diffusion Model for Code Generation",
        "Authors": [
            "Mukul Singh",
            "JosÃ© Cambronero",
            "Sumit Gulwani",
            "Vu Le",
            "Carina Negreanu",
            "Gust Verbruggen"
        ],
        "Abstract": "Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.",
        "Publication date": "1 November, 2023",
        "Link": null
    },
    {
        "ID": 387,
        "Title": "FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language",
        "Authors": [
            "Mukul Singh",
            "JosÃ© Cambronero",
            "Sumit Gulwani",
            "Vu Le",
            "Carina Negreanu",
            "Elnaz Nouri",
            "Mohammad Raza",
            "Gust Verbruggen"
        ],
        "Abstract": "Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.",
        "Publication date": "1 November, 2023",
        "Link": null
    },
    {
        "ID": 388,
        "Title": "Demystifying Compiler Unstable Feature Usage and Impacts in the Rust Ecosystem",
        "Authors": [
            "Chenghao Li",
            "Yifei Wu",
            "Wenbo Shen",
            "Zichen Zhao",
            "Rui Chang",
            "Chengwei Liu",
            "Yang Liu",
            "Kui Ren"
        ],
        "Abstract": "Rust programming language is gaining popularity rapidly in building reliable and secure systems due to its security guarantees and outstanding performance. To provide extra functionalities, the Rust compiler introduces Rust unstable features (RUF) to extend compiler functionality, syntax, and standard library support. However, these features are unstable and may get removed, introducing compilation failures to dependent packages. Even worse, their impacts propagate through transitive dependencies, causing large-scale failures in the whole ecosystem. Although RUF is widely used in Rust, previous research has primarily concentrated on Rust code safety, with the usage and impacts of RUF from the Rust compiler remaining unexplored. Therefore, we aim to bridge this gap by systematically analyzing the RUF usage and impacts in the Rust ecosystem. We propose novel techniques for extracting RUF precisely, and to assess its impact on the entire ecosystem quantitatively, we accurately resolve package dependencies. We have analyzed the whole Rust ecosystem with 590K package versions and 140M transitive dependencies. Our study shows that the Rust ecosystem uses 1000 different RUF, and at most 44% of package versions are affected by RUF, causing compiling failures for at most 12%. To mitigate wide RUF impacts, we further design and implement a RUF-compilation-failure recovery tool that can recover up to 90% of the failure. We believe our techniques, findings, and tools can help to stabilize the Rust compiler, ultimately enhancing the security and reliability of the Rust ecosystem.",
        "Publication date": "26 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.17186"
    },
    {
        "ID": 389,
        "Title": "Diseno y Desarrollo de Prototipos Roboticos para Competencias de Futbol utilizando Motores Dynamixel",
        "Authors": [
            "Pablo Moraes",
            "Hiago Sodre",
            "Monica Rodriguez",
            "Andre Kelbouscas",
            "Jean Schuster",
            "Cristiano Schuster",
            "Ricardo Grando"
        ],
        "Abstract": "This article describes the design and development of robotic prototypes for robotic soccer competitions using Dynamixel motors. Although the prototypes are not aimed at world-class competitions, they represent a significant step in the development of sports robots. Model XL430-W250 Dynamixel motors were chosen and electronic circuits were implemented using control boards such as OpenCR and Raspberry Pi 3. A crucial component was introduced: a step-up board that charges a capacitor to create a powerful kick to the ball via anelectromagnet controlled by Arduino Nano. The programming and coordination of the prototypes was carried out using the ROS environment (Robot Operating System), which allows effective integration of movements and communication. Although the prototypes were not optimized for global competition, they underwent extensive testing, evaluating their speed and maneuverability, as well as soccer tactics in the GRSim simulator. These prototypes contribute to the further development of sports robotics and illustrate the research potential in this exciting area.",
        "Publication date": "25 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.17058"
    },
    {
        "ID": 390,
        "Title": "A Type System for Julia",
        "Authors": [
            "Benjamin Chung"
        ],
        "Abstract": "The Julia programming language was designed to fill the needs of scientific computing by combining the benefits of productivity and performance languages. Julia allows users to write untyped scripts easily without needing to worry about many implementation details, as do other productivity languages. If one just wants to get the work done-regardless of how efficient or general the program might be, such a paradigm is ideal. Simultaneously, Julia also allows library developers to write efficient generic code that can run as fast as implementations in performance languages such as C or Fortran. This combination of user-facing ease and library developer-facing performance has proven quite attractive, and the language has increasing adoption.\n  With adoption comes combinatorial challenges to correctness. Multiple dispatch -- Julia's key mechanism for abstraction -- allows many libraries to compose \"out of the box.\" However, it creates bugs where one library's requirements do not match what another provides. Typing could address this at the cost of Julia's flexibility for scripting.\n  I developed a \"best of both worlds\" solution: gradual typing for Julia. My system forms the core of a gradual type system for Julia, laying the foundation for improving the correctness of Julia programs while not getting in the way of script writers. My framework allows methods to be individually typed or untyped, allowing users to write untyped code that interacts with typed library code and vice versa. Typed methods then get a soundness guarantee that is robust in the presence of both dynamically typed code and dynamically generated definitions. I additionally describe protocols, a mechanism for typing abstraction over concrete implementation that accommodates one common pattern in Julia libraries, and describe its implementation into my typed Julia framework.",
        "Publication date": "25 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.16866"
    },
    {
        "ID": 391,
        "Title": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models",
        "Authors": [
            "Chenyuan Yang",
            "Yinlin Deng",
            "Runyu Lu",
            "Jiayi Yao",
            "Jiawei Liu",
            "Reyhaneh Jabbarvand",
            "Lingming Zhang"
        ],
        "Abstract": "Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences. Fuzzing has been studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates tests without sufficient understanding of internal compiler behaviors. Meanwhile, traditional white-box techniques, like symbolic execution, are computationally inapplicable to the giant codebase of compilers. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing.\n  To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are used as feedback to enhance the generation on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows WhiteFox can generate high-quality test programs to exercise deep optimizations, practicing up to 8X more than state-of-the-art fuzzers. WhiteFox has found 101 bugs for the DL compilers, with 92 confirmed as previously unknown and 70 fixed. WhiteFox has been acknowledged by the PyTorch team and is being incorporated into its development workflow. Beyond DL compilers, WhiteFox can also be adapted for compilers in different domains.",
        "Publication date": "4 September, 2024",
        "Link": "https://arxiv.org/pdf/2310.15991"
    },
    {
        "ID": 392,
        "Title": "SecV: Secure Code Partitioning via Multi-Language Secure Values",
        "Authors": [
            "Peterson Yuhala",
            "Pascal Felber",
            "Hugo Guiroux",
            "Jean-Pierre Lozi",
            "Alain Tchana",
            "Valerio Schiavoni",
            "GaÃ«l Thomas"
        ],
        "Abstract": "Trusted execution environments like Intel SGX provide \\emph{enclaves}, which offer strong security guarantees for applications. Running entire applications inside enclaves is possible, but this approach leads to a large trusted computing base (TCB). As such, various tools have been developed to partition programs written in languages such as C or Java into \\emph{trusted} and \\emph{untrusted} parts, which are run in and out of enclaves respectively. However, those tools depend on language-specific taint-analysis and partitioning techniques. They cannot be reused for other languages and there is thus a need for tools that transcend this language barrier.\n  We address this challenge by proposing a multi-language technique to specify sensitive code or data, as well as a multi-language tool to analyse and partition the resulting programs for trusted execution environments like Intel SGX. We leverage GraalVM's Truffle framework, which provides a language-agnostic abstract syntax tree (AST) representation for programs, to provide special AST nodes called \\emph{secure nodes} that encapsulate sensitive program information. Secure nodes can easily be embedded into the ASTs of a wide range of languages via Truffle's \\emph{polyglot API}. Our technique includes a multi-language dynamic taint tracking tool to analyse and partition applications based on our generic secure nodes. Our extensive evaluation with micro- and macro-benchmarks shows that we can use our technique for two languages (Javascript and \\python), and that partitioned programs can obtain up to $14.5\\%$ performance improvement as compared to unpartitioned versions.",
        "Publication date": "20 December, 2023",
        "Link": "https://arxiv.org/pdf/2310.15582"
    },
    {
        "ID": 393,
        "Title": "SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation",
        "Authors": [
            "Jialing Pan",
            "Adrien SadÃ©",
            "Jin Kim",
            "Eric Soriano",
            "Guillem Sole",
            "Sylvain Flamant"
        ],
        "Abstract": "With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (RoziÃ¨re et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance training efficiency in terms of time, we adopt curriculum learning strategy and use self-instruct data for efficient fine-tuning. As a result, each expert takes only 6 hours to train on one single 80Gb A100 HBM. With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76 CodeBLEU score in multi-programming language-to-Python translation, surpassing the top performance from the leaderboard by at least 3.5. This accomplishment is attributed to only 45M extra parameters with StarCoder as the backbone and 32 hours of valid training on one 80GB A100 HBM. The source code is release here: https://github.com/sade-adrien/SteloCoder.",
        "Publication date": "15 December, 2023",
        "Link": "https://arxiv.org/pdf/2310.15539"
    },
    {
        "ID": 394,
        "Title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
        "Authors": [
            "Theo X. Olausson",
            "Alex Gu",
            "Benjamin Lipkin",
            "Cedegao E. Zhang",
            "Armando Solar-Lezama",
            "Joshua B. Tenenbaum",
            "Roger Levy"
        ],
        "Abstract": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available at https://github.com/benlipkin/linc",
        "Publication date": "14 February, 2024",
        "Link": "https://arxiv.org/pdf/2310.15164"
    },
    {
        "ID": 395,
        "Title": "SUT: Active Defects Probing for Transcompiler Models",
        "Authors": [
            "Mengnan Qi",
            "Yufan Huang",
            "Maoquan Wang",
            "Yongqiang Yao",
            "Zihan Liu",
            "Bin Gu",
            "Colin Clement",
            "Neel Sundaresan"
        ],
        "Abstract": "Automatic Program translation has enormous application value and hence has been attracting significant interest from AI researchers. However, we observe that current program translation models still make elementary syntax errors, particularly, when the target language does not have syntax elements in the source language. Metrics like BLUE, CodeBLUE and computation accuracy may not expose these issues. In this paper we introduce a new metrics for programming language translation and these metrics address these basic syntax errors. We develop a novel active defects probing suite called Syntactic Unit Tests (SUT) which includes a highly interpretable evaluation harness for accuracy and test scoring. Experiments have shown that even powerful models like ChatGPT still make mistakes on these basic unit tests. Specifically, compared to previous program translation task evaluation dataset, its pass rate on our unit tests has decreased by 26.15%. Further our evaluation harness reveal syntactic element errors in which these models exhibit deficiencies.",
        "Publication date": "22 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.14209"
    },
    {
        "ID": 396,
        "Title": "A prototype software framework for transferable computational health economic models and its early application in youth mental health",
        "Authors": [
            "Matthew P Hamilton",
            "Caroline X Gao",
            "Glen Wiesner",
            "Kate M Filia",
            "Jana M Menssink",
            "Petra Plencnerova",
            "David G Baker",
            "Patrick D McGorry",
            "Alexandra Parker",
            "Jonathan Karnon",
            "Sue M Cotton",
            "Cathrine Mihalopoulos"
        ],
        "Abstract": "We are developing an economic model to explore multiple topics in Australian youth mental health policy. We want that model to be readily transferable to other jurisdictions. We developed a software framework for authoring transparent, reusable and updatable Computational Health Economic Models (CHEMs) (the software files that implement health economic models). We specified framework user requirements of a template CHEM module that facilitates modular model implementations, a simple programming syntax and tools for authoring new CHEM modules, supplying CHEMs with data, reporting reproducible CHEM analyses, searching for CHEM modules and maintaining a CHEM project website. We implemented the framework as six development version code libraries in the programming language R that integrate with online services for software development and research data archiving. We used the framework to author five development version R libraries of CHEM modules focused on utility mapping in youth mental health. These modules provide tools for variable validation, dataset description, multi-attribute instrument scoring, construction of mapping models, reporting of mapping studies and making out of sample predictions. We assessed these CHEM module libraries as mostly meeting transparency, reusability and updatability criteria that we have previously developed, but requiring more detailed documentation and unit testing of individual modules. Our software framework has potential value as a prototype for future tools to support the development of transferable CHEMs.",
        "Publication date": "25 March, 2024",
        "Link": "https://arxiv.org/pdf/2310.14138"
    },
    {
        "ID": 397,
        "Title": "With a Few Square Roots, Quantum Computing is as Easy as Î ",
        "Authors": [
            "Jacques Carette",
            "Chris Heunen",
            "Robin Kaarsgaard",
            "Amr Sabry"
        ],
        "Abstract": "Rig groupoids provide a semantic model of \\PiLang, a universal classical reversible programming language over finite types. We prove that extending rig groupoids with just two maps and three equations about them results in a model of quantum computing that is computationally universal and equationally sound and complete for a variety of gate sets. The first map corresponds to an $8^{\\text{th}}$ root of the identity morphism on the unit $1$. The second map corresponds to a square root of the symmetry on $1+1$. As square roots are generally not unique and can sometimes even be trivial, the maps are constrained to satisfy a nondegeneracy axiom, which we relate to the Euler decomposition of the Hadamard gate. The semantic construction is turned into an extension of \\PiLang, called \\SPiLang, that is a computationally universal quantum programming language equipped with an equational theory that is sound and complete with respect to the Clifford gate set, the standard gate set of Clifford+T restricted to $\\le 2$ qubits, and the computationally universal Gaussian Clifford+T gate set.",
        "Publication date": "21 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.14056"
    },
    {
        "ID": 398,
        "Title": "MeaeQ: Mount Model Extraction Attacks with Efficient Queries",
        "Authors": [
            "Chengwei Dai",
            "Minxuan Lv",
            "Kun Li",
            "Wei Zhou"
        ],
        "Abstract": "We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries. Our code is available at https://github.com/C-W-D/MeaeQ.",
        "Publication date": "21 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.14047"
    },
    {
        "ID": 399,
        "Title": "Software Metadata Classification based on Generative Artificial Intelligence",
        "Authors": [
            "Seetharam Killivalavan",
            "Durairaj Thenmozhi"
        ],
        "Abstract": "This paper presents a novel approach to enhance the performance of binary code comment quality classification models through the application of Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a dataset comprising 1239 newly generated code-comment pairs, extracted from various GitHub repositories and open-source projects, has been labelled as \"Useful\" or \"Not Useful\", and integrated into the existing corpus of 9048 pairs in the C programming language. Employing a cutting-edge Large Language Model Architecture, the generated dataset demonstrates notable improvements in model accuracy. Specifically, when incorporated into the Support Vector Machine (SVM) model, a 6% increase in precision is observed, rising from 0.79 to 0.85. Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5% increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the potential of Generative AI in augmenting code comment quality classification models. The results affirm the effectiveness of this methodology, indicating its applicability in broader contexts within software development and quality assurance domains. The findings underscore the significance of integrating generative techniques to advance the accuracy and efficacy of machine learning models in practical software engineering scenarios.",
        "Publication date": "14 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.13006"
    },
    {
        "ID": 400,
        "Title": "Large Language Models for Code Analysis: Do LLMs Really Do Their Job?",
        "Authors": [
            "Chongzhou Fang",
            "Ning Miao",
            "Shaurya Srivastav",
            "Jialin Liu",
            "Ruoyu Zhang",
            "Ruijie Fang",
            "Asmita",
            "Ryan Tsang",
            "Najmeh Nazari",
            "Han Wang",
            "Houman Homayoun"
        ],
        "Abstract": "Large language models (LLMs) have demonstrated significant potential in the realm of natural language understanding and programming code processing tasks. Their capacity to comprehend and generate human-like code has spurred research into harnessing LLMs for code analysis purposes. However, the existing body of literature falls short in delivering a systematic evaluation and assessment of LLMs' effectiveness in code analysis, particularly in the context of obfuscated code.\n  This paper seeks to bridge this gap by offering a comprehensive evaluation of LLMs' capabilities in performing code analysis tasks. Additionally, it presents real-world case studies that employ LLMs for code analysis. Our findings indicate that LLMs can indeed serve as valuable tools for automating code analysis, albeit with certain limitations. Through meticulous exploration, this research contributes to a deeper understanding of the potential and constraints associated with utilizing LLMs in code analysis, paving the way for enhanced applications in this critical domain.",
        "Publication date": "5 March, 2024",
        "Link": "https://arxiv.org/pdf/2310.12357"
    },
    {
        "ID": 401,
        "Title": "Program Translation via Code Distillation",
        "Authors": [
            "Yufan Huang",
            "Mengnan Qi",
            "Yongqiang Yao",
            "Maoquan Wang",
            "Bin Gu",
            "Colin Clement",
            "Neel Sundaresan"
        ],
        "Abstract": "Software version migration and program translation are an important and costly part of the lifecycle of large codebases. Traditional machine translation relies on parallel corpora for supervised translation, which is not feasible for program translation due to a dearth of aligned data. Recent unsupervised neural machine translation techniques have overcome data limitations by included techniques such as back translation and low level compiler intermediate representations (IR). These methods face significant challenges due to the noise in code snippet alignment and the diversity of IRs respectively. In this paper we propose a novel model called Code Distillation (CoDist) whereby we capture the semantic and structural equivalence of code in a language agnostic intermediate representation. Distilled code serves as a translation pivot for any programming language, leading by construction to parallel corpora which scale to all available source code by simply applying the distillation compiler. We demonstrate that our approach achieves state-of-the-art performance on CodeXGLUE and TransCoder GeeksForGeeks translation benchmarks, with an average absolute increase of 12.7% on the TransCoder GeeksforGeeks translation benchmark compare to TransCoder-ST.",
        "Publication date": "17 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.11476"
    },
    {
        "ID": 402,
        "Title": "Enhancing Binary Code Comment Quality Classification: Integrating Generative AI for Improved Accuracy",
        "Authors": [
            "Rohith Arumugam S",
            "Angel Deborah S"
        ],
        "Abstract": "This report focuses on enhancing a binary code comment quality classification model by integrating generated code and comment pairs, to improve model accuracy. The dataset comprises 9048 pairs of code and comments written in the C programming language, each annotated as \"Useful\" or \"Not Useful.\" Additionally, code and comment pairs are generated using a Large Language Model Architecture, and these generated pairs are labeled to indicate their utility. The outcome of this effort consists of two classification models: one utilizing the original dataset and another incorporating the augmented dataset with the newly generated code comment pairs and labels.",
        "Publication date": "14 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.11467"
    },
    {
        "ID": 403,
        "Title": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion",
        "Authors": [
            "Yangruibo Ding",
            "Zijian Wang",
            "Wasi Uddin Ahmad",
            "Hantian Ding",
            "Ming Tan",
            "Nihal Jain",
            "Murali Krishna Ramanathan",
            "Ramesh Nallapati",
            "Parminder Bhatia",
            "Dan Roth",
            "Bing Xiang"
        ],
        "Abstract": "Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.\n  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file.\n  Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.",
        "Publication date": "16 November, 2023",
        "Link": "https://arxiv.org/pdf/2310.11248"
    },
    {
        "ID": 404,
        "Title": "ACES: Generating Diverse Programming Puzzles with with Autotelic Generative Models",
        "Authors": [
            "Julien Pourcel",
            "CÃ©dric Colas",
            "Gaia Molinaro",
            "Pierre-Yves Oudeyer",
            "Laetitia Teodorescu"
        ],
        "Abstract": "The ability to invent novel and interesting problems is a remarkable feature of human intelligence that drives innovation, art, and science. We propose a method that aims to automate this process by harnessing the power of state-of-the-art generative models to produce a diversity of challenging yet solvable problems, here in the context of Python programming puzzles. Inspired by the intrinsically motivated literature, Autotelic CodE Search (ACES) jointly optimizes for the diversity and difficulty of generated problems. We represent problems in a space of LLM-generated semantic descriptors describing the programming skills required to solve them (e.g. string manipulation, dynamic programming, etc.) and measure their difficulty empirically as a linearly decreasing function of the success rate of Llama-3-70B, a state-of-the-art LLM problem solver. ACES iteratively prompts a large language model to generate difficult problems achieving a diversity of target semantic descriptors (goal-directed exploration) using previously generated problems as in-context examples. ACES generates problems that are more diverse and more challenging than problems produced by baseline methods and three times more challenging than problems found in existing Python programming benchmarks on average across 11 state-of-the-art code LLMs.",
        "Publication date": "29 May, 2024",
        "Link": "https://arxiv.org/pdf/2310.10692"
    },
    {
        "ID": 405,
        "Title": "PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming",
        "Authors": [
            "Chufan Gao",
            "Xulin Fan",
            "Jimeng Sun",
            "Xuan Wang"
        ],
        "Abstract": "Relation extraction aims to classify the relationships between two entities into pre-defined categories. While previous research has mainly focused on sentence-level relation extraction, recent studies have expanded the scope to document-level relation extraction. Traditional relation extraction methods heavily rely on human-annotated training data, which is time-consuming and labor-intensive. To mitigate the need for manual annotation, recent weakly-supervised approaches have been developed for sentence-level relation extraction while limited work has been done on document-level relation extraction. Weakly-supervised document-level relation extraction faces significant challenges due to an imbalanced number \"no relation\" instances and the failure of directly probing pretrained large language models for document relation extraction. To address these challenges, we propose PromptRE, a novel weakly-supervised document-level relation extraction method that combines prompting-based techniques with data programming. Furthermore, PromptRE incorporates the label distribution and entity types as prior knowledge to improve the performance. By leveraging the strengths of both prompting and data programming, PromptRE achieves improved performance in relation classification and effectively handles the \"no relation\" problem. Experimental results on ReDocRED, a benchmark dataset for document-level relation extraction, demonstrate the superiority of PromptRE over baseline approaches.",
        "Publication date": "13 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.09265"
    },
    {
        "ID": 406,
        "Title": "CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming",
        "Authors": [
            "Li Feng",
            "Ryan Yen",
            "Yuzhe You",
            "Mingming Fan",
            "Jian Zhao",
            "Zhicong Lu"
        ],
        "Abstract": "Natural language (NL) programming has become more approachable due to the powerful code-generation capability of large language models (LLMs). This shift to using NL to program enhances collaborative programming by reducing communication barriers and context-switching among programmers from varying backgrounds. However, programmers may face challenges during prompt engineering in a collaborative setting as they need to actively keep aware of their collaborators' progress and intents. In this paper, we aim to investigate ways to assist programmers' prompt engineering in a collaborative context. We first conducted a formative study to understand the workflows and challenges of programmers when using NL for collaborative programming. Based on our findings, we implemented a prototype, CoPrompt, to support collaborative prompt engineering by providing referring, requesting, sharing, and linking mechanisms. Our user study indicates that CoPrompt assists programmers in comprehending collaborators' prompts and building on their collaborators' work, reducing repetitive updates and communication costs.",
        "Publication date": "1 March, 2024",
        "Link": "https://arxiv.org/pdf/2310.09235"
    },
    {
        "ID": 407,
        "Title": "A Critical Review of Large Language Model on Software Engineering: An Example from ChatGPT and Automated Program Repair",
        "Authors": [
            "Quanjun Zhang",
            "Tongke Zhang",
            "Juan Zhai",
            "Chunrong Fang",
            "Bowen Yu",
            "Weisong Sun",
            "Zhenyu Chen"
        ],
        "Abstract": "Large Language Models (LLMs) have been gaining increasing attention and demonstrated promising performance across a variety of Software Engineering (SE) tasks, such as Automated Program Repair (APR), code summarization, and code completion. For example, ChatGPT, the latest black-box LLM, has been investigated by numerous recent research studies and has shown impressive performance in various tasks. However, there exists a potential risk of data leakage since these LLMs are usually close-sourced with unknown specific training details, e.g., pre-training datasets.\n  In this paper, we seek to review the bug-fixing capabilities of ChatGPT on a clean APR benchmark with different research objectives. We first introduce {\\benchmark}, a new benchmark with buggy and the corresponding fixed programs from competitive programming problems starting from 2023, after the training cutoff point of ChatGPT. The results on {\\benchmark} show that ChatGPT is able to fix 109 out of 151 buggy programs using the basic prompt within 35 independent rounds, outperforming state-of-the-art LLMs CodeT5 and PLBART by 27.5\\% and 62.4\\% prediction accuracy. We also investigate the impact of three types of prompts, i.e., problem description, error feedback, and bug localization, leading to additional 34 fixed bugs. Besides, we provide additional discussion from the interactive nature of ChatGPT to illustrate the capacity of a dialog-based repair workflow with 9 additional fixed bugs. Inspired by the findings, we further pinpoint various challenges and opportunities for advanced SE study equipped with such LLMs (e.g.,~ChatGPT) in the near future. More importantly, our work calls for more research on the reevaluation of the achievements obtained by existing black-box LLMs across various SE tasks, not limited to ChatGPT on APR.",
        "Publication date": "17 April, 2024",
        "Link": "https://arxiv.org/pdf/2310.08879"
    },
    {
        "ID": 408,
        "Title": "CoLadder: Supporting Programmers with Hierarchical Code Generation in Multi-Level Abstraction",
        "Authors": [
            "Ryan Yen",
            "Jiawen Zhu",
            "Sangho Suh",
            "Haijun Xia",
            "Jian Zhao"
        ],
        "Abstract": "Programmers increasingly rely on Large Language Models (LLMs) for code generation. However, misalignment between programmers' goals and generated code complicates the code evaluation process and demands frequent switching between prompt authoring and code evaluation. Yet, current LLM-driven code assistants lack sufficient scaffolding to help programmers format intentions from their overarching goals, a crucial step before translating these intentions into natural language prompts. To address this gap, we adopted an iterative design process to gain insights into programmers' strategies when using LLMs for programming. Building on our findings, we created CoLadder, a system that supports programmers by facilitating hierarchical task decomposition, direct code segment manipulation, and result evaluation during prompt authoring. A user study with 12 experienced programmers showed that CoLadder is effective in helping programmers externalize their problem-solving intentions flexibly, improving their ability to evaluate and modify code across various abstraction levels, from goal to final code implementation.",
        "Publication date": "26 December, 2023",
        "Link": "https://arxiv.org/pdf/2310.08699"
    },
    {
        "ID": 409,
        "Title": "Rethinking Negative Pairs in Code Search",
        "Authors": [
            "Haochen Li",
            "Xin Zhou",
            "Luu Anh Tuan",
            "Chunyan Miao"
        ],
        "Abstract": "Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of negative pairs and show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE. Theoretically, we analyze the effects of Soft-InfoNCE on controlling the distribution of learnt code representations and on deducing a more precise mutual information estimation. We furthermore discuss the superiority of proposed loss functions with other design alternatives. Extensive experiments demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods under state-of-the-art code search models on a large-scale public dataset consisting of six programming languages. Source code is available at \\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.",
        "Publication date": "12 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.08069"
    },
    {
        "ID": 410,
        "Title": "Lemur: Harmonizing Natural Language and Code for Language Agents",
        "Authors": [
            "Yiheng Xu",
            "Hongjin Su",
            "Chen Xing",
            "Boyu Mi",
            "Qian Liu",
            "Weijia Shi",
            "Binyuan Hui",
            "Fan Zhou",
            "Yitao Liu",
            "Tianbao Xie",
            "Zhoujun Cheng",
            "Siheng Zhao",
            "Lingpeng Kong",
            "Bailin Wang",
            "Caiming Xiong",
            "Tao Yu"
        ],
        "Abstract": "We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks among open-source models. Comprehensive experiments demonstrate Lemur's superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. https://github.com/OpenLemur/Lemur",
        "Publication date": "24 August, 2024",
        "Link": "https://arxiv.org/pdf/2310.06830"
    },
    {
        "ID": 411,
        "Title": "Forgetful Large Language Models: Lessons Learned from Using LLMs in Robot Programming",
        "Authors": [
            "Juo-Tung Chen",
            "Chien-Ming Huang"
        ],
        "Abstract": "Large language models offer new ways of empowering people to program robot applications-namely, code generation via prompting. However, the code generated by LLMs is susceptible to errors. This work reports a preliminary exploration that empirically characterizes common errors produced by LLMs in robot programming. We categorize these errors into two phases: interpretation and execution. In this work, we focus on errors in execution and observe that they are caused by LLMs being \"forgetful\" of key information provided in user prompts. Based on this observation, we propose prompt engineering tactics designed to reduce errors in execution. We then demonstrate the effectiveness of these tactics with three language models: ChatGPT, Bard, and LLaMA-2. Finally, we discuss lessons learned from using LLMs in robot programming and call for the benchmarking of LLM-powered end-user development of robot applications.",
        "Publication date": "10 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.06646"
    },
    {
        "ID": 412,
        "Title": "DASICS: Enhancing Memory Protection with Dynamic Compartmentalization",
        "Authors": [
            "Yue Jin",
            "Yibin Xu",
            "Chengyuan Yang",
            "Han Wang",
            "Tianyi Huang",
            "Tianyue Lu",
            "Mingyu Chen"
        ],
        "Abstract": "In the existing software development ecosystem, security issues introduced by third-party code cannot be overlooked. Among these security concerns, memory access vulnerabilities stand out prominently, leading to risks such as the theft or tampering of sensitive data. To address this issue, software-based defense mechanisms have been established at the programming language, compiler, and operating system levels. However, as a trade-off, these mechanisms significantly reduce software execution efficiency. Hardware-software co-design approaches have sought to either construct entirely isolated trusted execution environments or attempt to partition security domains within the same address space. While such approaches enhance efficiency compared to pure software methods, they also encounter challenges related to granularity of protection, performance overhead, and portability. In response to these challenges, we present the DASICS (Dynamic in-Address-Space Isolation by Code Segments) secure processor design, which offers dynamic and flexible security protection across multiple privilege levels, addressing data flow protection, control flow protection, and secure system calls. We have implemented hardware FPGA prototypes and software QEMU simulator prototypes based on DASICS, along with necessary modifications to system software for adaptability. We illustrate the protective mechanisms and effectiveness of DASICS with two practical examples and provide potential real-world use cases where DASICS could be applied.",
        "Publication date": "10 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.06435"
    },
    {
        "ID": 413,
        "Title": "CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model",
        "Authors": [
            "Peng Di",
            "Jianguo Li",
            "Hang Yu",
            "Wei Jiang",
            "Wenting Cai",
            "Yang Cao",
            "Chaoyu Chen",
            "Dajun Chen",
            "Hongwei Chen",
            "Liang Chen",
            "Gang Fan",
            "Jie Gong",
            "Zi Gong",
            "Wen Hu",
            "Tingting Guo",
            "Zhichao Lei",
            "Ting Li",
            "Zheng Li",
            "Ming Liang",
            "Cong Liao",
            "Bingchang Liu",
            "Jiachen Liu",
            "Zhiwei Liu",
            "Shaojun Lu",
            "Min Shen"
        ],
        "Abstract": "Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",
        "Publication date": "10 January, 2024",
        "Link": "https://arxiv.org/pdf/2310.06266"
    },
    {
        "ID": 414,
        "Title": "LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation",
        "Authors": [
            "Christian Munley",
            "Aaron Jarmusch",
            "Sunita Chandrasekaran"
        ],
        "Abstract": "Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. The goal of this work is to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based parallel programming paradigm, OpenACC. To do so, in this paper, we explore the capabilities of state-of-the-art LLMs, including open-source LLMs -- Meta Codellama, Phind fine-tuned version of Codellama, Deepseek Deepseek Coder and closed-source LLMs -- OpenAI GPT-3.5-Turbo and GPT-4-Turbo. We further fine-tuned the open-source LLMs and GPT-3.5-Turbo using our own testsuite dataset along with using the OpenACC specification. We also explored these LLMs using various prompt engineering techniques that include code template, template with retrieval-augmented generation (RAG), one-shot example, one-shot with RAG, expressive prompt with code template and RAG. This paper highlights our findings from over 5000 tests generated via all the above mentioned methods. Our contributions include: (a) exploring the capabilities of the latest and relevant LLMs for code generation, (b) investigating fine-tuning and prompt methods, and (c) analyzing the outcome of LLMs generated tests including manually analysis of representative set of tests. We found the LLM Deepseek-Coder-33b-Instruct produced the most passing tests followed by GPT-4-Turbo.",
        "Publication date": "10 March, 2024",
        "Link": "https://arxiv.org/pdf/2310.04963"
    },
    {
        "ID": 415,
        "Title": "mlirSynth: Automatic, Retargetable Program Raising in Multi-Level IR using Program Synthesis",
        "Authors": [
            "Alexander Brauckmann",
            "Elizabeth Polgreen",
            "Tobias Grosser",
            "Michael F. P. O'Boyle"
        ],
        "Abstract": "MLIR is an emerging compiler infrastructure for modern hardware, but existing programs cannot take advantage of MLIR's high-performance compilation if they are described in lower-level general purpose languages. Consequently, to avoid programs needing to be rewritten manually, this has led to efforts to automatically raise lower-level to higher-level dialects in MLIR. However, current methods rely on manually-defined raising rules, which limit their applicability and make them challenging to maintain as MLIR dialects evolve.\n  We present mlirSynth -- a novel approach which translates programs from lower-level MLIR dialects to high-level ones without manually defined rules. Instead, it uses available dialect definitions to construct a program space and searches it effectively using type constraints and equivalences. We demonstrate its effectiveness \\revi{by raising C programs} to two distinct high-level MLIR dialects, which enables us to use existing high-level dialect specific compilation flows. On Polybench, we show a greater coverage than previous approaches, resulting in geomean speedups of 2.5x (Intel) and 3.4x (AMD) over state-of-the-art compilation flows for the C programming language. mlirSynth also enables retargetability to domain-specific accelerators, resulting in a geomean speedup of 21.6x on a TPU.",
        "Publication date": "6 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.04196"
    },
    {
        "ID": 416,
        "Title": "Trustworthy Formal Natural Language Specifications",
        "Authors": [
            "Colin S. Gordon",
            "Sergey Matskevich"
        ],
        "Abstract": "Interactive proof assistants are computer programs carefully constructed to check a human-designed proof of a mathematical claim with high confidence in the implementation. However, this only validates truth of a formal claim, which may have been mistranslated from a claim made in natural language. This is especially problematic when using proof assistants to formally verify the correctness of software with respect to a natural language specification. The translation from informal to formal remains a challenging, time-consuming process that is difficult to audit for correctness.\n  This paper shows that it is possible to build support for specifications written in expressive subsets of natural language, within existing proof assistants, consistent with the principles used to establish trust and auditability in proof assistants themselves. We implement a means to provide specifications in a modularly extensible formal subset of English, and have them automatically translated into formal claims, entirely within the Lean proof assistant. Our approach is extensible (placing no permanent restrictions on grammatical structure), modular (allowing information about new words to be distributed alongside libraries), and produces proof certificates explaining how each word was interpreted and how the sentence's structure was used to compute the meaning.\n  We apply our prototype to the translation of various English descriptions of formal specifications from a popular textbook into Lean formalizations; all can be translated correctly with a modest lexicon with only minor modifications related to lexicon size.",
        "Publication date": "5 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.03885"
    },
    {
        "ID": 417,
        "Title": "Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation",
        "Authors": [
            "Tung Phung",
            "Victor-Alexandru PÄdurean",
            "Anjali Singh",
            "Christopher Brooks",
            "JosÃ© Cambronero",
            "Sumit Gulwani",
            "Adish Singla",
            "Gustavo Soares"
        ],
        "Abstract": "Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a ``tutor'' model to generate hints -- it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a ``student'' model to further validate the hint quality -- it performs an automatic quality validation by simulating the potential utility of providing this feedback. We show the efficacy of our technique via extensive evaluation using three real-world datasets of Python programs covering a variety of concepts ranging from basic algorithms to regular expressions and data analysis using pandas library.",
        "Publication date": "6 August, 2024",
        "Link": "https://arxiv.org/pdf/2310.03780"
    },
    {
        "ID": 418,
        "Title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",
        "Authors": [
            "Omar Khattab",
            "Arnav Singhvi",
            "Paridhi Maheshwari",
            "Zhiyuan Zhang",
            "Keshav Santhanam",
            "Sri Vardhamanan",
            "Saiful Haq",
            "Ashutosh Sharma",
            "Thomas T. Joshi",
            "Hanna Moazam",
            "Heather Miller",
            "Matei Zaharia",
            "Christopher Potts"
        ],
        "Abstract": "The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded \"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy",
        "Publication date": "5 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.03714"
    },
    {
        "ID": 419,
        "Title": "ORTAC+ : A User Friendly Domain Specific Language for Multi-Agent Mission Planning",
        "Authors": [
            "Caroline Bonhomme",
            "Jean-Louis Dufour"
        ],
        "Abstract": "A tactical military unit is a complex system composed of many agents such as infantry, robots, or drones. Given a mission, an automated planner can find an optimal plan. Therefore, the mission itself must be modeled. The problem is that languages like PDDL are too low-level to be usable by the end-user: an officer in the field. We present ORTAC+, a language and a planning tool designed for this end-user. Its main objective is to allow a natural modeling of the mission, to minimize the risk of bad modeling, and thus obtain reliable plans. The language offers high-level constructs specifically designed to describe tactical missions, but at the same time has clear semantics allowing a translation to PDDL, to take advantage of state-of-the-art planners.",
        "Publication date": "3 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.02356"
    },
    {
        "ID": 420,
        "Title": "Co-audit: tools to help humans double-check AI-generated content",
        "Authors": [
            "Andrew D. Gordon",
            "Carina Negreanu",
            "JosÃ© Cambronero",
            "Rasika Chakravarthy",
            "Ian Drosos",
            "Hao Fang",
            "Bhaskar Mitra",
            "Hannah Richardson",
            "Advait Sarkar",
            "Stephanie Simmons",
            "Jack Williams",
            "Ben Zorn"
        ],
        "Abstract": "Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges.",
        "Publication date": "2 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.01297"
    },
    {
        "ID": 421,
        "Title": "CGAAL: Distributed On-The-Fly ATL Model Checker with Heuristics",
        "Authors": [
            "Falke B. Ã. Carlsen",
            "Lars Bo P. Frydenskov",
            "Nicolaj Ã. Jensen",
            "Jener Rasmussen",
            "Mathias M. SÃ¸rensen",
            "Asger G. WeirsÃ¸e",
            "Mathias C. Jensen",
            "Kim G. Larsen"
        ],
        "Abstract": "We present CGAAL, our efficient on-the-fly model checker for alternating-time temporal logic (ATL) on concurrent game structures (CGS). We present how our tool encodes ATL as extended dependency graphs with negation edges and employs the distributed on-the-fly algorithm by Dalsgaard et al. Our tool offers multiple novel search strategies for the algorithm, including DHS which is inspired by PageRank and uses the in-degree of configurations as a heuristic, IHS which estimates instability of assignment values, and LPS which estimates the distance to a state satisfying the constituent property using linear programming. CGS are input using our modelling language LCGS, where composition and synchronisation are easily described. We prove the correctness of our encoding, and our experiments show that our tool CGAAL is often one to three orders of magnitude faster than the popular tool PRISM-games on case studies from PRISM's documentation and among case studies we have developed. In our evaluation, we also compare and evaluate our search strategies, and find that our custom search strategies are often significantly faster than the usual breadth-first and depth-first search strategies.",
        "Publication date": "2 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.00999"
    },
    {
        "ID": 422,
        "Title": "How Helpful do Novice Programmers Find the Feedback of an Automated Repair Tool?",
        "Authors": [
            "Oka Kurniawan",
            "Christopher M. Poskitt",
            "Ismam Al Hoque",
            "Norman Tiong Seng Lee",
            "Cyrille JÃ©gourel",
            "Nachamma Sockalingam"
        ],
        "Abstract": "Immediate feedback has been shown to improve student learning. In programming courses, immediate, automated feedback is typically provided in the form of pre-defined test cases run by a submission platform. While these are excellent for highlighting the presence of logical errors, they do not provide novice programmers enough scaffolding to help them identify where an error is or how to fix it. To address this, several tools have been developed that provide richer feedback in the form of program repairs. Studies of such tools, however, tend to focus more on whether correct repairs can be generated, rather than how novices are using them. In this paper, we describe our experience of using CLARA, an automated repair tool, to provide feedback to novices. First, we extended CLARA to support a larger subset of the Python language, before integrating it with the Jupyter Notebooks used for our programming exercises. Second, we devised a preliminary study in which students tackled programming problems with and without support of the tool using the 'think aloud' protocol. We found that novices often struggled to understand the proposed repairs, echoing the well-known challenge to understand compiler/interpreter messages. Furthermore, we found that students valued being told where a fix was needed - without necessarily the fix itself - suggesting that 'less may be more' from a pedagogical perspective.",
        "Publication date": "7 October, 2023",
        "Link": "https://arxiv.org/pdf/2310.00954"
    },
    {
        "ID": 423,
        "Title": "Case Study: Securing MMU-less Linux Using CHERI",
        "Authors": [
            "Hesham Almatary",
            "Alfredo Mazzinghi",
            "Robert N. M. Watson"
        ],
        "Abstract": "MMU-less Linux variant lacks security because it does not have protection or isolation mechanisms. It also does not use MPUs as they do not fit with its software model because of the design drawbacks of MPUs (\\ie coarse-grained protection with fixed number of protected regions). We secure the existing MMU-less Linux version of the RISC-V port using CHERI. CHERI is a hardware-software capability-based system that extends the ISA, toolchain, programming languages, operating systems, and applications in order to provide complete pointer and memory safety. We believe that CHERI could provide significant security guarantees for high-end dynamic MMU-less embedded systems at lower costs, compared to MMUs and MPUs, by: 1) building the entire software stack in pure-capability CHERI C mode which provides complete spatial memory safety at the kernel and user-level, 2) isolating user programs as separate ELFs, each with its own CHERI-based capability table; this provides spatial memory safety similar to what the MMU offers (\\ie user programs cannot access each other's memory), 3) isolating user programs from the kernel as the kernel has its own capability table from the users and vice versa, and 4) compartmentalising kernel modules using CompartOS' linkage-based compartmentalisation. This offers a new security front that is not possible using the current MMU-based Linux, where vulnerable/malicious kernel modules (\\eg device drivers) executing in the kernel space would not compromise or take down the entire system. These are the four main contributions of this paper, presenting novel CHERI-based mechanisms to secure MMU-less embedded Linux.",
        "Publication date": "18 January, 2024",
        "Link": "https://arxiv.org/pdf/2310.00933"
    },
    {
        "ID": 424,
        "Title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models",
        "Authors": [
            "Ansong Ni",
            "Pengcheng Yin",
            "Yilun Zhao",
            "Martin Riddell",
            "Troy Feng",
            "Rui Shen",
            "Stephen Yin",
            "Ye Liu",
            "Semih Yavuz",
            "Caiming Xiong",
            "Shafiq Joty",
            "Yingbo Zhou",
            "Dragomir Radev",
            "Arman Cohan"
        ],
        "Abstract": "Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluations of the output programs. This enables us to identify and analyze the typical failure modes across various tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We also release the evaluation framework and all model outputs, hoping to lay the groundwork for further future research in this domain.",
        "Publication date": "2 October, 2023",
        "Link": "https://arxiv.org/pdf/2309.17446"
    },
    {
        "ID": 425,
        "Title": "Language Models as a Service: Overview of a New Paradigm and its Challenges",
        "Authors": [
            "Emanuele La Malfa",
            "Aleksandar Petrov",
            "Simon Frieder",
            "Christoph Weinhuber",
            "Ryan Burnell",
            "Raza Nazar",
            "Anthony G. Cohn",
            "Nigel Shadbolt",
            "Michael Wooldridge"
        ],
        "Abstract": "Some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. In contrast with scenarios where full model access is available, as in the case of open-source models, such closed-off language models present specific challenges for evaluating, benchmarking, and testing them. This paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, replicability, reliability, and trustworthiness of LMaaS. We systematically examine the issues that arise from a lack of information about language models for each of these four aspects. We conduct a detailed analysis of existing solutions and put forth a number of considered recommendations, and highlight the directions for future advancements. On the other hand, it serves as a comprehensive resource for existing knowledge on current, major LMaaS, offering a synthesized overview of the licences and capabilities their interfaces offer.",
        "Publication date": "30 November, 2023",
        "Link": "https://arxiv.org/pdf/2309.16573"
    },
    {
        "ID": 426,
        "Title": "Using Integer Constraint Solving in Reuse Based Requirements Engineering",
        "Authors": [
            "Camille Salinesi",
            "Raul Mazo",
            "Daniel Diaz",
            "Olfa Djebbi"
        ],
        "Abstract": "Product Lines (PL) have proved an effective approach to reuse-based systems development. Several modeling languages were proposed so far to specify PL. Although they can be very different, these languages show two common features: they emphasize (a) variability, and (b) the specification of constraints to define acceptable configurations. It is now widely acknowledged that configuring a product can be considered as a constraint satisfaction problem. It is thus natural to consider constraint programming as a first choice candidate to specify constraints on PL. For instance, the different constraints that can be specified using the FODA language can easily be expressed using boolean constraints, which enables automated calculation and configuration using a SAT solver. But constraint programming proposes other domains than the boolean domain: for instance integers, real, or sets. The integer domain was, for instance, proposed by Benavides to specify constraints on feature attributes. This paper proposes to further explore the use of integer constraint programming to specify PL constraints. The approach was implemented in a prototype tool. Its use in a real case showed that constraint programming encompasses different PL modeling languages (such as FORE, OVM, or else), and allows specifying complex constraints that are difficult to specify with these languages.",
        "Publication date": "28 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.16279"
    },
    {
        "ID": 427,
        "Title": "T5APR: Empowering Automated Program Repair across Languages through Checkpoint Ensemble",
        "Authors": [
            "Reza Gharibi",
            "Mohammad Hadi Sadreddini",
            "Seyed Mostafa Fakhrahmad"
        ],
        "Abstract": "Automated program repair (APR) using deep learning techniques has become an important area of research in recent years, aiming to automatically generate bug-fixing patches that can improve software reliability and maintainability. However, most existing methods either target a single language or require high computational resources to train multilingual models. In this paper, we propose T5APR, a novel neural program repair approach that provides a unified solution for bug fixing across multiple programming languages. T5APR leverages CodeT5, a powerful pre-trained text-to-text transformer model, and adopts a checkpoint ensemble strategy to improve patch recommendation. We conduct comprehensive evaluations on six well-known benchmarks in four programming languages (Java, Python, C, JavaScript), demonstrating T5APR's competitiveness against state-of-the-art techniques. T5APR correctly fixes 1,985 bugs, including 1,442 bugs that none of the compared techniques has fixed. We further support the effectiveness of our approach by conducting detailed analyses, such as comparing the correct patch ranking among different techniques. The findings of this study demonstrate the potential of T5APR for use in real-world applications and highlight the importance of multilingual approaches in the field of APR.",
        "Publication date": "30 April, 2024",
        "Link": "https://arxiv.org/pdf/2309.15742"
    },
    {
        "ID": 428,
        "Title": "ComPile: A Large IR Dataset from Production Sources",
        "Authors": [
            "Aiden Grossman",
            "Ludger Paehler",
            "Konstantinos Parasyris",
            "Tal Ben-Nun",
            "Jacob Hegna",
            "William Moses",
            "Jose M Monsalve Diaz",
            "Mircea Trofin",
            "Johannes Doerfert"
        ],
        "Abstract": "Code is increasingly becoming a core data modality of modern machine learning research impacting not only the way we write code with conversational agents like OpenAI's ChatGPT, Google's Bard, or Anthropic's Claude, the way we translate code from one language into another, but also the compiler infrastructure underlying the language. While modeling approaches may vary and representations differ, the targeted tasks often remain the same within the individual classes of models. Relying solely on the ability of modern models to extract information from unstructured code does not take advantage of 70 years of programming language and compiler development by not utilizing the structure inherent to programs in the data collection. This detracts from the performance of models working over a tokenized representation of input code and precludes the use of these models in the compiler itself. To work towards the first intermediate representation (IR) based models, we fully utilize the LLVM compiler infrastructure, shared by a number of languages, to generate a 182B token dataset of LLVM IR. We generated this dataset from programming languages built on the shared LLVM infrastructure, including Rust, Swift, Julia, and C/C++, by hooking into LLVM code generation either through the language's package manager or the compiler directly to extract the dataset of intermediate representations from production grade programs. Statistical analysis proves the utility of our dataset not only for large language model training, but also for the introspection into the code generation process itself with the dataset showing great promise for machine-learned compiler components.",
        "Publication date": "30 April, 2024",
        "Link": "https://arxiv.org/pdf/2309.15432"
    },
    {
        "ID": 429,
        "Title": "The Design and Implementation of an Extensible System Meta-Programming Language",
        "Authors": [
            "Ronie Salgado"
        ],
        "Abstract": "System programming languages are typically compiled in a linear pipeline process, which is a completely opaque and isolated to end-users. This limits the possibilities of performing meta-programming in the same language and environment, and the extensibility of the compiler itself by end-users. We propose a novel redefinition of the compilation process in terms of interpreting the program definition as a script. This evaluation is performed in an environment where the full compilation pipeline is implemented and exposed to the user via a meta-object protocol, which forms the basis for a meta-circular definition and implementation of the programming language itself. We demonstrate the feasibility of this approach by bootstrapping a self-compiling implementation of Sysmel, a static and dynamic typed Smalltalk and C++ inspired programming language.",
        "Publication date": "27 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.15416"
    },
    {
        "ID": 430,
        "Title": "Types and Semantics for Extensible Data Types (Extended Version)",
        "Authors": [
            "Cas van der Rest",
            "Casper Bach Poulsen"
        ],
        "Abstract": "Developing and maintaining software commonly requires (1) adding new data type constructors to existing applications, but also (2) adding new functions that work on existing data. Most programming languages have native support for defining data types and functions in a way that supports either (1) or (2), but not both. This lack of native support makes it difficult to use and extend libraries. A theoretically well-studied solution is to define data types and functions using initial algebra semantics. While it is possible to encode this solution in existing programming languages, such encodings add syntactic and interpretive overhead, and commonly fail to take advantage of the map and fold fusion laws of initial algebras which compilers could exploit to generate more efficient code. A solution to these is to provide native support for initial algebra semantics. In this paper, we develop such a solution and present a type discipline and core calculus for a language with native support for initial algebra semantics.",
        "Publication date": "26 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.14985"
    },
    {
        "ID": 431,
        "Title": "Guess & Sketch: Language Model Guided Transpilation",
        "Authors": [
            "Celine Lee",
            "Abdulrahman Mahmoud",
            "Michal Kurek",
            "Simone Campanoni",
            "David Brooks",
            "Stephen Chong",
            "Gu-Yeon Wei",
            "Alexander M. Rush"
        ],
        "Abstract": "Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. Guess & Sketch extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test Guess & Sketch on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.",
        "Publication date": "15 March, 2024",
        "Link": "https://arxiv.org/pdf/2309.14396"
    },
    {
        "ID": 432,
        "Title": "Mechanizing Session-Types using a Structural View: Enforcing Linearity without Linearity",
        "Authors": [
            "Chuta Sano",
            "Ryan Kavanagh",
            "Brigitte Pientka"
        ],
        "Abstract": "Session types employ a linear type system that ensures that communication channels cannot be implicitly copied or discarded. As a result, many mechanizations of these systems require modeling channel contexts and carefully ensuring that they treat channels linearly. We demonstrate a technique that localizes linearity conditions as additional predicates embedded within type judgments, which allows us to use structural typing contexts instead of linear ones. This technique is especially relevant when leveraging (weak) higher-order abstract syntax to handle channel mobility and the intricate binding structures that arise in session-typed systems. Following this approach, we mechanize a session-typed system based on classical linear logic and its type preservation proof in the proof assistant Beluga, which uses the logical framework LF as its encoding language. We also prove adequacy for our encoding. This shows the tractability and effectiveness of our approach in modelling substructural systems such as session-typed languages.",
        "Publication date": "21 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.12466"
    },
    {
        "ID": 433,
        "Title": "How Do Analysts Understand and Verify AI-Assisted Data Analyses?",
        "Authors": [
            "Ken Gu",
            "Ruoxi Shang",
            "Tim Althoff",
            "Chenglong Wang",
            "Steven M. Drucker"
        ],
        "Abstract": "Data analysis is challenging as it requires synthesizing domain knowledge, statistical expertise, and programming skills. Assistants powered by large language models (LLMs), such as ChatGPT, can assist analysts by translating natural language instructions into code. However, AI-assistant responses and analysis code can be misaligned with the analyst's intent or be seemingly correct but lead to incorrect conclusions. Therefore, validating AI assistance is crucial and challenging. Here, we explore how analysts understand and verify the correctness of AI-generated analyses. To observe analysts in diverse verification approaches, we develop a design probe equipped with natural language explanations, code, visualizations, and interactive data tables with common data operations. Through a qualitative user study (n=22) using this probe, we uncover common behaviors within verification workflows and how analysts' programming, analysis, and tool backgrounds reflect these behaviors. Additionally, we provide recommendations for analysts and highlight opportunities for designers to improve future AI-assistant experiences.",
        "Publication date": "4 March, 2024",
        "Link": "https://arxiv.org/pdf/2309.10947"
    },
    {
        "ID": 434,
        "Title": "Julia as a unifying end-to-end workflow language on the Frontier exascale system",
        "Authors": [
            "William F. Godoy",
            "Pedro Valero-Lara",
            "Caira Anderson",
            "Katrina W. Lee",
            "Ana Gainaru",
            "Rafael Ferreira da Silva",
            "Jeffrey S. Vetter"
        ],
        "Abstract": "We evaluate Julia as a single language and ecosystem paradigm powered by LLVM to develop workflow components for high-performance computing. We run a Gray-Scott, 2-variable diffusion-reaction application using a memory-bound, 7-point stencil kernel on Frontier, the US Department of Energy's first exascale supercomputer. We evaluate the performance, scaling, and trade-offs of (i) the computational kernel on AMD's MI250x GPUs, (ii) weak scaling up to 4,096 MPI processes/GPUs or 512 nodes, (iii) parallel I/O writes using the ADIOS2 library bindings, and (iv) Jupyter Notebooks for interactive analysis. Results suggest that although Julia generates a reasonable LLVM-IR, a nearly 50% performance difference exists vs. native AMD HIP stencil codes when running on the GPUs. As expected, we observed near-zero overhead when using MPI and parallel I/O bindings for system-wide installed implementations. Consequently, Julia emerges as a compelling high-performance and high-productivity workflow composition language, as measured on the fastest supercomputer in the world.",
        "Publication date": "27 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.10292"
    },
    {
        "ID": 435,
        "Title": "ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing",
        "Authors": [
            "Ian Arawjo",
            "Chelse Swoopes",
            "Priyan Vaithilingam",
            "Martin Wattenberg",
            "Elena Glassman"
        ],
        "Abstract": "Evaluating outputs of large language models (LLMs) is challenging, requiring making -- and making sense of -- many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.",
        "Publication date": "3 May, 2024",
        "Link": "https://arxiv.org/pdf/2309.09128"
    },
    {
        "ID": 436,
        "Title": "Superpositioner -- A non-logical computation model",
        "Authors": [
            "Chuyu Xiong"
        ],
        "Abstract": "We have been striving to exceed computational complexity, and in the process, we have come to realize the dilemma of classical computing, and in turn we realize that superpositioner may be a way to solve. A superpositioner is a model formed by several Boolean functions that their variables and function values are feeding back to each other. The component of the superpositioner is the reentry function, which can be fully described by classical logic and can be calculated by classical computation, but the superpositioner as a whole is a non-logical entity, and it is impossible for classical computation to fully compute it. In this article, we present the concept of a superpositioner and discuss its basic properties. We find that the superpositioner + dispositioner will form a new type of computation model whose capabilities can surpass Turing computation. We envision that this new model will help implement these functions in the intelligent agent: a whole new way of programming and a whole new way of learning, endogenous feelings, analogies and associations, forming understanding, dynamic action, participating in the formation of subjectivity, and more. We will also discuss how to implement superpositioner in the most preliminary way.",
        "Publication date": "5 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.08618"
    },
    {
        "ID": 437,
        "Title": "REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes",
        "Authors": [
            "Chaozheng Wang",
            "Zongjie Li",
            "Yun Peng",
            "Shuzheng Gao",
            "Sirong Chen",
            "Shuai Wang",
            "Cuiyun Gao",
            "Michael R. Lyu"
        ],
        "Abstract": "Software plays a crucial role in our daily lives, and therefore the quality and security of software systems have become increasingly important. However, vulnerabilities in software still pose a significant threat, as they can have serious consequences. Recent advances in automated program repair have sought to automatically detect and fix bugs using data-driven techniques. Sophisticated deep learning methods have been applied to this area and have achieved promising results. However, existing benchmarks for training and evaluating these techniques remain limited, as they tend to focus on a single programming language and have relatively small datasets. Moreover, many benchmarks tend to be outdated and lack diversity, focusing on a specific codebase. Worse still, the quality of bug explanations in existing datasets is low, as they typically use imprecise and uninformative commit messages as explanations.\n  To address these issues, we propose an automated collecting framework REEF to collect REal-world vulnErabilities and Fixes from open-source repositories. We develop a multi-language crawler to collect vulnerabilities and their fixes, and design metrics to filter for high-quality vulnerability-fix pairs. Furthermore, we propose a neural language model-based approach to generate high-quality vulnerability explanations, which is key to producing informative fix messages. Through extensive experiments, we demonstrate that our approach can collect high-quality vulnerability-fix pairs and generate strong explanations. The dataset we collect contains 4,466 CVEs with 30,987 patches (including 236 CWE) across 7 programming languages with detailed related information, which is superior to existing benchmarks in scale, coverage, and quality. Evaluations by human experts further confirm that our framework produces high-quality vulnerability explanations.",
        "Publication date": "14 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.08115"
    },
    {
        "ID": 438,
        "Title": "A Deductive Verification Infrastructure for Probabilistic Programs (Extended Version)",
        "Authors": [
            "Philipp SchrÃ¶er",
            "Kevin Batz",
            "Benjamin Lucien Kaminski",
            "Joost-Pieter Katoen",
            "Christoph Matheja"
        ],
        "Abstract": "This paper presents a quantitative program verification infrastructure for discrete probabilistic programs. Our infrastructure can be viewed as the probabilistic analogue of Boogie: its central components are an intermediate verification language (IVL) together with a real-valued logic. Our IVL provides a programming-language-style for expressing verification conditions whose validity implies the correctness of a program under investigation. As our focus is on verifying quantitative properties such as bounds on expected outcomes, expected run-times, or termination probabilities, off-the-shelf IVLs based on Boolean first-order logic do not suffice. Instead, a paradigm shift from the standard Boolean to a real-valued domain is required.\n  Our IVL features quantitative generalizations of standard verification constructs such as assume- and assert-statements. Verification conditions are generated by a weakest-precondition-style semantics, based on our real-valued logic. We show that our verification infrastructure supports natural encodings of numerous verification techniques from the literature. With our SMT-based implementation, we automatically verify a variety of benchmarks. To the best of our knowledge, this establishes the first deductive verification infrastructure for expectation-based reasoning about probabilistic programs.",
        "Publication date": "15 November, 2023",
        "Link": "https://arxiv.org/pdf/2309.07781"
    },
    {
        "ID": 439,
        "Title": "Aligning Speakers: Evaluating and Visualizing Text-based Diarization Using Efficient Multiple Sequence Alignment (Extended Version)",
        "Authors": [
            "Chen Gong",
            "Peilin Wu",
            "Jinho D. Choi"
        ],
        "Abstract": "This paper presents a novel evaluation approach to text-based speaker diarization (SD), tackling the limitations of traditional metrics that do not account for any contextual information in text. Two new metrics are proposed, Text-based Diarization Error Rate and Diarization F1, which perform utterance- and word-level evaluations by aligning tokens in reference and hypothesis transcripts. Our metrics encompass more types of errors compared to existing ones, allowing us to make a more comprehensive analysis in SD. To align tokens, a multiple sequence alignment algorithm is introduced that supports multiple sequences in the reference while handling high-dimensional alignment to the hypothesis using dynamic programming. Our work is packaged into two tools, align4d providing an API for our alignment algorithm and TranscribeView for visualizing and evaluating SD errors, which can greatly aid in the creation of high-quality data, fostering the advancement of dialogue systems.",
        "Publication date": "14 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.07677"
    },
    {
        "ID": 440,
        "Title": "Automatic Data Visualization Generation from Chinese Natural Language Questions",
        "Authors": [
            "Yan Ge",
            "Victor Junqiu Wei",
            "Yuanfeng Song",
            "Jason Chen Zhang",
            "Raymond Chi-Wing Wong"
        ],
        "Abstract": "Data visualization has emerged as an effective tool for getting insights from massive datasets. Due to the hardness of manipulating the programming languages of data visualization, automatic data visualization generation from natural languages (Text-to-Vis) is becoming increasingly popular. Despite the plethora of research effort on the English Text-to-Vis, studies have yet to be conducted on data visualization generation from questions in Chinese. Motivated by this, we propose a Chinese Text-to-Vis dataset in the paper and demonstrate our first attempt to tackle this problem. Our model integrates multilingual BERT as the encoder, boosts the cross-lingual ability, and infuses the $n$-gram information into our word representation learning. Our experimental results show that our dataset is challenging and deserves further research.",
        "Publication date": "14 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.07650"
    },
    {
        "ID": 441,
        "Title": "WASM-MUTATE: Fast and Effective Binary Diversification for WebAssembly",
        "Authors": [
            "Javier Cabrera-Arteaga",
            "Nicholas Fitzgerald",
            "Martin Monperrus",
            "Benoit Baudry"
        ],
        "Abstract": "WebAssembly is the fourth officially endorsed Web language. It is recognized because of its efficiency and design, focused on security. Yet, its swiftly expanding ecosystem lacks robust software diversification systems. We introduce WASM-MUTATE, a diversification engine specifically designed for WebAssembly. Our engine meets several essential criteria: 1) To quickly generate functionally identical, yet behaviorally diverse, WebAssembly variants, 2) To be universally applicable to any WebAssembly program, irrespective of the source programming language, and 3) Generated variants should counter side-channels. By leveraging an e-graph data structure, WASM-MUTATE is implemented to meet both speed and efficacy. We evaluate WASM-MUTATE by conducting experiments on 404 programs, which include real-world applications. Our results highlight that WASM-MUTATE can produce tens of thousands of unique and efficient WebAssembly variants within minutes. Significantly, WASM-MUTATE can safeguard WebAssembly binaries against timing side-channel attacks,especially those of the Spectre type.",
        "Publication date": "17 January, 2024",
        "Link": "https://arxiv.org/pdf/2309.07638"
    },
    {
        "ID": 442,
        "Title": "Comparing Llama-2 and GPT-3 LLMs for HPC kernels generation",
        "Authors": [
            "Pedro Valero-Lara",
            "Alexis Huante",
            "Mustafa Al Lail",
            "William F. Godoy",
            "Keita Teranishi",
            "Prasanna Balaprakash",
            "Jeffrey S. Vetter"
        ],
        "Abstract": "We evaluate the use of the open-source Llama-2 model for generating well-known, high-performance computing kernels (e.g., AXPY, GEMV, GEMM) on different parallel programming models and languages (e.g., C++: OpenMP, OpenMP Offload, OpenACC, CUDA, HIP; Fortran: OpenMP, OpenMP Offload, OpenACC; Python: numpy, Numba, pyCUDA, cuPy; and Julia: Threads, CUDA.jl, AMDGPU.jl). We built upon our previous work that is based on the OpenAI Codex, which is a descendant of GPT-3, to generate similar kernels with simple prompts via GitHub Copilot. Our goal is to compare the accuracy of Llama-2 and our original GPT-3 baseline by using a similar metric. Llama-2 has a simplified model that shows competitive or even superior accuracy. We also report on the differences between these foundational large language models as generative AI continues to redefine human-computer interactions. Overall, Copilot generates codes that are more reliable but less optimized, whereas codes generated by Llama-2 are less reliable but more optimized when correct.",
        "Publication date": "11 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.07103"
    },
    {
        "ID": 443,
        "Title": "Large Language Models for Compiler Optimization",
        "Authors": [
            "Chris Cummins",
            "Volker Seeker",
            "Dejan Grubisic",
            "Mostafa Elhoushi",
            "Youwei Liang",
            "Baptiste Roziere",
            "Jonas Gehring",
            "Fabian Gloeckle",
            "Kim Hazelwood",
            "Gabriel Synnaeve",
            "Hugh Leather"
        ],
        "Abstract": "We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding.\n  We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time.",
        "Publication date": "11 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.07062"
    },
    {
        "ID": 444,
        "Title": "Unveiling the potential of large language models in generating semantic and cross-language clones",
        "Authors": [
            "Palash R. Roy",
            "Ajmain I. Alam",
            "Farouq Al-omari",
            "Banani Roy",
            "Chanchal K. Roy",
            "Kevin A. Schneider"
        ],
        "Abstract": "Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance. In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment.We have comprised a diverse set of code fragments and assessed GPT-3s performance in generating code variants.Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate the model's ability to produce accurate and semantically correct variants. Our findings shed light on GPT-3's strengths in code generation, offering insights into the potential applications and challenges of using advanced language models in software development. Our quantitative analysis yields compelling results. In the realm of semantic clones, GPT-3 attains an impressive accuracy of 62.14% and 0.55 BLEU score, achieved through few-shot prompt engineering. Furthermore, the model shines in transcending linguistic confines, boasting an exceptional 91.25% accuracy in generating cross-language clones",
        "Publication date": "12 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.06424"
    },
    {
        "ID": 445,
        "Title": "RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair",
        "Authors": [
            "Weishi Wang",
            "Yue Wang",
            "Shafiq Joty",
            "Steven C. H. Hoi"
        ],
        "Abstract": "Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art approaches on all benchmarks, e.g., repairing 15 more bugs on 818 Defects4J bugs.",
        "Publication date": "12 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.06057"
    },
    {
        "ID": 446,
        "Title": "Latte: Lightweight Aliasing Tracking for Java",
        "Authors": [
            "Conrad Zimmerman",
            "Catarina Gamboa",
            "Alcides Fonseca",
            "Jonathan Aldrich"
        ],
        "Abstract": "Many existing systems track aliasing and uniqueness, each with their own trade-off between expressiveness and developer effort. We propose Latte, a new approach that aims to minimize both the amount of annotations and the complexity of invariants necessary for reasoning about aliasing in an object-oriented language with mutation. Our approach only requires annotations for parameters and fields, while annotations for local variables are inferred. Furthermore, it relaxes uniqueness to allow aliasing among local variables, as long as this aliasing can be precisely determined. This enables support for destructive reads without changes to the language or its run-time semantics. Despite this simplicity, we show how this design can still be used for tracking uniqueness and aliasing in a local sequential setting, with practical applications, such as modeling a stack.",
        "Publication date": "11 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.05637"
    },
    {
        "ID": 447,
        "Title": "Data-Flow-Based Normalization Generation Algorithm of R1CS for Zero-Knowledge Proof",
        "Authors": [
            "Chenhao Shi",
            "Hao Chen",
            "Ruibang Liu",
            "Guoqiang Li"
        ],
        "Abstract": "The communities of blockchains and distributed ledgers have been stirred up by the introduction of zero-knowledge proofs (ZKPs). Originally designed to solve privacy issues, ZKPs have now evolved into an effective remedy for scalability concerns and are applied in Zcash (internet money like Bitcoin). To enable ZKPs, Rank-1 Constraint Systems (R1CS) offer a verifier for bi-linear equations. To accurately and efficiently represent R1CS, several language tools like Circom, Noir, and Snarky have been proposed to automate the compilation of advanced programs into R1CS. However, due to the flexible nature of R1CS representation, there can be significant differences in the compiled R1CS forms generated from circuit language programs with the same underlying semantics. To address this issue, this paper uses a data-flow-based R1CS paradigm algorithm, which produces a standardized format for different R1CS instances with identical semantics. By using the normalized R1CS format circuits, the complexity of circuits' verification can be reduced. In addition, this paper presents an R1CS normalization algorithm benchmark, and our experimental evaluation demonstrates the effectiveness and correctness of our methods.",
        "Publication date": "16 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.04274"
    },
    {
        "ID": 448,
        "Title": "DevGPT: Studying Developer-ChatGPT Conversations",
        "Authors": [
            "Tao Xiao",
            "Christoph Treude",
            "Hideaki Hata",
            "Kenichi Matsumoto"
        ],
        "Abstract": "This paper introduces DevGPT, a dataset curated to explore how software developers interact with ChatGPT, a prominent large language model (LLM). The dataset encompasses 29,778 prompts and responses from ChatGPT, including 19,106 code snippets, and is linked to corresponding software development artifacts such as source code, commits, issues, pull requests, discussions, and Hacker News threads. This comprehensive dataset is derived from shared ChatGPT conversations collected from GitHub and Hacker News, providing a rich resource for understanding the dynamics of developer interactions with ChatGPT, the nature of their inquiries, and the impact of these interactions on their work. DevGPT enables the study of developer queries, the effectiveness of ChatGPT in code generation and problem solving, and the broader implications of AI-assisted programming. By providing this dataset, the paper paves the way for novel research avenues in software engineering, particularly in understanding and improving the use of LLMs like ChatGPT by developers.",
        "Publication date": "13 February, 2024",
        "Link": "https://arxiv.org/pdf/2309.03914"
    },
    {
        "ID": 449,
        "Title": "A pragma based C++ framework for hybrid quantum/classical computation",
        "Authors": [
            "Arnaud Gazda",
            "Oceane Koska"
        ],
        "Abstract": "Quantum computers promise exponential speed ups over classical computers for various tasks. This emerging technology is expected to have its first huge impact in High Performance Computing (HPC), as it can solve problems beyond the reach of HPC. To that end, HPC will require quantum accelerators, which will enable applications to run on both classical and quantum devices, via hybrid quantum-classical nodes. Hybrid quantum-HPC applications should be scalable, executable on Quantum Error Corrected (QEC) devices, and could use quantum-classical primitives. However, the lack of scalability, poor performances, and inability to insert classical schemes within quantum applications has prevented current quantum frameworks from being adopted by the HPC community.\n  This paper specifies the requirements of a hybrid quantum-classical framework compatible with HPC environments, and introduces a novel hardware-agnostic framework called Q-Pragma. This framework extends the classical programming language C++ heavily used in HPC via the addition of pragma directives to manage quantum computations.",
        "Publication date": "27 March, 2024",
        "Link": "https://arxiv.org/pdf/2309.02605"
    },
    {
        "ID": 450,
        "Title": "Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension",
        "Authors": [
            "Anushrut Jignasu",
            "Kelly Marshall",
            "Baskar Ganapathysubramanian",
            "Aditya Balu",
            "Chinmay Hegde",
            "Adarsh Krishnamurthy"
        ],
        "Abstract": "3D printing or additive manufacturing is a revolutionary technology that enables the creation of physical objects from digital models. However, the quality and accuracy of 3D printing depend on the correctness and efficiency of the G-code, a low-level numerical control programming language that instructs 3D printers how to move and extrude material. Debugging G-code is a challenging task that requires a syntactic and semantic understanding of the G-code format and the geometry of the part to be printed. In this paper, we present the first extensive evaluation of six state-of-the-art foundational large language models (LLMs) for comprehending and debugging G-code files for 3D printing. We design effective prompts to enable pre-trained LLMs to understand and manipulate G-code and test their performance on various aspects of G-code debugging and manipulation, including detection and correction of common errors and the ability to perform geometric transformations. We analyze their strengths and weaknesses for understanding complete G-code files. We also discuss the implications and limitations of using LLMs for G-code comprehension.",
        "Publication date": "4 September, 2023",
        "Link": "https://arxiv.org/pdf/2309.02465"
    },
    {
        "ID": 451,
        "Title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
        "Authors": [
            "Lingyue Fu",
            "Huacan Chai",
            "Shuang Luo",
            "Kounianhua Du",
            "Weiming Zhang",
            "Longteng Fan",
            "Jiayi Lei",
            "Renting Rui",
            "Jianghao Lin",
            "Yuchen Fang",
            "Yifan Liu",
            "Jingkuan Wang",
            "Siyuan Qi",
            "Kangning Zhang",
            "Weinan Zhang",
            "Yong Yu"
        ],
        "Abstract": "With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. Evaluating the programming capabilities of LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has numerous downstream applications. In this paper, we propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension, code generation, and code correction abilities of LLMs. Programming comprehension task tests LLMs on multiple-choice exam questions covering conceptual understanding, commonsense reasoning, and multi-hop reasoning. The code generation task evaluates LLMs through completing C++ functions based on provided descriptions and prototypes. The code correction task asks LLMs to fix real-world erroneous code segments with different error messages. We evaluate 12 widely used LLMs, including both general-purpose and specialized models. GPT-4 exhibits the best programming capabilities, achieving approximate accuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to human performance, there is still significant room for improvement in LLM programming. We hope that CodeApex can serve as a reference for evaluating the coding capabilities of LLMs, further promoting their development and growth.",
        "Publication date": "11 March, 2024",
        "Link": "https://arxiv.org/pdf/2309.01940"
    },
    {
        "ID": 452,
        "Title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
        "Authors": [
            "Yuxiang Wei",
            "Chunqiu Steven Xia",
            "Lingming Zhang"
        ],
        "Abstract": "During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful \"copilots\" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a general code generation framework to further copilot the AI \"copilots\" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot outperforms state-of-the-art techniques by fixing 27% and 47% more bugs, respectively. Moreover, Repilot produces more valid and correct patches than the base LLM with the same budget. While we focus on leveraging Repilot for APR in this work, the overall approach is also generalizable to other code generation tasks.",
        "Publication date": "8 November, 2023",
        "Link": "https://arxiv.org/pdf/2309.00608"
    },
    {
        "ID": 453,
        "Title": "Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets",
        "Authors": [
            "Simon Thorne"
        ],
        "Abstract": "Large Language Models (LLM) have become sophisticated enough that complex computer programs can be created through interpretation of plain English sentences and implemented in a variety of modern languages such as Python, Java Script, C++ and Spreadsheets. These tools are powerful and relatively accurate and therefore provide broad access to computer programming regardless of the background or knowledge of the individual using them. This paper presents a series of experiments with ChatGPT to explore the tool's ability to produce valid spreadsheet formulae and related computational outputs in situations where ChatGPT has to deduce, infer and problem solve the answer. The results show that in certain circumstances, ChatGPT can produce correct spreadsheet formulae with correct reasoning, deduction and inference. However, when information is limited, uncertain or the problem is too complex, the accuracy of ChatGPT breaks down as does its ability to reason, infer and deduce. This can also result in false statements and \"hallucinations\" that all subvert the process of creating spreadsheet formulae.",
        "Publication date": "31 August, 2023",
        "Link": "https://arxiv.org/pdf/2309.00095"
    },
    {
        "ID": 454,
        "Title": "Can Programming Languages Boost Each Other via Instruction Tuning?",
        "Authors": [
            "Daoguang Zan",
            "Ailun Yu",
            "Bo Shen",
            "Jiaxin Zhang",
            "Taihong Chen",
            "Bing Geng",
            "Bei Chen",
            "Jichuan Ji",
            "Yafen Yao",
            "Yongji Wang",
            "Qianxiang Wang"
        ],
        "Abstract": "When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.",
        "Publication date": "3 September, 2023",
        "Link": "https://arxiv.org/pdf/2308.16824"
    },
    {
        "ID": 455,
        "Title": "Inferring Compensatory Kinase Networks in Yeast using Prolog",
        "Authors": [
            "George A. Elder",
            "Conrad Bessant"
        ],
        "Abstract": "Signalling pathways are conserved across different species, therefore making yeast a model organism to study these via disruption of kinase activity. Yeast has 159 genes that encode protein kinases and phosphatases, and 136 of these have counterparts in humans. Therefore any insight in this model organism could potentially offer indications of mechanisms of action in the human kinome. The study utilises a Prolog-based approach, data from a yeast kinase deletions strains study and publicly available kinase-protein associations. Prolog, a programming language that is well-suited for symbolic reasoning is used to reason over the data and infer compensatory kinase networks. This approach is based on the idea that when a kinase is knocked out, other kinases may compensate for this loss of activity. Background knowledge on kinases targeting proteins is used to guide the analysis. This knowledge is used to infer the potential compensatory interactions between kinases based on the changes in phosphorylation observed in the phosphoproteomics data from the yeast study. The results demonstrate the effectiveness of the Prolog-based approach in analysing complex cell signalling mechanisms in yeast. The inferred compensatory kinase networks provide new insights into the regulation of cell signalling in yeast and may aid in the identification of potential therapeutic targets for modulating signalling pathways in yeast and other organisms.",
        "Publication date": "30 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.16309"
    },
    {
        "ID": 456,
        "Title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
        "Authors": [
            "Neha Sengupta",
            "Sunil Kumar Sahu",
            "Bokang Jia",
            "Satheesh Katipomu",
            "Haonan Li",
            "Fajri Koto",
            "William Marshall",
            "Gurpreet Gosal",
            "Cynthia Liu",
            "Zhiming Chen",
            "Osama Mohammed Afzal",
            "Samta Kamboj",
            "Onkar Pandit",
            "Rahul Pal",
            "Lalit Pradhan",
            "Zain Muhammad Mujahid",
            "Massa Baali",
            "Xudong Han",
            "Sondos Mahmoud Bsharat",
            "Alham Fikri Aji",
            "Zhiqiang Shen",
            "Zhengzhong Liu",
            "Natalia Vassilieva",
            "Joel Hestness",
            "Andy Hock"
        ],
        "Abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat",
        "Publication date": "29 September, 2023",
        "Link": "https://arxiv.org/pdf/2308.16149"
    },
    {
        "ID": 457,
        "Title": "The Janus System: Multi-paradigm Programming in Prolog and Python",
        "Authors": [
            "Theresa Swift",
            "Carl Andersen"
        ],
        "Abstract": "Python and Prolog express different programming paradigms, with different strengths.  Python is wildly popular because it is well-structured, easy to use, and mixes well with thousands of scientific and machine learning programs written in C.  Prolog's logic-based approach provides powerful reasoning capabilities, especially when combined with constraint evaluation, probabilistic reasoning, well-founded negation, and other advances. Both languages have commonalities as well: both are usually written in C, both are dynamically typed, and both use data structures based on a small number of recursive types.\n  This paper describes the design and implementation of Janus, a system that tightly combines Prolog and Python into a single process. Janus bi-translates data structures and offers performance of many hundreds of thousands of round-trip inter-language calls per second. Although Janus is still new, it has been used in commercial applications including natural language processing, visual query answering and robotic automation.  Janus was developed for XSB, but porting Janus code to a second Prolog has been straightforward, indicating that Janus is a tool that other Prologs may easily adopt.",
        "Publication date": "30 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.15893"
    },
    {
        "ID": 458,
        "Title": "Accurate complex Jacobi rotations",
        "Authors": [
            "Vedran NovakoviÄ"
        ],
        "Abstract": "This note shows how to compute, to high relative accuracy under mild assumptions, complex Jacobi rotations for diagonalization of Hermitian matrices of order two, using the correctly rounded functions $\\mathtt{cr\\_hypot}$ and $\\mathtt{cr\\_rsqrt}$, proposed for standardization in the C programming language as recommended by the IEEE-754 floating-point standard. The rounding to nearest (ties to even) and the non-stop arithmetic are assumed. The numerical examples compare the observed with theoretical bounds on the relative errors in the rotations' elements, and show that the maximal observed departure of the rotations' determinants from unity is smaller than that of the transformations computed by LAPACK.",
        "Publication date": "11 May, 2024",
        "Link": "https://arxiv.org/pdf/2308.14222"
    },
    {
        "ID": 459,
        "Title": "GPTCloneBench: A comprehensive benchmark of semantic clones and cross-language clones using GPT-3 model and SemanticCloneBench",
        "Authors": [
            "Ajmain Inqiad Alam",
            "Palash Ranjan Roy",
            "Farouq Al-omari",
            "Chanchal Kumar Roy",
            "Banani Roy",
            "Kevin Schneider"
        ],
        "Abstract": "With the emergence of Machine Learning, there has been a surge in leveraging its capabilities for problem-solving across various domains. In the code clone realm, the identification of type-4 or semantic clones has emerged as a crucial yet challenging task. Researchers aim to utilize Machine Learning to tackle this challenge, often relying on the BigCloneBench dataset. However, it's worth noting that BigCloneBench, originally not designed for semantic clone detection, presents several limitations that hinder its suitability as a comprehensive training dataset for this specific purpose. Furthermore, CLCDSA dataset suffers from a lack of reusable examples aligning with real-world software systems, rendering it inadequate for cross-language clone detection approaches. In this work, we present a comprehensive semantic clone and cross-language clone benchmark, GPTCloneBench by exploiting SemanticCloneBench and OpenAI's GPT-3 model. In particular, using code fragments from SemanticCloneBench as sample inputs along with appropriate prompt engineering for GPT-3 model, we generate semantic and cross-language clones for these specific fragments and then conduct a combination of extensive manual analysis, tool-assisted filtering, functionality testing and automated validation in building the benchmark. From 79,928 clone pairs of GPT-3 output, we created a benchmark with 37,149 true semantic clone pairs, 19,288 false semantic pairs(Type-1/Type-2), and 20,770 cross-language clones across four languages (Java, C, C#, and Python). Our benchmark is 15-fold larger than SemanticCloneBench, has more functional code examples for software systems and programming language support than CLCDSA, and overcomes BigCloneBench's qualities, quantification, and language variety limitations.",
        "Publication date": "1 September, 2023",
        "Link": "https://arxiv.org/pdf/2308.13963"
    },
    {
        "ID": 460,
        "Title": "Modeling Programmer Attention as Scanpath Prediction",
        "Authors": [
            "Aakash Bansal",
            "Chia-Yi Su",
            "Zachary Karas",
            "Yifan Zhang",
            "Yu Huang",
            "Toby Jia-Jun Li",
            "Collin McMillan"
        ],
        "Abstract": "This paper launches a new effort at modeling programmer attention by predicting eye movement scanpaths. Programmer attention refers to what information people intake when performing programming tasks. Models of programmer attention refer to machine prediction of what information is important to people. Models of programmer attention are important because they help researchers build better interfaces, assistive technologies, and more human-like AI. For many years, researchers in SE have built these models based on features such as mouse clicks, key logging, and IDE interactions. Yet the holy grail in this area is scanpath prediction -- the prediction of the sequence of eye fixations a person would take over a visual stimulus. A person's eye movements are considered the most concrete evidence that a person is taking in a piece of information. Scanpath prediction is a notoriously difficult problem, but we believe that the emergence of lower-cost, higher-accuracy eye tracking equipment and better large language models of source code brings a solution within grasp. We present an eye tracking experiment with 27 programmers and a prototype scanpath predictor to present preliminary results and obtain early community feedback.",
        "Publication date": "26 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.13920"
    },
    {
        "ID": 461,
        "Title": "ZC3: Zero-Shot Cross-Language Code Clone Detection",
        "Authors": [
            "Jia Li",
            "Chongyang Tao",
            "Zhi Jin",
            "Fang Liu",
            "Jia Li",
            "Ge Li"
        ],
        "Abstract": "Developers introduce code clones to improve programming productivity. Many existing studies have achieved impressive performance in monolingual code clone detection. However, during software development, more and more developers write semantically equivalent programs with different languages to support different platforms and help developers translate projects from one language to another. Considering that collecting cross-language parallel data, especially for low-resource languages, is expensive and time-consuming, how designing an effective cross-language model that does not rely on any parallel data is a significant problem. In this paper, we propose a novel method named ZC3 for Zero-shot Cross-language Code Clone detection. ZC3 designs the contrastive snippet prediction to form an isomorphic representation space among different programming languages. Based on this, ZC3 exploits domain-aware learning and cycle consistency learning to further constrain the model to generate representations that are aligned among different languages meanwhile are diacritical for different types of clones. To evaluate our approach, we conduct extensive experiments on four representative cross-language clone detection datasets. Experimental results show that ZC3 outperforms the state-of-the-art baselines by 67.12%, 51.39%, 14.85%, and 53.01% on the MAP score, respectively. We further investigate the representational distribution of different languages and discuss the effectiveness of our method.",
        "Publication date": "7 September, 2023",
        "Link": "https://arxiv.org/pdf/2308.13754"
    },
    {
        "ID": 462,
        "Title": "On the Impact of Language Selection for Training and Evaluating Programming Language Models",
        "Authors": [
            "Jonathan Katzy",
            "Maliheh Izadi",
            "Arie van Deursen"
        ],
        "Abstract": "The recent advancements in Transformer-based Language Models have demonstrated significant potential in enhancing the multilingual capabilities of these models. The remarkable progress made in this domain not only applies to natural language tasks but also extends to the domain of programming languages. Despite the ability of these models to learn from multiple languages, evaluations typically focus on particular combinations of the same languages. In this study, we evaluate the similarity of programming languages by analyzing their representations using a CodeBERT-based model. Our experiments reveal that token representation in languages such as C++, Python, and Java exhibit proximity to one another, whereas the same tokens in languages such as Mathematica and R display significant dissimilarity. Our findings suggest that this phenomenon can potentially result in performance challenges when dealing with diverse languages. Thus, we recommend using our similarity measure to select a diverse set of programming languages when training and evaluating future models.",
        "Publication date": "25 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.13354"
    },
    {
        "ID": 463,
        "Title": "COCO: Testing Code Generation Systems via Concretized Instructions",
        "Authors": [
            "Ming Yan",
            "Junjie Chen",
            "Jie M. Zhang",
            "Xuejie Cao",
            "Chen Yang",
            "Mark Harman"
        ],
        "Abstract": "Code generation systems have been extensively developed in recent years to generate source code based on natural language instructions. However, despite their advancements, these systems still face robustness issues where even slightly different instructions can result in significantly different code semantics. Robustness is critical for code generation systems, as it can have significant impacts on software development, software quality, and trust in the generated code. Although existing testing techniques for general text-to-text software can detect some robustness issues, they are limited in effectiveness due to ignoring the characteristics of code generation systems. In this work, we propose a novel technique COCO to test the robustness of code generation systems. It exploits the usage scenario of code generation systems to make the original programming instruction more concrete by incorporating features known to be contained in the original code. A robust system should maintain code semantics for the concretized instruction, and COCO detects robustness inconsistencies when it does not. We evaluated COCO on eight advanced code generation systems, including commercial tools such as Copilot and ChatGPT, using two widely-used datasets. Our results demonstrate the effectiveness of COCO in testing the robustness of code generation systems, outperforming two techniques adopted from general text-to-text software testing by 466.66% and 104.02%, respectively. Furthermore, concretized instructions generated by COCO can help reduce robustness inconsistencies by 18.35% to 53.91% through fine-tuning.",
        "Publication date": "25 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.13319"
    },
    {
        "ID": 464,
        "Title": "Fortran High-Level Synthesis: Reducing the barriers to accelerating HPC codes on FPGAs",
        "Authors": [
            "Gabriel Rodriguez-Canal",
            "Nick Brown",
            "Tim Dykes",
            "Jessica R. Jones",
            "Utz-Uwe Haus"
        ],
        "Abstract": "In recent years the use of FPGAs to accelerate scientific applications has grown, with numerous applications demonstrating the benefit of FPGAs for high performance workloads. However, whilst High Level Synthesis (HLS) has significantly lowered the barrier to entry in programming FPGAs by enabling programmers to use C++, a major challenge is that most often these codes are not originally written in C++. Instead, Fortran is the lingua franca of scientific computing and-so it requires a complex and time consuming initial step to convert into C++ even before considering the FPGA.\n  In this paper we describe work enabling Fortran for AMD Xilinx FPGAs by connecting the LLVM Flang front end to AMD Xilinx's LLVM back end. This enables programmers to use Fortran as a first-class language for programming FPGAs, and as we demonstrate enjoy all the tuning and optimisation opportunities that HLS C++ provides. Furthermore, we demonstrate that certain language features of Fortran make it especially beneficial for programming FPGAs compared to C++. The result of this work is a lowering of the barrier to entry in using FPGAs for scientific computing, enabling programmers to leverage their existing codebase and language of choice on the FPGA directly.",
        "Publication date": "25 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.13274"
    },
    {
        "ID": 465,
        "Title": "PEak: A Single Source of Truth for Hardware Design and Verification",
        "Authors": [
            "Caleb Donovick",
            "Ross Daly",
            "Jackson Melchert",
            "Lenny Truong",
            "Priyanka Raina",
            "Pat Hanrahan",
            "Clark Barrett"
        ],
        "Abstract": "Domain-specific languages for hardware can significantly enhance designer productivity, but sometimes at the cost of ease of verification. On the other hand, ISA specification languages are too static to be used during early stage design space exploration. We present PEak, an open-source hardware design and specification language, which aims to improve both design productivity and verification capability. PEak does this by providing a single source of truth for functional models, formal specifications, and RTL. PEak has been used in several academic projects, and PEak-generated RTL has been included in three fabricated hardware accelerators. In these projects, the formal capabilities of PEak were crucial for enabling both novel design space exploration techniques and automated compiler synthesis.",
        "Publication date": "24 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.13106"
    },
    {
        "ID": 466,
        "Title": "Code Llama: Open Foundation Models for Code",
        "Authors": [
            "Baptiste RoziÃ¨re",
            "Jonas Gehring",
            "Fabian Gloeckle",
            "Sten Sootla",
            "Itai Gat",
            "Xiaoqing Ellen Tan",
            "Yossi Adi",
            "Jingyu Liu",
            "Romain Sauvestre",
            "Tal Remez",
            "JÃ©rÃ©my Rapin",
            "Artyom Kozhevnikov",
            "Ivan Evtimov",
            "Joanna Bitton",
            "Manish Bhatt",
            "Cristian Canton Ferrer",
            "Aaron Grattafiori",
            "Wenhan Xiong",
            "Alexandre DÃ©fossez",
            "Jade Copet",
            "Faisal Azhar",
            "Hugo Touvron",
            "Louis Martin",
            "Nicolas Usunier",
            "Thomas Scialom"
        ],
        "Abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",
        "Publication date": "31 January, 2024",
        "Link": "https://arxiv.org/pdf/2308.12950"
    },
    {
        "ID": 467,
        "Title": "Dcc --help: Generating Context-Aware Compiler Error Explanations with Large Language Models",
        "Authors": [
            "Andrew Taylor",
            "Alexandra Vassar",
            "Jake Renzella",
            "Hammond Pearce"
        ],
        "Abstract": "In the challenging field of introductory programming, high enrollments and failure rates drive us to explore tools and systems to enhance student outcomes, especially automated tools that scale to large cohorts. This paper presents and evaluates the dcc --help tool, an integration of a Large Language Model (LLM) into the Debugging C Compiler (DCC) to generate unique, novice-focused explanations tailored to each error. dcc --help prompts an LLM with contextual information of compile- and run-time error occurrences, including the source code, error location and standard compiler error message. The LLM is instructed to generate novice-focused, actionable error explanations and guidance, designed to help students understand and resolve problems without providing solutions. dcc --help was deployed to our CS1 and CS2 courses, with 2,565 students using the tool over 64,000 times in ten weeks. We analysed a subset of these error/explanation pairs to evaluate their properties, including conceptual correctness, relevancy, and overall quality. We found that the LLM-generated explanations were conceptually accurate in 90% of compile-time and 75% of run-time cases, but often disregarded the instruction not to provide solutions in code. Our findings, observations and reflections following deployment indicate that dcc-help provides novel opportunities for scaffolding students' introduction to programming.",
        "Publication date": "15 October, 2023",
        "Link": "https://arxiv.org/pdf/2308.11873"
    },
    {
        "ID": 468,
        "Title": "Spectral Bounds on Hyperbolic 3-Manifolds: Associativity and the Trace Formula",
        "Authors": [
            "James Bonifacio",
            "Dalimil Mazac",
            "Sridip Pal"
        ],
        "Abstract": "We constrain the low-energy spectra of Laplace operators on closed hyperbolic manifolds and orbifolds in three dimensions, including the standard Laplace-Beltrami operator on functions and the Laplacian on powers of the cotangent bundle. Our approach employs linear programming techniques to derive rigorous bounds by leveraging two types of spectral identities. The first type, inspired by the conformal bootstrap, arises from the consistency of the spectral decomposition of the product of Laplace eigensections, and involves the Laplacian spectra as well as integrals of triple products of eigensections. We formulate these conditions in the language of representation theory of $\\mathrm{PSL}_2(\\mathbb{C})$ and use them to prove upper bounds on the first and second Laplacian eigenvalues. The second type of spectral identities follows from the Selberg trace formula. We use them to find upper bounds on the spectral gap of the Laplace-Beltrami operator on hyperbolic 3-orbifolds, as well as on the systole length of hyperbolic 3-manifolds, as a function of the volume. Further, we prove that the spectral gap $Î»_1$ of the Laplace-Beltrami operator on all closed hyperbolic 3-manifolds satisfies $Î»_1 < 47.32$. Along the way, we use the trace formula to estimate the low-energy spectra of a large set of example orbifolds and compare them with our general bounds, finding that the bounds are nearly sharp in several cases.",
        "Publication date": "22 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.11174"
    },
    {
        "ID": 469,
        "Title": "Elucidating STEM Concepts through Generative AI: A Multi-modal Exploration of Analogical Reasoning",
        "Authors": [
            "Chen Cao",
            "Zijian Ding",
            "Gyeong-Geon Lee",
            "Jiajun Jiao",
            "Jionghao Lin",
            "Xiaoming Zhai"
        ],
        "Abstract": "This study explores the integration of generative artificial intelligence (AI), specifically large language models, with multi-modal analogical reasoning as an innovative approach to enhance science, technology, engineering, and mathematics (STEM) education. We have developed a novel system that utilizes the capacities of generative AI to transform intricate principles in mathematics, physics, and programming into comprehensible metaphors. To further augment the educational experience, these metaphors are subsequently converted into visual form. Our study aims to enhance the learners' understanding of STEM concepts and their learning engagement by using the visual metaphors. We examine the efficacy of our system via a randomized A/B/C test, assessing learning gains and motivation shifts among the learners. Our study demonstrates the potential of applying large language models to educational practice on STEM subjects. The results will shed light on the design of educational system in terms of harnessing AI's potential to empower educational stakeholders.",
        "Publication date": "21 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.10454"
    },
    {
        "ID": 470,
        "Title": "Local Periodicity-Based Beat Tracking for Expressive Classical Piano Music",
        "Authors": [
            "Ching-Yu Chiu",
            "Meinard MÃ¼ller",
            "Matthew E. P. Davies",
            "Alvin Wen-Yu Su",
            "Yi-Hsuan Yang"
        ],
        "Abstract": "To model the periodicity of beats, state-of-the-art beat tracking systems use \"post-processing trackers\" (PPTs) that rely on several empirically determined global assumptions for tempo transition, which work well for music with a steady tempo. For expressive classical music, however, these assumptions can be too rigid. With two large datasets of Western classical piano music, namely the Aligned Scores and Performances (ASAP) dataset and a dataset of Chopin's Mazurkas (Maz-5), we report on experiments showing the failure of existing PPTs to cope with local tempo changes, thus calling for new methods. In this paper, we propose a new local periodicity-based PPT, called predominant local pulse-based dynamic programming (PLPDP) tracking, that allows for more flexible tempo transitions. Specifically, the new PPT incorporates a method called \"predominant local pulses\" (PLP) in combination with a dynamic programming (DP) component to jointly consider the locally detected periodicity and beat activation strength at each time instant. Accordingly, PLPDP accounts for the local periodicity, rather than relying on a global tempo assumption. Compared to existing PPTs, PLPDP particularly enhances the recall values at the cost of a lower precision, resulting in an overall improvement of F1-score for beat tracking in ASAP (from 0.473 to 0.493) and Maz-5 (from 0.595 to 0.838).",
        "Publication date": "20 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.10355"
    },
    {
        "ID": 471,
        "Title": "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs",
        "Authors": [
            "Federico Cassano",
            "John Gouwar",
            "Francesca Lucchetti",
            "Claire Schlesinger",
            "Anders Freeman",
            "Carolyn Jane Anderson",
            "Molly Q Feldman",
            "Michael Greenberg",
            "Abhinav Jangda",
            "Arjun Guha"
        ],
        "Abstract": "Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available. Low resource languages include OCaml, Racket, and several others.\n  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, MultiPL-T, translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize tests for commented code from a high-resource language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate Python code to a target low-resource language, and use tests to validate the translation. We apply this approach to generate tens of thousands of validated training items for Julia, Lua, OCaml, R, and Racket. Furthermore, we use an open model (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done.\n  With MultiPL-T generated data, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket. On established benchmarks (MultiPL-E), these models outperform other open Code LLMs. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.",
        "Publication date": "21 September, 2024",
        "Link": "https://arxiv.org/pdf/2308.09895"
    },
    {
        "ID": 472,
        "Title": "Quantifying OpenMP: Statistical Insights into Usage and Adoption",
        "Authors": [
            "Tal Kadosh",
            "Niranjan Hasabnis",
            "Timothy Mattson",
            "Yuval Pinter",
            "Gal Oren"
        ],
        "Abstract": "In high-performance computing (HPC), the demand for efficient parallel programming models has grown dramatically since the end of Dennard Scaling and the subsequent move to multi-core CPUs. OpenMP stands out as a popular choice due to its simplicity and portability, offering a directive-driven approach for shared-memory parallel programming. Despite its wide adoption, however, there is a lack of comprehensive data on the actual usage of OpenMP constructs, hindering unbiased insights into its popularity and evolution. This paper presents a statistical analysis of OpenMP usage and adoption trends based on a novel and extensive database, HPCORPUS, compiled from GitHub repositories containing C, C++, and Fortran code. The results reveal that OpenMP is the dominant parallel programming model, accounting for 45% of all analyzed parallel APIs. Furthermore, it has demonstrated steady and continuous growth in popularity over the past decade. Analyzing specific OpenMP constructs, the study provides in-depth insights into their usage patterns and preferences across the three languages. Notably, we found that while OpenMP has a strong \"common core\" of constructs in common usage (while the rest of the API is less used), there are new adoption trends as well, such as simd and target directives for accelerated computing and task for irregular parallelism. Overall, this study sheds light on OpenMP's significance in HPC applications and provides valuable data for researchers and practitioners. It showcases OpenMP's versatility, evolving adoption, and relevance in contemporary parallel programming, underlining its continued role in HPC applications and beyond. These statistical insights are essential for making informed decisions about parallelization strategies and provide a foundation for further advancements in parallel programming models and techniques.",
        "Publication date": "17 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.08002"
    },
    {
        "ID": 473,
        "Title": "Fuzzing for CPS Mutation Testing",
        "Authors": [
            "Jaekwon Lee",
            "Enrico ViganÃ²",
            "Oscar Cornejo",
            "Fabrizio Pastore",
            "Lionel Briand"
        ],
        "Abstract": "Mutation testing can help reduce the risks of releasing faulty software. For such reason, it is a desired practice for the development of embedded software running in safety-critical cyber-physical systems (CPS). Unfortunately, state-of-the-art test data generation techniques for mutation testing of C and C++ software, two typical languages for CPS software, rely on symbolic execution, whose limitations often prevent its application (e.g., it cannot test black-box components).\n  We propose a mutation testing approach that leverages fuzz testing, which has proved effective with C and C++ software. Fuzz testing automatically generates diverse test inputs that exercise program branches in a varied number of ways and, therefore, exercise statements in different program states, thus maximizing the likelihood of killing mutants, our objective.\n  We performed an empirical assessment of our approach with software components used in satellite systems currently in orbit. Our empirical evaluation shows that mutation testing based on fuzz testing kills a significantly higher proportion of live mutants than symbolic execution (i.e., up to an additional 47 percentage points). Further, when symbolic execution cannot be applied, fuzz testing provides significant benefits (i.e., up to 41% mutants killed). Our study is the first one comparing fuzz testing and symbolic execution for mutation testing; our results provide guidance towards the development of fuzz testing tools dedicated to mutation testing.",
        "Publication date": "15 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.07949"
    },
    {
        "ID": 474,
        "Title": "A C++ program for estimating detector sensitivities to long-lived particles: Displaced Decay Counter",
        "Authors": [
            "Florian Domingo",
            "Julian GÃ¼nther",
            "Jong Soo Kim",
            "Zeren Simon Wang"
        ],
        "Abstract": "A series of far-detector programs have been proposed for operation at various interaction points of the Large Hadron Collider during the upcoming runs. Investigating the potential and complementarity of these experiments for new-physics searches goes through the estimation of their sensitivity to specific long-lived particle models. Here, we present an integrated numerical tool written in the C++ language and called Displaced Decay Counter, which we have created to this end and which can be used in association with MadGraph5, Pythia8, or any other state-of-the-art Monte-Carlo collider simulation tool. Several far-detector models have been implemented within the program, accounting for the geometry and integrated luminosity of projected detectors. Additional or more accurate designs can be easily constructed through a dedicated interface. The functionality of this tool is exemplified through the discussion of three benchmark scenarios, which we consider for the validation of the implemented detector models.",
        "Publication date": "3 July, 2024",
        "Link": "https://arxiv.org/pdf/2308.07371"
    },
    {
        "ID": 475,
        "Title": "OctoPack: Instruction Tuning Code Large Language Models",
        "Authors": [
            "Niklas Muennighoff",
            "Qian Liu",
            "Armel Zebaze",
            "Qinkai Zheng",
            "Binyuan Hui",
            "Terry Yue Zhuo",
            "Swayam Singh",
            "Xiangru Tang",
            "Leandro von Werra",
            "Shayne Longpre"
        ],
        "Abstract": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.",
        "Publication date": "18 February, 2024",
        "Link": "https://arxiv.org/pdf/2308.07124"
    },
    {
        "ID": 476,
        "Title": "Fast Deterministic Black-box Context-free Grammar Inference",
        "Authors": [
            "Mohammad Rifat Arefin",
            "Suraj Shetiya",
            "Zili Wang",
            "Christoph Csallner"
        ],
        "Abstract": "Black-box context-free grammar inference is a hard problem as in many practical settings it only has access to a limited number of example programs. The state-of-the-art approach Arvada heuristically generalizes grammar rules starting from flat parse trees and is non-deterministic to explore different generalization sequences. We observe that many of Arvada's generalization steps violate common language concept nesting rules. We thus propose to pre-structure input programs along these nesting rules, apply learnt rules recursively, and make black-box context-free grammar inference deterministic. The resulting TreeVada yielded faster runtime and higher-quality grammars in an empirical comparison. The TreeVada source code, scripts, evaluation parameters, and training data are open-source and publicly available (https://doi.org/10.6084/m9.figshare.23907738).",
        "Publication date": "16 January, 2024",
        "Link": "https://arxiv.org/pdf/2308.06163"
    },
    {
        "ID": 477,
        "Title": "ESBMC v7.3: Model Checking C++ Programs using Clang AST",
        "Authors": [
            "Kunjian Song",
            "Mikhail R. Gadelha",
            "Franz BrauÃe",
            "Rafael S. Menezes",
            "Lucas C. Cordeiro"
        ],
        "Abstract": "This paper introduces ESBMC v7.3, the latest Efficient SMT-Based Context-Bounded Model Checker version, which now incorporates a new clang-based C++ front-end. While the previous CPROVER-based front-end served well for handling C++03 programs, it encountered challenges keeping up with the evolving C++ language. As new language and library features were added in each C++ version, the limitations of the old front-end became apparent, leading to difficult-to-maintain code. Consequently, modern C++ programs were challenging to verify. To overcome this obstacle, we redeveloped the front-end, opting for a more robust approach using clang. The new front-end efficiently traverses the Abstract Syntax Tree (AST) in-memory using clang APIs and transforms each AST node into ESBMC's Intermediate Representation. Through extensive experimentation, our results demonstrate that ESBMC v7.3 with the new front-end significantly reduces parse and conversion errors, enabling successful verification of a wide range of C++ programs, thereby outperforming previous ESBMC versions.",
        "Publication date": "10 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.05649"
    },
    {
        "ID": 478,
        "Title": "Fixing Rust Compilation Errors using LLMs",
        "Authors": [
            "Pantazis Deligiannis",
            "Akash Lal",
            "Nikita Mehrotra",
            "Aseem Rastogi"
        ],
        "Abstract": "The Rust programming language, with its safety guarantees, has established itself as a viable choice for low-level systems programming language over the traditional, unsafe alternatives like C/C++. These guarantees come from a strong ownership-based type system, as well as primitive support for features like closures, pattern matching, etc., that make the code more concise and amenable to reasoning. These unique Rust features also pose a steep learning curve for programmers.\n  This paper presents a tool called RustAssistant that leverages the emergent capabilities of Large Language Models (LLMs) to automatically suggest fixes for Rust compilation errors. RustAssistant uses a careful combination of prompting techniques as well as iteration with an LLM to deliver high accuracy of fixes. RustAssistant is able to achieve an impressive peak accuracy of roughly 74% on real-world compilation errors in popular open-source Rust repositories. We plan to release our dataset of Rust compilation errors to enable further research.",
        "Publication date": "9 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.05177"
    },
    {
        "ID": 479,
        "Title": "Fuzz4All: Universal Fuzzing with Large Language Models",
        "Authors": [
            "Chunqiu Steven Xia",
            "Matteo Paltenghi",
            "Jia Le Tian",
            "Michael Pradel",
            "Lingming Zhang"
        ],
        "Abstract": "Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 64 bugs already confirmed by developers as previously unknown.",
        "Publication date": "15 January, 2024",
        "Link": "https://arxiv.org/pdf/2308.04748"
    },
    {
        "ID": 480,
        "Title": "Generative AI in Computing Education: Perspectives of Students and Instructors",
        "Authors": [
            "Cynthia Zastudil",
            "Magdalena Rogalska",
            "Christine Kapp",
            "Jennifer Vaughn",
            "Stephen MacNeil"
        ],
        "Abstract": "Generative models are now capable of producing natural language text that is, in some cases, comparable in quality to the text produced by people. In the computing education context, these models are being used to generate code, code explanations, and programming exercises. The rapid adoption of these models has prompted multiple position papers and workshops which discuss the implications of these models for computing education, both positive and negative. This paper presents results from a series of semi-structured interviews with 12 students and 6 instructors about their awareness, experiences, and preferences regarding the use of tools powered by generative AI in computing classrooms. The results suggest that Generative AI (GAI) tools will play an increasingly significant role in computing education. However, students and instructors also raised numerous concerns about how these models should be integrated to best support the needs and learning goals of students. We also identified interesting tensions and alignments that emerged between how instructors and students prefer to engage with these models. We discuss these results and provide recommendations related to curriculum development, assessment methods, and pedagogical practice. As GAI tools become increasingly prevalent, it's important to understand educational stakeholders' preferences and values to ensure that these tools can be used for good and that potential harms can be mitigated.",
        "Publication date": "8 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.04309"
    },
    {
        "ID": 481,
        "Title": "Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts",
        "Authors": [
            "Tyler Angert",
            "Miroslav Ivan Suzara",
            "Jenny Han",
            "Christopher Lawrence Pondoc",
            "Hariharan Subramonyam"
        ],
        "Abstract": "Creative coding tasks are often exploratory in nature. When producing digital artwork, artists usually begin with a high-level semantic construct such as a \"stained glass filter\" and programmatically implement it by varying code parameters such as shape, color, lines, and opacity to produce visually appealing results. Based on interviews with artists, it can be effortful to translate semantic constructs to program syntax, and current programming tools don't lend well to rapid creative exploration. To address these challenges, we introduce Spellburst, a large language model (LLM) powered creative-coding environment. Spellburst provides (1) a node-based interface that allows artists to create generative art and explore variations through branching and merging operations, (2) expressive prompt-based interactions to engage in semantic programming, and (3) dynamic prompt-driven interfaces and direct code editing to seamlessly switch between semantic and syntactic exploration. Our evaluation with artists demonstrates Spellburst's potential to enhance creative coding practices and inform the design of computational creativity tools that bridge semantic and syntactic spaces.",
        "Publication date": "7 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.03921"
    },
    {
        "ID": 482,
        "Title": "Studying Large Language Model Generalization with Influence Functions",
        "Authors": [
            "Roger Grosse",
            "Juhan Bae",
            "Cem Anil",
            "Nelson Elhage",
            "Alex Tamkin",
            "Amirhossein Tajdini",
            "Benoit Steiner",
            "Dustin Li",
            "Esin Durmus",
            "Ethan Perez",
            "Evan Hubinger",
            "KamilÄ LukoÅ¡iÅ«tÄ",
            "Karina Nguyen",
            "Nicholas Joseph",
            "Sam McCandlish",
            "Jared Kaplan",
            "Samuel R. Bowman"
        ],
        "Abstract": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.",
        "Publication date": "7 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.03296"
    },
    {
        "ID": 483,
        "Title": "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code",
        "Authors": [
            "Rangeet Pan",
            "Ali Reza Ibrahimzada",
            "Rahul Krishna",
            "Divya Sankar",
            "Lambert Pouguem Wassi",
            "Michele Merler",
            "Boris Sobolev",
            "Raju Pavuluri",
            "Saurabh Sinha",
            "Reyhaneh Jabbarvand"
        ],
        "Abstract": "Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation -- with correct translations ranging from 2.1% to 47.3% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset -- consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs -- can help drive research in this area.",
        "Publication date": "16 January, 2024",
        "Link": "https://arxiv.org/pdf/2308.03109"
    },
    {
        "ID": 484,
        "Title": "dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning",
        "Authors": [
            "Renato Lui Geh",
            "Jonas GonÃ§alves",
            "Igor Cataneo Silveira",
            "Denis Deratani MauÃ¡",
            "Fabio Gagliardi Cozman"
        ],
        "Abstract": "We present dPASP, a novel declarative probabilistic logic programming framework for differentiable neuro-symbolic reasoning. The framework allows for the specification of discrete probabilistic models with neural predicates, logic constraints and interval-valued probabilistic choices, thus supporting models that combine low-level perception (images, texts, etc), common-sense reasoning, and (vague) statistical knowledge. To support all such features, we discuss the several semantics for probabilistic logic programs that can express nondeterministic, contradictory, incomplete and/or statistical knowledge. We also discuss how gradient-based learning can be performed with neural predicates and probabilistic choices under selected semantics. We then describe an implemented package that supports inference and learning in the language, along with several example programs. The package requires minimal user knowledge of deep learning system's inner workings, while allowing end-to-end training of rather sophisticated models and loss functions.",
        "Publication date": "5 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.02944"
    },
    {
        "ID": 485,
        "Title": "PyPartMC: A Pythonic interface to a particle-resolved, Monte Carlo aerosol simulation framework",
        "Authors": [
            "Zachary D'Aquino",
            "Sylwester Arabas",
            "Jeffrey Curtis",
            "Akshunna Vaishnav",
            "Nicole Riemer",
            "Matthew West"
        ],
        "Abstract": "PyPartMC is a Pythonic interface to PartMC, a stochastic, particle-resolved aerosol model implemented in Fortran. Both PyPartMC and PartMC are free, libre, and open-source. PyPartMC reduces the number of steps and mitigates the effort necessary to install and utilize the resources of PartMC. Without PyPartMC, setting up PartMC requires: working with UNIX shell, providing Fortran and C libraries, and performing standard Fortran and C source code configuration, compilation and linking. This can be challenging for those less experienced with computational research or those intending to use PartMC in environments where provision of UNIX tools is less straightforward (e.g., on Windows). PyPartMC offers a single-step installation/upgrade process of PartMC and all dependencies through the pip Python package manager on Linux, macOS, and Windows. This allows streamlined access to the unmodified and versioned Fortran internals of the PartMC codebase from both Python and other interoperable environments (e.g., Julia through PyCall). Consequently, users of PyPartMC can setup, run, process and visualize output of PartMC simulations using a single general-purpose programming language.",
        "Publication date": "19 December, 2023",
        "Link": "https://arxiv.org/pdf/2308.02052"
    },
    {
        "ID": 486,
        "Title": "cuQuantum SDK: A High-Performance Library for Accelerating Quantum Science",
        "Authors": [
            "Harun Bayraktar",
            "Ali Charara",
            "David Clark",
            "Saul Cohen",
            "Timothy Costa",
            "Yao-Lung L. Fang",
            "Yang Gao",
            "Jack Guan",
            "John Gunnels",
            "Azzam Haidar",
            "Andreas Hehn",
            "Markus Hohnerbach",
            "Matthew Jones",
            "Tom Lubowe",
            "Dmitry Lyakh",
            "Shinya Morino",
            "Paul Springer",
            "Sam Stanwyck",
            "Igor Terentyev",
            "Satya Varadhan",
            "Jonathan Wong",
            "Takuma Yamaguchi"
        ],
        "Abstract": "We present the NVIDIA cuQuantum SDK, a state-of-the-art library of composable primitives for GPU-accelerated quantum circuit simulations. As the size of quantum devices continues to increase, making their classical simulation progressively more difficult, the availability of fast and scalable quantum circuit simulators becomes vital for quantum algorithm developers, as well as quantum hardware engineers focused on the validation and optimization of quantum devices. The cuQuantum SDK was created to accelerate and scale up quantum circuit simulators developed by the quantum information science community by enabling them to utilize efficient scalable software building blocks optimized for NVIDIA GPU platforms. The functional building blocks provided cover the needs of both state vector- and tensor network- based simulators, including approximate tensor network simulation methods based on matrix product state, projected entangled pair state, and other factorized tensor representations. By leveraging the enormous computing power of the latest NVIDIA GPU architectures, quantum circuit simulators that have adopted the cuQuantum SDK demonstrate significant acceleration, compared to CPU-only execution, for both the state vector and tensor network simulation methods. Furthermore, by utilizing the parallel primitives available in the cuQuantum SDK, one can easily transition to distributed GPU-accelerated platforms, including those furnished by cloud service providers and high-performance computing systems deployed by supercomputing centers, extending the scale of possible quantum circuit simulations. The rich capabilities provided by the SDK are conveniently made available via both Python and C application programming interfaces, where the former is directly targeting a broad Python quantum community and the latter allows tight integration with simulators written in any programming language.",
        "Publication date": "3 August, 2023",
        "Link": "https://arxiv.org/pdf/2308.01999"
    },
    {
        "ID": 487,
        "Title": "Density-Based Semantics for Reactive Probabilistic Programming",
        "Authors": [
            "Guillaume Baudart",
            "Louis Mandel",
            "Christine Tasson"
        ],
        "Abstract": "Synchronous languages are now a standard industry tool for critical embedded systems. Designers write high-level specifications by composing streams of values using block diagrams. These languages have been extended with Bayesian reasoning to program state-space models which compute a stream of distributions given a stream of observations. However, the semantics of probabilistic models is only defined for scheduled equations -- a significant limitation compared to dataflow synchronous languages and block diagrams which do not require any ordering.\n  In this paper we propose two schedule agnostic semantics for a probabilistic synchronous language. The key idea is to interpret probabilistic expressions as a stream of un-normalized density functions which maps random variable values to a result and positive score. The co-iterative semantics interprets programs as state machines and equations are computed using a fixpoint operator. The relational semantics directly manipulates streams and is thus a better fit to reason about program equivalence. We use the relational semantics to prove the correctness of a program transformation required to run an optimized inference algorithm for state-space models with constant parameters.",
        "Publication date": "7 September, 2023",
        "Link": "https://arxiv.org/pdf/2308.01676"
    },
    {
        "ID": 488,
        "Title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
        "Authors": [
            "Sirui Hong",
            "Mingchen Zhuge",
            "Jonathan Chen",
            "Xiawu Zheng",
            "Yuheng Cheng",
            "Ceyao Zhang",
            "Jinlin Wang",
            "Zili Wang",
            "Steven Ka Shing Yau",
            "Zijuan Lin",
            "Liyang Zhou",
            "Chenyu Ran",
            "Lingfeng Xiao",
            "Chenglin Wu",
            "JÃ¼rgen Schmidhuber"
        ],
        "Abstract": "Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at https://github.com/geekan/MetaGPT",
        "Publication date": "6 November, 2023",
        "Link": "https://arxiv.org/pdf/2308.00352"
    },
    {
        "ID": 489,
        "Title": "Pyrus Base: An Open Source Python Framework for the RoboCup 2D Soccer Simulation",
        "Authors": [
            "Nader Zare",
            "Aref Sayareh",
            "Omid Amini",
            "Mahtab Sarvmaili",
            "Arad Firouzkouhi",
            "Stan Matwin",
            "Amilcar Soares"
        ],
        "Abstract": "Soccer, also known as football in some parts of the world, involves two teams of eleven players whose objective is to score more goals than the opposing team. To simulate this game and attract scientists from all over the world to conduct research and participate in an annual computer-based soccer world cup, Soccer Simulation 2D (SS2D) was one of the leagues initiated in the RoboCup competition. In every SS2D game, two teams of 11 players and one coach connect to the RoboCup Soccer Simulation Server and compete against each other. Over the past few years, several C++ base codes have been employed to control agents' behavior and their communication with the server. Although C++ base codes have laid the foundation for the SS2D, developing them requires an advanced level of C++ programming. C++ language complexity is a limiting disadvantage of C++ base codes for all users, especially for beginners. To conquer the challenges of C++ base codes and provide a powerful baseline for developing machine learning concepts, we introduce Pyrus, the first Python base code for SS2D. Pyrus is developed to encourage researchers to efficiently develop their ideas and integrate machine learning algorithms into their teams. Pyrus base is open-source code, and it is publicly available under MIT License on GitHub",
        "Publication date": "21 July, 2023",
        "Link": "https://arxiv.org/pdf/2307.16875"
    },
    {
        "ID": 490,
        "Title": "SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation",
        "Authors": [
            "Haiyue Song",
            "Raj Dabre",
            "Chenhui Chu",
            "Sadao Kurohashi",
            "Eiichiro Sumita"
        ],
        "Abstract": "Sub-word segmentation is an essential pre-processing step for Neural Machine Translation (NMT). Existing work has shown that neural sub-word segmenters are better than Byte-Pair Encoding (BPE), however, they are inefficient as they require parallel corpora, days to train and hours to decode. This paper introduces SelfSeg, a self-supervised neural sub-word segmentation method that is much faster to train/decode and requires only monolingual dictionaries instead of parallel corpora. SelfSeg takes as input a word in the form of a partially masked character sequence, optimizes the word generation probability and generates the segmentation with the maximum posterior probability, which is calculated using a dynamic programming algorithm. The training time of SelfSeg depends on word frequencies, and we explore several word frequency normalization strategies to accelerate the training phase. Additionally, we propose a regularization mechanism that allows the segmenter to generate various segmentations for one word. To show the effectiveness of our approach, we conduct MT experiments in low-, middle- and high-resource scenarios, where we compare the performance of using different segmentation methods. The experimental results demonstrate that on the low-resource ALT dataset, our method achieves more than 1.2 BLEU score improvement compared with BPE and SentencePiece, and a 1.1 score improvement over Dynamic Programming Encoding (DPE) and Vocabulary Learning via Optimal Transport (VOLT) on average. The regularization method achieves approximately a 4.3 BLEU score improvement over BPE and a 1.2 BLEU score improvement over BPE-dropout, the regularized version of BPE. We also observed significant improvements on IWSLT15 Vi->En, WMT16 Ro->En and WMT15 Fi->En datasets, and competitive results on the WMT14 De->En and WMT14 Fr->En datasets.",
        "Publication date": "31 July, 2023",
        "Link": "https://arxiv.org/pdf/2307.16400"
    },
    {
        "ID": 491,
        "Title": "RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics",
        "Authors": [
            "Ajay Bati",
            "Spencer H. Bryngelson"
        ],
        "Abstract": "The rise of neural network-based machine learning ushered in high-level libraries, including TensorFlow and PyTorch, to support their functionality. Computational fluid dynamics (CFD) researchers have benefited from this trend and produced powerful neural networks that promise shorter simulation times. For example, multilayer perceptrons (MLPs) and Long Short Term Memory (LSTM) recurrent-based (RNN) architectures can represent sub-grid physical effects, like turbulence. Implementing neural networks in CFD solvers is challenging because the programming languages used for machine learning and CFD are mostly non-overlapping, We present the roseNNa library, which bridges the gap between neural network inference and CFD. RoseNNa is a non-invasive, lightweight (1000 lines), and performant tool for neural network inference, with focus on the smaller networks used to augment PDE solvers, like those of CFD, which are typically written in C/C++ or Fortran. RoseNNa accomplishes this by automatically converting trained models from typical neural network training packages into a high-performance Fortran library with C and Fortran APIs. This reduces the effort needed to access trained neural networks and maintains performance in the PDE solvers that CFD researchers build and rely upon. Results show that RoseNNa reliably outperforms PyTorch (Python) and libtorch (C++) on MLPs and LSTM RNNs with less than 100 hidden layers and 100 neurons per layer, even after removing the overhead cost of API calls. Speedups range from a factor of about 10 and 2 faster than these established libraries for the smaller and larger ends of the neural network size ranges tested.",
        "Publication date": "30 July, 2023",
        "Link": "https://arxiv.org/pdf/2307.16322"
    },
    {
        "ID": 492,
        "Title": "Multilingual Code Co-Evolution Using Large Language Models",
        "Authors": [
            "Jiyang Zhang",
            "Pengyu Nie",
            "Junyi Jessy Li",
            "Milos Gligoric"
        ],
        "Abstract": "Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.",
        "Publication date": "11 September, 2023",
        "Link": "https://arxiv.org/pdf/2307.14991"
    },
    {
        "ID": 493,
        "Title": "Fuzzy order-sorted feature logic",
        "Authors": [
            "Gian Carlo Milanese",
            "Gabriella Pasi"
        ],
        "Abstract": "Order-Sorted Feature (OSF) logic is a knowledge representation and reasoning language based on function-denoting feature symbols and set-denoting sort symbols ordered in a subsumption lattice. OSF logic allows the construction of record-like terms that represent classes of entities and that are themselves ordered in a subsumption relation. The unification algorithm for such structures provides an efficient calculus of type subsumption, which has been applied in computational linguistics and implemented in constraint logic programming languages such as LOGIN and LIFE and automated reasoners such as CEDAR. This work generalizes OSF logic to a fuzzy setting. We give a flexible definition of a fuzzy subsumption relation which generalizes Zadeh's inclusion between fuzzy sets. Based on this definition we define a fuzzy semantics of OSF logic where sort symbols and OSF terms denote fuzzy sets. We extend the subsumption relation to OSF terms and prove that it constitutes a fuzzy partial order with the property that two OSF terms are subsumed by one another in the crisp sense if and only if their subsumption degree is greater than 0. We show how to find the greatest lower bound of two OSF terms by unifying them and how to compute the subsumption degree between two OSF terms, and we provide the complexity of these operations.",
        "Publication date": "20 November, 2023",
        "Link": "https://arxiv.org/pdf/2307.14669"
    },
    {
        "ID": 494,
        "Title": "Modal Abstractions for Virtualizing Memory Addresses",
        "Authors": [
            "Ismail Kuru",
            "Colin S. Gordon"
        ],
        "Abstract": "Operating system kernels employ virtual memory subsystems, which use a CPU's memory management units (MMUs) to virtualize the addresses of memory regions Operating systems manipulate these virtualized memory mappings to isolate untrusted processes, restrict which memory is accessible to different processes, hide memory limits from user programs, ensure process isolation, implement demand-paging and copy-on-write behaviors for performance and resource controls.\n  Virtual memory management (VMM) code is a critical piece of general-purpose OS kernels, but verification of this functionality is challenging due to the complexity of the hardware interface. In this paper, we introduce a modal abstraction to describe the truth of assertions relative to a specific virtual address space: [r]P indicating that P holds in the virtual address space rooted at r. Such modal assertions allow different address spaces to refer to each other, enabling complete verification of instruction sequences manipulating multiple address spaces. Using them effectively requires working with other assertions, such as points-to assertions in our separation logic, as relative to a given address space. We therefore define virtual points-to relations, which mimic hardware address translation, relative to a page table root. We demonstrate our approach with challenging fragments of VMM code showing that our approach handles examples beyond what prior work can address, including reasoning about a sequence of instructions as it changes address spaces. All definitions and theorems mentioned in this paper including the operational model of a RISC-like fragment of x86-64, a simple language run on this operational model, and a logic as an instantiation of the Iris framework are mechanized inside Coq.",
        "Publication date": "14 September, 2024",
        "Link": "https://arxiv.org/pdf/2307.14471"
    },
    {
        "ID": 495,
        "Title": "Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models",
        "Authors": [
            "Chee Wei Tan",
            "Shangxin Guo",
            "Man Fai Wong",
            "Ching Nam Hang"
        ],
        "Abstract": "This paper presents an AI-assisted programming tool called Copilot for Xcode for program composition and design to support human software developers. By seamlessly integrating cloud-based Large Language Models (LLM) with Apple's local development environment, Xcode, this tool enhances productivity and unleashes creativity for software development in Apple software ecosystem (e.g., iOS apps, macOS). Leveraging advanced natural language processing (NLP) techniques, Copilot for Xcode effectively processes source code tokens and patterns within code repositories, enabling features such as code generation, autocompletion, documentation, and error detection. Software developers can also query and make \"small\" decisions for program composition, some of which can be made simultaneously, and this is facilitated through prompt engineering in a chat interface of Copilot for Xcode. Finally, we present simple case studies as evidence of the effectiveness of utilizing NLP in Xcode to prompt popular LLM services like OpenAI ChatGPT for program composition and design.",
        "Publication date": "8 July, 2023",
        "Link": "https://arxiv.org/pdf/2307.14349"
    },
    {
        "ID": 496,
        "Title": "HasTEE: Programming Trusted Execution Environments with Haskell",
        "Authors": [
            "Abhiroop Sarkar",
            "Robert Krook",
            "Alejandro Russo",
            "Koen Claessen"
        ],
        "Abstract": "Trusted Execution Environments (TEEs) are hardware-enforced memory isolation units, emerging as a pivotal security solution for security-critical applications. TEEs, like Intel SGX and ARM TrustZone, allow the isolation of confidential code and data within an untrusted host environment, such as the cloud and IoT. Despite strong security guarantees, TEE adoption has been hindered by an awkward programming model. This model requires manual application partitioning and the use of error-prone, memory-unsafe, and potentially information-leaking low-level C/C++ libraries.\n  We address the above with \\textit{HasTEE}, a domain-specific language (DSL) embedded in Haskell for programming TEE applications. HasTEE includes a port of the GHC runtime for the Intel-SGX TEE. HasTEE uses Haskell's type system to automatically partition an application and to enforce \\textit{Information Flow Control} on confidential data. The DSL, being embedded in Haskell, allows for the usage of higher-order functions, monads, and a restricted set of I/O operations to write any standard Haskell application. Contrary to previous work, HasTEE is lightweight, simple, and is provided as a \\emph{simple security library}; thus avoiding any GHC modifications. We show the applicability of HasTEE by implementing case studies on federated learning, an encrypted password wallet, and a differentially-private data clean room.",
        "Publication date": "24 July, 2023",
        "Link": "https://arxiv.org/pdf/2307.13172"
    },
    {
        "ID": 497,
        "Title": "Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues",
        "Authors": [
            "Yue Liu",
            "Thanh Le-Cong",
            "Ratnadira Widyasari",
            "Chakkrit Tantithamthavorn",
            "Li Li",
            "Xuan-Bach D. Le",
            "David Lo"
        ],
        "Abstract": "We systematically study the quality of 4,066 ChatGPT-generated code implemented in two popular programming languages, i.e., Java and Python, for 2,033 programming tasks. The goal of this work is three folds. First, we analyze the correctness of ChatGPT on code generation tasks and uncover the factors that influence its effectiveness, including task difficulty, programming language, time that tasks are introduced, and program size. Second, we identify and characterize potential issues with the quality of ChatGPT-generated code. Last, we provide insights into how these issues can be mitigated. Experiments highlight that out of 4,066 programs generated by ChatGPT, 2,756 programs are deemed correct, 1,082 programs provide wrong outputs, and 177 programs contain compilation or runtime errors. Additionally, we further analyze other characteristics of the generated code through static analysis tools, such as code style and maintainability, and find that 1,930 ChatGPT-generated code snippets suffer from maintainability issues. Subsequently, we investigate ChatGPT's self-repairing ability and its interaction with static analysis tools to fix the errors uncovered in the previous step. Experiments suggest that ChatGPT can partially address these challenges, improving code quality by more than 20%, but there are still limitations and opportunities for improvement. Overall, our study provides valuable insights into the current limitations of ChatGPT and offers a roadmap for future research and development efforts to enhance the code generation capabilities of AI models like ChatGPT.",
        "Publication date": "14 December, 2023",
        "Link": "https://arxiv.org/pdf/2307.12596"
    },
    {
        "ID": 498,
        "Title": "How Does Naming Affect LLMs on Code Analysis Tasks?",
        "Authors": [
            "Zhilong Wang",
            "Lan Zhang",
            "Chen Cao",
            "Nanqing Luo",
            "Xinzhi Luo",
            "Peng Liu"
        ],
        "Abstract": "The Large Language Models (LLMs), such as GPT and BERT, were proposed for natural language processing (NLP) and have shown promising results as general-purpose language models. An increasing number of industry professionals and researchers are adopting LLMs for program analysis tasks. However, one significant difference between programming languages and natural languages is that a programmer has the flexibility to assign any names to variables, methods, and functions in the program, whereas a natural language writer does not. Intuitively, the quality of naming in a program affects the performance of LLMs in program analysis tasks. This paper investigates how naming affects LLMs on code analysis tasks. Specifically, we create a set of datasets with code containing nonsense or misleading names for variables, methods, and functions, respectively. We then use well-trained models (CodeBERT) to perform code analysis tasks on these datasets. The experimental results show that naming has a significant impact on the performance of code analysis tasks based on LLMs, indicating that code representation learning based on LLMs heavily relies on well-defined names in code. Additionally, we conduct a case study on some special code analysis tasks using GPT, providing further insights.",
        "Publication date": "28 July, 2024",
        "Link": "https://arxiv.org/pdf/2307.12488"
    },
    {
        "ID": 499,
        "Title": "How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation",
        "Authors": [
            "Cen Zhang",
            "Yaowen Zheng",
            "Mingqiang Bai",
            "Yeting Li",
            "Wei Ma",
            "Xiaofei Xie",
            "Yuekang Li",
            "Limin Sun",
            "Yang Liu"
        ],
        "Abstract": "LLM-based (Large Language Model) fuzz driver generation is a promising research area. Unlike traditional program analysis-based method, this text-based approach is more general and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges. To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs ($8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that: - While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications; - LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process; - While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection. Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.",
        "Publication date": "29 July, 2024",
        "Link": "https://arxiv.org/pdf/2307.12469"
    },
    {
        "ID": 500,
        "Title": "Dyadic Existential Rules",
        "Authors": [
            "Georg Gottlob",
            "Marco Manna",
            "Cinzia Marte"
        ],
        "Abstract": "Existential rules form an expressive Datalog-based language to specify ontological knowledge. The presence of existential quantification in rule-heads, however, makes the main reasoning tasks undecidable. To overcome this limitation, in the last two decades, a number of classes of existential rules guaranteeing the decidability of query answering have been proposed. Unfortunately, only some of these classes fully encompass Datalog and, often, this comes at the price of higher computational complexity. Moreover, expressive classes are typically unable to exploit tools developed for classes exhibiting lower expressiveness. To mitigate these shortcomings, this paper introduces a novel general syntactic condition that allows us to define, systematically and in a uniform way, from any decidable class $\\mathcal{C}$ of existential rules, a new class called Dyadic-$\\mathcal{C}$ enjoying the following properties: $(i)$ it is decidable; $(ii)$ it generalises Datalog; $(iii)$ it generalises $\\mathcal{C}$; $(iv)$ it can effectively exploit any reasoner for query answering over $\\mathcal{C}$; and $(v)$ its computational complexity does not exceed the highest between the one of $\\mathcal{C}$ and the one of Datalog. Under consideration in Theory and Practice of Logic Programming (TPLP).",
        "Publication date": "22 July, 2023",
        "Link": "https://arxiv.org/pdf/2307.12051"
    }
]